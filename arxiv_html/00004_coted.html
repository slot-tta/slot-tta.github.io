<!doctype html>
    <html>
    <head>
        <meta charset="UTF-8">
        <link rel="stylesheet" media="all" href="normalize.css">
        <link rel="stylesheet" media="all" href="core.css">
        <link rel="stylesheet" media="all" href="style.css">
        <script type="text/javascript"
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
    
        <script type="text/javascript">
          MathJax.Hub.Config({
            tex2jax: {
              inlineMath: [['$', '$'], ['\(', '\)']],
              displayMath: [['$$', '$$'], ['\[', '\]']],
              processEscapes: true
            }
          });
        </script>
        
</head>
    <body>
    <h1>ON IDENTIFIABILITY IN TRANSFORMERS</h1>
<p>Gino Brunnerl*, Yang $\mathrm{{\mathbf{Liu}^{2*}}}$ , Damian Pascuall*, Oliver Richter', Massimiliano Ciaramita?, Roger Wattenhofer1  </p>
<p>Departments of Electrical Engineering and Information Technology, 2Computer Science <br />
ETH Zurich, Switzerland <br />
3Google Research, Zurich, Switzerland <br />
1{brunnegi, dpascual,richtero, wattenhofer}@ethz.ch, <br />
2iiu.yang@alumni.ethz.ch <br />
3massi@google.com  </p>
<h1>ABSTRACT</h1>
<p>In this paper we delve deep in the Transformer architecture by investigating two of its core components: self-attention and contextual embeddings. In particular, we study the identifiability of attention weights and token embeddings, and the aggregation of context into hidden tokens. We show that, for sequences longer than the attention head dimension, attention weights are not identifiable. We propose effective attention as a complementary tool for improving explanatory interpretations based on attention. Furthermore, we show that input tokens retain to a large degree their identity across the model. We also find evidence suggesting that identity information is mainly encoded in the angle of the embeddings and gradually decreases with depth. Finally, we demonstrate strong mixing of input information in the generation of contextual embeddings by means of a novel quantification method based on gradient attribution. Overall, we show that self-attention distributions are not directly interpretable and present tools to better understand and further investigate Transformer models.  </p>
<h1>1 INTRODUCTION</h1>
<p>In this paper we investigate neural models of language based on self-attention by concentrating on the concept of identifiability. Intuitively, identifiability refers to the ability of a model to learn stable representations. This is arguably a desirable property, as it affects the replicability and interpretability of the model's predictions. Concretely, we focus on two aspects of identifiability. The first is related to structural identifiability (Bellman &amp; Astrom, 1970): the theoretical possibility (a priori) to learn a unique optimal parameterization of a statistical model. From this perspective, we analyze the identifiability of attention weights, what we call attention identifiability, in the self-attention components of transformers (Vaswani et al., 2017), one of the most popular neural architectures for language encoding and decoding. We also investigate token identifiability, as the fine-grained, word-level mappings between input and output generated by the model. The role of attention as a means of recovering input-output mappings, and various types of explanatory insights, is currently the focus of much research and depends to a significant extent on both types of identifiability.  </p>
<p>We contribute the following findings to the ongoing work: With respect to attention indentifiability, in Section 3, we show that - under mild conditions with respect to input sequence length and attention head dimension - the attention weights for a given input are not identifiable. This implies that there can be infinitely many different attention weights that yield the same output. This finding challenges the direct interpretability of attention distributions. As a supplement, we propose the concept of effective attention, a diagnostic tool that examines attention weights for model explanations by removing the weight components that do not infuence the model's predictions.  </p>
<p>With respect to token identifiability, in Section 4, we devise an experimental setting where we probe the hypothesis that contextual word embeddings maintain their identity as they pass through successive layers of a transformer. This is an assumption made in much current research, which has not received a clear validation yet. Our findings give substance to this assumption, although it does not always hold in later layers. Furthermore, we show that the identity information is largely encoded in the angle of the embeddings and that it can be recovered by a nearest neighbour lookup after a learned linear mapping from hidden to input token space.  </p>
<p>In Section 5 we further investigate the contribution of all input tokens in the generation of the contextual embeddings in order to quantify the mixing of token and context information. We introduce Hidden Token Attribution, a quantification method based on gradient attribution. We find that selfattention strongly mixes context and token contributions. Token contribution decreases monotonically with depth, but the corresponding token typically remains the largest individual contributor. We also find that, despite visible effects of long term dependencies, the context aggregated into the hidden embeddings is mostly local. We notice how, remarkably, this must be an effect of learning.  </p>
<h1>2  BACKGROUND ON TRANSFORMERS</h1>
<p>The Transformer (Vaswani et al., 2017) is currently the neural architecture of choice for natural language processing (NLP). At its core it consists of several multi-head self-attention layers. In these layers, every token of the input sequence attends to all other tokens by projecting its embedding to a query, key and value yector. Formally, let $Q\,\in\,\mathbb{R}^{d_{s}\times d_{q}}$ be the query matrix, $\boldsymbol{K}\in\mathbb{R}^{d_{s}\times d_{q}}$ the key matrix and $V\in\mathbb{R}^{d_{s}\times d_{v}}$ the value matrix, where $d_{s}$ is the sequence length and $d_{q}$ and $d_{v}$ the dimension of query and value vectors, respectively. The output of an attention head is given by:  </p>
<p>$$
{\mathrm{Attention}}(Q,K,V)=A\cdot V\qquad{\mathrm{with}}\quad A={\mathrm{softmax}}\left({\frac{Q K^{T}}{\sqrt{d_{q}}}}\right)
$$  </p>
<p>The attentionmatrix $A\in\mathbb{R}^{d_{s}\times d_{s}}$ calculates, for each token in the sequence, how much the computation of the hidden embedding at this sequence position should be influenced by each of the other (hidden) embeddings. Self-attention is a non-local operator, which means that at any layer a token can attend to all other tokens regardless of the distance in the input. Self-attention thus produces socalled contextual word embeddings, as successive layers gradually aggregate contextual information into the embedding of the input word.  </p>
<p>We focus on a Transformer model called BERT (Devlin et al., 2019), although our analysis can be easily extended to other models such as GPT, (Radford et al., 2018; 2019) RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019b), or ALBERT (Lan et al., 2020). BERT operates on input sequences of length $d_{s}$ . We denote input tokens in the sentence as $\pmb{x}_{i}$ , where $i\in[1,...,d_{s}]$ . We use $\pmb{x}_{i}\ \in\ \mathbb{R}^{d}$ with embedding dimension $d$ to refer to the sum of the token-, segment- and position embeddings corresponding to the input word at position $i$ . We denote the contextual embedding at position $i$ and layer $l$ as $e_{i}^{l}$ . Lastly, we refer to the inputs and embeddings of all sequence positions as matrices $\mathbf{\deltaX}$ and $\boldsymbol{E}$ , respectively, both in $\mathbb{R}^{d_{s}\times d}$ . For all experiments we use the pre-trained uncased BERT-Base model as provided by Devlin et al. $(2019)^{1}$  </p>
<h1>3ATTENTION IDENTIFIABILITY</h1>
<p>We begin with the identifiability analysis of self-attention weights. Drawing an analogy with structural identifiability (Bellman &amp; Astrom, 1970), we state that the attention weights of an attention head for a given input are identijfiable if they can be uniquely determined from the head's output.2 We emphasize that attention weights are input dependent and not model parameters. However, their identifiability affects the interpretability of the output, i.e., whether attention weights can provide the basis for explanatory insights on the model's predictions (cf. Jain &amp; Wallace (2019) and Wiegreffe &amp; Pinter (2019)). If attention is not identifiable, explanations based on attention may be unwarranted.  </p>
<p>The output of a multi-head attention layer is the summation over each of the $h$ single head outputs (cf. Eq. 1) multiplied by the matrix $\dot{\boldsymbol{H}}\in\mathbb{R}^{d_{v}\times d}$ withreduced head dimension $d_{v}=d/h$  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We begin by recalling the output of a single attention head, which is given by the matrix product of the attention matrix A and the value matrix V. In the context of a multi-head attention layer, the output of each head is weighted by the matrix H, which reduces the dimensionality of the head's output. By considering the summation over each of the h single head outputs, we can express the overall output of the multi-head attention layer as a product of the attention matrix A, the value matrix V, and the weighting matrix H., 
 before perplexity: 4.019504219371204, after perplexity: 3.7698140422192714</p>
<p>Completions End </p>
<p></span>$$
\mathrm{Attention}(Q,K,V)H=A E W^{V}H=A T
$$  </p>
<p>where $W^{V}\in\mathbb{R}^{d\times d_{v}}$ projects the embedding $\boldsymbol{E}$ into the value matrix $\boldsymbol{V}=E\boldsymbol{W}^{V}$ , and we define ${\pmb T}=E W^{V}{\pmb H}$ . Here, the layer and head indices are omitted for simplicity, since the proof below is valid for each individual head and layer in Transformer models. Intuitively, the head output is a linear combination of the $_T$ vectors using the attention as weighting coefficients. If the sequence length, i.e. the number of weighting coefficient, is larger than the rank of $\textbf{\emph{T}}$ ,attention weights are not uniquely determined from the head output; i.e., they include free variables. In other words, some of the $_T$ rows are linear combinations of others. We now prove, by analyzing the null space dimension of $\textbf{\emph{T}}$ , that attention weights are not identifiable using the head or final model output.  </p>
<p>3.1 UPPER BOUND FOR RANK $(T)$  </p>
<p>We first derive the upper bound of the rank of matix ${\pmb T}\,=\,E W^{V}{\pmb H}$ . Note that rani $\operatorname{k}(A B C)\leq$ $\operatorname*{min}\left(\operatorname{rank}(A),\operatorname{rank}({\bar{B}}),\operatorname{rank}(C)\right)$ , therefore,  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We observe that the rank of matrix $\pmb{T}$ is bounded by the minimum of the ranks of matrices $\pmb{E}$, $\pmb{W}^{V}$, and $\pmb{H}$. Specifically, we consider the rank of the product of matrices $\pmb{A}$, $\pmb{B}$, and $\pmb{C}$, which is less than or equal to the minimum of the ranks of $\pmb{A}$, $\pmb{\bar{B}}$, and $\pmb{C}$. Applying this property, we derive the upper bound for the rank of matrix $\pmb{T}$ as the minimum of the ranks of $\pmb{E}$, $\pmb{W}^{V}$, and $\pmb{H}$., 
 before perplexity: 0.9322797042423323, after perplexity: 0.8784694643316149</p>
<p>Completions End </p>
<p></span>$$
\begin{array}{r l}&amp;{\operatorname{rank}\left(\pmb{T}\right)\leq\operatorname_{min}\left(\operatorname{rank}(\pmb{E}),\operatorname{rank}(\pmb{W}^{V}),\operatorname{rank}(\pmb{H})\right)}\ &amp;{~~~~~~~~~~\leq\operatorname_{min}(d_{s},d,d_{v},d_{v},d)}\ &amp;{~~~~~~~~~~=\operatorname*{min}\left(d_{s},d_{v}\right).}\end{array}
$$  </p>
<p>The second step holds since $\mathrm{rank}(E)\,\leq\,\operatorname_{min}(d_{s},d)$ $\operatorname{rank}(W^{V})\,\leq\,\operatorname_{min}(d,d_{v})$ and $\operatorname{rank}(H)\ \leq$ $\operatorname*{min}(d_{v},d)$  </p>
<h1>3.2THE NULL SPACE OF $T$</h1>
<p>The (left) null space $\mathrm{LN}(\mathbf{\calT})$ of $\textbf{\emph{T}}$ describes all vectors that are mapped to the zero vector by $_T$  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We begin by considering the definition of the left null space of a matrix, which describes all vectors that are mapped to the zero vector by the matrix. Specifically, we are interested in the left null space of matrix $\mathbf{T}$, denoted as $\mathrm{LN}(\mathbf{T})$, which consists of all vectors $\tilde{\pmb{x}}^{T}$ in $\mathbb{R}^{1\times d_{s}}$ that satisfy the equation $\tilde{\pmb{x}}^{T}\mathbf{T}=\pmb{0}$. This equation represents the condition for a vector to be in the left null space of $\mathbf{T}$., 
 before perplexity: 0.9869940711301994, after perplexity: 0.7373902573171897</p>
<p>Completions End </p>
<p></span>$$
\begin{array}{r}{\mathrm{LN}(\pmb{T})={\tilde{\pmb{x}}^{T}\in\mathbb{R}^{1\times d_{s}}|\tilde{\pmb{x}}^{T}\pmb{T}=\pmb{0}}}\end{array}
$$  </p>
<p>Its special property is that, for $\tilde{\pmb{A}}=[\tilde{\pmb{x}}_{1},\tilde{\pmb{x}}_{2},...,\tilde{\pmb{x}}_{d_{s}}]^{T}$ where $\tilde{\pmb{x}}_{i}^{T}$ are vectors in this null space,  </p>
<p>$$
(A+{\tilde{A}})\mathbf{T}=A T.
$$  </p>
<p>If the dimension of $\mathrm{LN}(\mathbf{\calT})$ is not zero, there exist infinitely many attention weights $A+{\tilde{A}}$ yielding the exact same attention layer output and final model outputs. By applying the Rank-Nullity theorem, the dimension of the null space is:  </p>
<p>$$
\dim(\operatorname{LN}(T))=d_{s}-\operatorname{rank}\left(T\right)\geq d_{s}-\operatorname*{min}\left(d_{s},d_{v}\right)={\left{\begin{array}{l l}{d_{s}-d_{v},}&amp;{{\mathrm{if~}}d_{s}&gt;d_{v}}\ {0,}&amp;{{\mathrm{otherwise}}}\end{array}\right.}
$$  </p>
<p>here we make use of the facts that $\mathrm{dim}(\mathrm{LN}(\mathbf{{T}}))=\mathrm{dim}(\mathrm{N}(\mathbf{{T}}^{T}))$ and $\operatorname{rank}(\pmb{T})=\operatorname{rank}(\pmb{T}^{T})$ where $\mathrm{N}(T)$ represents the null space of a matrix $\textbf{\emph{T}}$ . Equality holds if $\boldsymbol{E}$ ， $W^{V}$ and $\pmb{H}$ are of full rank and their matrix product does not bring further rank reductions.  </p>
<p>Hence, when the sequence length is larger than the attention head dimension $(d_{s}\ &gt;\ d_{v})$ ),selfattention is not unique. Furthermore, the null space dimension increases with the sequence length. In the next section we show that the attention weights are also non-identifiable; i.e., the non-trivial null spaceof $_T$ exists, even when the weights are constrained within the probability simplex.  </p>
<h1>3.3  THE NULL SPACE WITH PROBABILITY CONSTRAINTS</h1>
<p>Since $\pmb{A}$ is the result of a softmax operation, its rows are constrained within the probability simplex: $A~\ge~0$ (element-wise), and $\boldsymbol{A}\boldsymbol{1}\,=\,\boldsymbol{1}$ , where $\textbf{1}\in\mathbb{R}^{d_{s}}$ is the vector of all ones. However, the derivation in Section (3.2) does not take these constraints into account. It shows that $\pmb{A}$ is not unique, but it does not prove that alternative attention weights exist within the probability simplex, and thus that $\pmb{A}$ is not identifiable. Below, we show that $\bar{A}$ exists in $\mathrm{LN}(\mathbf{\calT})$ also when constraining the weights of $(A+{\tilde{A}})$ to the probability simplex.  </p>
<p>For the row vectors from an alternative attention matrix $A+{\tilde{A}}$ to be valid distributions, we require, in addition to $A\geq0$ (element-wise) and $\mathbf{A1}=\mathbf{1}$ , that $A+\tilde{A}\geq0$ (element-wise), and $A\mathbf{1}+\bar{\tilde{A}}\mathbf{1}=\mathbf{1}$ Furthermore, $\tilde{A}$ must be in the (left) null space of $_T$ . We formalize the three conditions below:  </p>
<p>$$
\mathbf{a})\ \tilde{\mathbf{A}}\mathbf{T}=\mathbf{0}\quad\mathbf{b})\ \tilde{\mathbf{A}}\mathbf{1}=\mathbf{0}\quad\mathrm{c})\ \tilde{\mathbf{A}}\geq-\mathbf{A}
$$  </p>
<p>Conditions $(7\mathrm{a})$ and $(7\mathsf{b})$ can be combined as $A[\pmb{T},\mathbf{1}]=\mathbf{0}$ ,where $[\pmb{T},\pmb{1}]$ is the augmented matrix resulting from adding a column vector of ones to $_T$ . Reusing the argument presented in Section (3.2), the dimension of its (left) null space is: $\dim(\operatorname{LN}([\pmb{T},\pmb{1}]))~\geq~\operatorname*{max}(d_{s}~-~d_{v}~-~1,0)$ . Thus, for $d_{s}-d_{v}&gt;1$ , the null space of $[\pmb{T},\pmb{1}]$ exists: it is a linear subspace of $\mathrm{LN}(T)$  </p>
<p>We now prove that condition (7c) can also be satisfied. We begin by providing an intuitive justification. The condition restricts the space of $\tilde{A}$ from $\mathrm{LN}([\pmb{T},\mathbf{1}])$ to be a bounded region which could be different for each row vector $\pmb{a}=(a_{1},a_{2},\ldots)$ of $\pmb{A}$ . The null space $\mathrm{LN}([\pmb{T},\mathbf{1}])$ contains O, defining a surface passing through the origin. Since $\textbf{\em a}$ is a probability vector, resulting from a softmax transformation, each of its components is strictly positive, i.e., $A&gt;0$ (element-wise). Hence, there exists $\epsilon\,&gt;\,0$ such that any point $\tilde{\pmb{a}}$ in the sphere centered at the origin with radius $\epsilon$ will satisfy condition (7c), $\tilde{\pmb{a}}&gt;-\pmb{a}$ . Crucially, this sphere intersects the null space, as they share the origin. Any point in this intersection satisfies all three conditions in (7).  </p>
<p>Formally, the construction of the null space vector $\tilde{\pmb{a}}$ for the alternative attention weights $\tilde{\pmb{a}}+\pmb{a}$ goes as follows. For a vector $\tilde{\pmb{a}}=(\tilde{a}_{1},\tilde{a}_{2},...)\in\mathrm{LN}([\pmb{T},\mathbf{1}])$ , to ensure one of its negative components $\tilde{a}_{i}~&lt;~0$ satisfying condition $(7\mathrm{c})$ that $\tilde{a}_{i}~\geq~-a_{i}$ , one could shrink its magnitude into $\lambda\tilde{a}$ with $0\leq\lambda\leq-a_{i}/\tilde{a}_{i}$ ,so $\lambda\tilde{a_{i}}\geq-a_{i}$ . Considering all negative components $i$ , the overall scaling factor is $\begin{array}{r}{\lambda_{m a x}=\operatorname*{min}_{i\in{i|\tilde{a}_{i}&lt;0}}(-a_{i}/\tilde{a}_{i})}\end{array}$ so that the direction from the origin ${\lambda\tilde{\mathbf{a}}|0\leq\lambda\leq\lambda_{m a x}}$ satisfies condition (7c). Here $\lambda_{m a x}$ is strictly greater than O because $a_{i}~&gt;~0$ . Only when there exists an index $i$ that $\tilde{a}_{i}&lt;0$ and $a_{i}\approx0$ , then this particular null space direction $\tilde{\pmb{a}}$ is highly confined. In the extreme case, where $\textbf{\em a}$ is a one-hot distribution, the solution should be an $\tilde{\pmb{a}}$ with only one negative component. If such an $\tilde{\pmb{a}}$ does not exist in $\mathrm{LN}([\pmb{T},\mathbf{1}])$ , the solution collapses to the trivial single point $\tilde{\mathbf{a}}=\mathbf{0}$ . However, in general, $\mathrm{LN}([\pmb{T},\mathbf{1}])$ with probability constraints is non-trivial.  </p>
<h1>3.4EFFECTIVE ATTENTION</h1>
<p>The non-identifiability of self-attention, due to the existence of the non-trivial null space of $\textbf{\emph{T}}$ challenges the interpretability of attention weights. However, one can decompose attention weights $\pmb{A}$ into the component in the null space $A^{\parallel}$ and the component orthogonal to the null space $A^{\perp}$  </p>
<p>$$
A T=(A^{\parallel}+A^{\perp})T=A^{\perp}T
$$  </p>
<p>since $\pmb{A}^{\parallel}\in\mathrm{LN}(\pmb{T})\implies\pmb{A}^{\parallel}\pmb{T}=\mathbf{0}$ Hence, we propose a novel concept named effective attention,  </p>
<p>$$
\begin{array}{r}{{\pmb A}^{\bot}={\pmb A}-\mathrm{Projection}_{\mathrm{LN}(T)}{\pmb A},}\end{array}
$$  </p>
<p>which is the part of the attention weights that actually affects the model output. The null space projection is calculated by projecting attention weights into the left null space basis, i.e., the associated left singular vectors.  </p>
<p>Here the definition of effective attention uses $\mathrm{LN}(\mathbf{\calT})$ instead of the null space $\mathrm{LN}([\pmb{T},1])$ With probability constraints. As a consequence, effective attention is not guaranteed to be a probability distribution; e.g., some of the weights might be negative. One could define effective attention as the minimal norm alternative attention using $\mathrm{LN}([T,\Bar{1}])$ , or possibly constrain it within the probability simplex. However, in this case, the minimal norm alternative attention is not orthogonal to the null space $\mathrm{LN}(\mathbf{\calT})$ anymore. It seems unclear how to interpret the minimal norm alternative attention. The reason being that the distinction between components that affect or do not affect the output computation? are defined with respect to $\mathrm{LN}(\mathbf{\calT})$ and not with respect to $\mathrm{LN}([\pmb{T},\mathbf{1}])$ . In fact, there may be useful information in the sign of the effective attention components. Although the combination of value vectors in the transformer architecture uses weights in the probability simplex, these probability constraints on attention may not be necessary. Hence, we provide here the base version of an effective attention, and leave the investigation of other formulations for future research.  </p>
<p><img alt="" src="images/77abf904195e59443797756c0f45bc6e218036873cfeb8183f58aba9f8ae0871.jpg" /><br />
Figure 1: (a) Each point represents the Pearson correlation coefficient of effective attention and raw attention as a function of token length. (b) Raw attention vs. (c) effective attention, where each point represents the average (effective) attention of a given head to a token type.  </p>
<h1>3.5 EMPIRICAL EVIDENCE</h1>
<p>We conclude by providing some initial empirical evidence in support of the notion that effective attention can serve as a complementary diagnostic tool for examining how attention weights influence model outputs.  </p>
<p>First, we show that effective attention can be detected, and can diverge significantly from raw attention. In Figure 1a, we illustrate how the Pearson correlation between effective and raw attention decreases with sequence length. We use the same Wikipedia samples as in Clark et al. (2019) with maximum sequence length 128. This result is in line with our theoretical finding in Eq. 6 that states an increase in the dimension of the null space with the sequence length. Given a bigger null space, more of the raw attention becomes irrelevant, yielding a lower correlation between effective and raw attention. Notice how, for sequences with fewer than $d_{v}\,=\,64$ tokens, the associated null space dimension is zero, and hence attention and effective attention are identical (Pearson correlation of value 1). This loss of correlation with increased sequence length questions the use of attention as explanation in practical models, where it is not uncommon to use large sequence lengths. A few examples include: BERT for question answering (Alberti et al., 2019) and XL-Net (Yang et al., 2019a) With $d_{s}=512$ , or document translation (Junczys-Dowmunt, 2019) with $d_{s}=1000$  </p>
<p>To illustrate the point further, Figure 1 shows in (b) raw attention $\pmb{A}$ and (c) effective attention $A^{\perp}$ using again the data of Clark et al. (2019). We compute the average attention of BERT and compare it to the corresponding average effective attention. Clark et al. (2019) conclude that the [CLS] token attracts more attention in early layers, the [SEP] tokens attract more in middle layers, and periods and commas do so in deep layers. However, after a gradient based investigation, they propose that attention to the [SEP] token is generally a “no-op". The effective attention weights suggest a more consistent pattern: while periods and commas seem to generally attract more attention than [CLS] and [SEP], the peak of [SEP] token observed by raw attention has disappeared. Effective attention provides an explanation: the [SEP] token peak is irrelevant to the computation of the output for middle layers; i.e., it is in the null space component of the corresponding attention vector. The same arguments also hold for the sharp peak of raw attention on punctuation tokens between layers 10 and 12. An additional example showing similar results can be found in Appendix A.2. See also Appendix A.3, where we discuss in more depth a case where effective attention would support interpretive conclusions that differ from those one might draw solely based on raw attention. In conclusion, effective attention can help discover interesting interactions encoded in the attention weights which may be otherwise obfuscated by the null attention.  </p>
<h1>4 TOKEN IDENTIFIABILITY</h1>
<p>We now study the other fundamental element of transformers; the hidden vector representations of tokens, or contextual word embeddings. It is commonly assumed that a contextual word embedding  </p>
<p>keeps its “identity", which is tied to the input word, as it passes through the self-attention layers. <br />
Specifically, we identify three cases where this assumption is made implicitly without justification. <br />
· Visualizations/interpretations linking attention weights to attention between words, when in fact the attention is between embeddings, i.e., mixtures of multiple words (Vaswani et al., 2017; Devlin et al., 2019; Vig, 2019; Clark et al., 2019; Raganato &amp; Tiedemann, 2018; Voita et al., 2019; Tang et al., 2018; Wangperawong, 2018; Padigela et al., 2019; Baan et al., 2019; Dehghani et al., 2019; Zenkel et al., 2019). <br />
· Attention accumulation methods that sum the attention to a specific sequence position over layers and/or attention heads, when the given position might encode a different mixture of inputs in each layer (Clark et al., 2019; Baan et al., 2019; Klein &amp; Nabi, 2019; Coenen et al.,2019). <br />
· Using classifiers to probe hidden embeddings for word-specific aspects without factoring in how much the word is still represented (Lin et al., 2019; Peters et al., 2018).  </p>
<p>To investigate this assumption we introduce the concept of token identifiability, as the existence of a mapping assigning contextual embeddings to their corresponding input tokens. Formally, we state that an embedding $\bar{e}_{i}^{l}$ is identifiable if there exists a classification function $c(\cdot)$ such that $\bar{c}(e_{i}^{l})=x_{i}$ For identifiability we only require $c(e_{i}^{l})$ to recover $\pmb{x}_{i}$ in a nearest neighbour sense within the same input sentence. Therefore, for each layer $l$ we define $c_{l}(\cdot)=N N(g_{l}(\cdot))$ , where $N N$ is a 1-nearest neighbour lookup, and $g_{l}:\,\mathbb{R}^{d}\,\rightarrow\,\dot{\mathbb{R}}^{d}$ is a continuous function mapping embeddings to vectors of real numbers. Since we cannot prove the existence of $g_{l}$ analytically, we instead use a function approximator $\hat{g}_{l}(e_{i}^{l})=\hat{x}_{i}$ , trained on a dataset of $(e_{i}^{l},{\pmb x}_{i})^{}$ pairs. We then say that $e_{i}^{l}$ is identifiable $\hat{c}_{l}(e_{i}^{l})=N N(\hat{g}_{l}(e_{i}^{l}))=x_{i}$ . For evaluation we report the token identifiability rate defined as the percentage of correctly identified tokens.  </p>
<h1>4.1 SETUP</h1>
<p>For the experiments in this and subsequent sections we use the development dataset from the Microsoft Research Paraphrase Corpus (MRPC) dataset (Dolan &amp; Brockett, 2005), while in Appendix $\mathrm{D}$ we provide results on two additional datasets. The MRPC development set contains 408 examples with a sequence length $d_{s}$ between 26 and 92 tokens, with 58 tokens on average. We pass all 408 sentences (21,723 tokens) through BERT and extract for each token the input embeddings $\pmb{x}_{i}$ and the hidden embeddings $e_{i}^{l}$ at all layers. We then train $\hat{g}$ on the regression task of predicting input tokens $\pmb{x}_{i}$ from hidden tokens $e_{i}^{l}$ . We experiment with two loss functions and similarity measures for finding the nearest neighbour; cosine distance and $L_{2}$ distance. We use 10-fold cross validation with 70/15/15 train/validation/test splits per fold and ensure that tokens from the same sentence are not split across sets. The validation set is used for early stopping. See Appendix B.1 for details.  </p>
<h1>4.2 EXPERIMENTAL RESULTS AND DISCUSSION</h1>
<p>In a frst experiment, we use alinear percptron without bias and a on-linear M $\hat{g}_{l}^{M L P}$ , where training, validation and test data all come from layer $l$ .Figure 2a shows the test set token identifiability rate of $\hat{c}_{l}$ for $l\,=\,[1,...,12]$ .We also report a naive baseline $\hat{g}_{l}^{n a i v e}(e_{i}^{l})\,=\,e_{i}^{l}$ , i.e., we directly retrieve the nearest neighbour of $e_{i}^{l}$ from the input tokens. The results for $\hat{g}_{l}^{n a i v e}$ show that, according to both similarity measures, contextual embeddings in BERT stay close to their input embeddings up to layer 4, followed by a linear decrease in token identifiability rate. By training a transformation to recover the original embedding, we see that most of the identity information is still present in the contextualized embeddings. Specifically, a linear projection is enough to recover $93\%$ of the tokens in the last layer based on a cosine distance nearest neighbour lookup.  </p>
<p>This experiment shows that although the identifiablity rate decreases with depth, tokens remain mostly identifiable across layers. Furthermore, we find that cosine distance is more effective to recoveridentitythan $L_{2}$ distance. Therefore, we conjecture that BERT encodes most of the identity information in the angle of the embeddings. Finally, Lin et al. (2019) show that BERT discards much of the positional information after layer 3. However, tokens remain largely identifiable throughout the model, indicating that BERT does not only rely on the positional embeddings to track token identity. To provide further insights into contextual word embeddings, Appendix B.5 shows results for recovering neighbouring input tokens.  </p>
<p><img alt="" src="images/4210cedd63d62b75601039e471a3bf68380c1e74a21ae24880c7affc44f2f150.jpg" /><br />
Figure 2: (a) Identifiability of contextual word embeddings at different layers. Here, $\hat{g}$ is trained and tested on the same layer. (b) $g_{c o s,l}^{l i n}$ trained on layer $l$ and tested on allayers.  </p>
<p>In a second experiment we test how well the $\hat{g}_{c o s,l}^{l i n}$ trained only on $(e_{i}^{l},x_{i})$ pairs from one layer $l$ generalizes to all layers, see Figure 2b. For $l=1$ , the token identifiability rate on subsequent layers drops quickly to below $70\%$ at layers 11 and 12. Interestingly, for $l=12$ a very different pattern can be observed, where the identifiability is $94\%$ for layer 12 and then almost monotonically increases when testing on earlier layers. Further, for $l=6$ we see both patterns.  </p>
<p>This experiment suggests that the nature of token identity changes as tokens pass through the model, and patterns learned on data from later layers transfer well to earlier layers. The experiment also shows that layer 12 is behaving differently than the other layers. In particular, generalizing to layer 12 from layer 11 seems to be difficult, signified by a sudden drop in token identifiability rate. We believe this is due to a task dependent parameter adaptation induced in the last layer by the nextsentence prediction task which only uses the CLS token (cf. Appendix B.4 for additional hints that the last layer(s) behave differently). See Appendix B.3 for results o gl2u, 9i.L andgcos.p.  </p>
<p>Overall, the results of this section suggest that one can associate most hidden embeddings with their input token, for example for drawing conclusions based on (effective) attention weights. However, self-attention has the potential to strongly mix tokens across multiple layers, and hence it is unclear whether token identifiability alone is enough to equate hidden embeddings with their input words, or whether we also need to take into account exactly how much of the word is still contained in the hidden embedding. In order to address this question, we now study the degree of information mixing among embeddings, and introduce a tool to track the contributions of tokens to embeddings throughout themodel.  </p>
<h1>5  ATTRIBUTION ANALYSIS TO IDENTIFY CONTEXT CONTRIBUTION</h1>
<p>We consider the role of the contextual information in the hidden embeddings, which is accumulated through multiple paths in a multi-layer network. To shed more light on this process, we introduce Hidden Token Attribution, a context quantification method based on gradient attribution (Simonyan et al., 2014) to investigate the hidden tokens’ sensitivity with respect to the input tokens.  </p>
<h1>5.1HIDDEN TOKEN ATTRIBUTION</h1>
<p>Gradient based attribution approximates the neural network function $f(X)$ around a given sequence of input word embeddings $\dot{\boldsymbol{X}}\in\mathbb{R}^{d_{s}\times d}$ by the linear part of the Taylor expansion:  </p>
<p>$$
f(X+\Delta X)\approx f(X)+\nabla_{X}f(X)^{T}\cdot\Delta X
$$  </p>
<p>With this, the network sensitivity is analyzed by looking at how small changes $\Delta{X}$ at the input correlate with changes at the output. Since in the linear approximation this change is given by the gradient $\begin{array}{r}{\nabla_{{\pmb x}_{i}}f\,=\,\frac{\mathbf{\bar{\delta}}f(\pmb X)}{\delta{\pmb x}_{i}}}\end{array}$ sf(X for a change in the i-th input token E;  Rd of X, the atribution of how much input token $\pmb{x}_{i}$ affects the network output $f(\boldsymbol X)$ can be approximated by the $L_{2}$ norm of the respective gradient: $\mathrm{attr}(\mathbf{\boldsymbol{x}}_{i})\;=\;||\nabla_{\mathbf{\boldsymbol{x}}_{i}}f||_{2}$ . Since we are interested in how much a given hidden embedding $e_{j}^{l}$ attributes to the input tokens $\pmb{x}_{i}$ ， $i\in[1,2,\ldots,d_{s}]$ , we define the relative input contribution $c_{i,j}^{l}$ of input $\pmb{x}_{i}$ to output $f(\mathbf{{\boldsymbol{X}}})=e_{j}^{l}$ as  </p>
<p><img alt="" src="images/7beb4279d8dc03e7c6ea9e568df005543d0b1e4f87584962ae71db75102b2097.jpg" /><br />
Figure 3: (a) Contribution of the input token to the embedding at the same position. The orange line represents the median value and outliers are not shown. (b) Percentage of tokens P that are not the main contributors to their corresponding contextual embedding at each layer.  </p>
<p>$$
c_{i,j}^{l}=\frac{||\nabla_{i,j}^{l}||_{2}}{\sum_{k=0}^{d_{s}}||\nabla_{k,j}^{l}||_{2}}\qquad\mathrm{with}\quad\nabla_{i,j}^{l}=\frac{\delta e_{j}^{l}}{\delta x_{i}}
$$  </p>
<p>Since we normalize by dividing by the sum of the attribution values to all input tokens, we obtain values between O and 1 that represent the contribution of each input token $\pmb{x}_{i}$ to the hidden embedding $e_{j}^{l}$ . Hidden Token Attribution differs from the standard use of gradient atribution in that, instead of taking the gradients of the output of the model with respect to the inputs in order to explain the model's decision, we calculate the contribution of the inputs to intermediate embeddings in order to track the mixing of information. Further details of this method are discussed in Appendix C.1.  </p>
<h1>5.2 TOKEN MIXING: CONTRIBUTION OF INPUT TOKENS</h1>
<p>We use Hidden Token Attribution to extend the results of Section 4 showing how much of the input token is contained in a given hidden embedding. In Figure 3a we report the contribution $c_{j,j}^{l}$ of input tokens $\scriptstyle{\mathbf{\boldsymbol{x}}}_{j}$ to their corresponding hidden embeddings $e_{j}^{l}$ at the same position $j$ for each layer $l$ . After the first layer the median contribution of the input token is less than a third $(30.6\%)$ . The contribution then decreases monotonically with depth; at layer 6 the median is only $14.4\%$ and after the last layer it is $10.7\%$ . In Appendix C.5 we provide detailed results by word type. Next, we study which input token is the largest contributor to a given hidden embedding $e_{j}^{l}$ . The corresponding input token $\pmb{x}_{j}$ generally has the largest contribution. Figure 3b shows the percentage $\tilde{P}$ of tokens that are not the highest contributor to their hidden embedding at each layer. In the first three layers the original input $\pmb{x}_{j}$ always contributes the most to the embedding $e_{j}^{l}$ In subsequent layers, $\tilde{P}$ increases monotonically, reaching $18\%$ in the sixth layer and $30\%$ in the last two layers.  </p>
<p>These results show that, starting from layer three, self-attention strongly mixes the input information by aggregating context into the hidden embeddings. This is in line with the results from Section 4, where we see a decrease in token identifiability rate after layer three. Nevertheless, $\tilde{P}$ isalways higher than the token identifiability error at the same layer, indicating that tokens are mixed in a way that often permits recovering token identity even if the contribution of the original token is outweighed by other tokens. This suggests that there is some “identity information” that is preserved throughthe layers.  </p>
<p>The strong mixing of information questions the common assumption that attention distributions can be interpreted as “how much a word attends to another word". However, the fact that tokens remain identifiable despite information mixing opens a number of new interesting questions to be addressed by future research. In particular, this seeming contradiction may be solved by investigating the space in which hidden embeddings operate: is there a relation between semantics and geometric distance for hidden embeddings? Are some embedding dimensions more important than others?  </p>
<p><img alt="" src="images/2caeb0f92e3dd10e4538f400e6e7992bfee2bee638661806ab458d6acfc55a07.jpg" /><br />
Figure 4: (a) Relative contribution per layer of neighbours at different positions. (b) Total contribution per neighbour for the first, middle and last layers.  </p>
<h1>5.3CONTRIBUTION OF CONTEXT TO HIDDEN TOKENS</h1>
<p>In this section we study how context is aggregated into hidden embeddings. Figure 4a shows the relative contribution of neighbouring tokens at each layer for the relative positions: first, second, third, fourth and fifth together, sixth to 10th together, and the rest. The closest neighbours (1st) contribute significantly more in the first layers than in later layers. Conversely, the most distant neighbours (11th onwards) contribute the most in deeper layers (cf. Appendix C.2). Despite the progressive increase in long-range dependencies, the context in the hidden embeddings remains mostly local. Figure 4b represents the normalized total contribution aggregated over all tokens from each of their neighbours at the first, middle and last layer. This figure shows that the closest neighbours consistently contribute the most to the contextual word embedding regardless of depth. On the other hand, we indeed observe an increase of distant contributions at later layers.  </p>
<p>The results of this section suggest that BERT learns local operators from data in an unsupervised manner, in the absence of any such prior in the architecture. This behavior is not obvious, since attention is a highly non-local operator, and in turn indicates the importance of local dependencies in natural language. While contribution is local on average, we find that there are exceptions, such as the [CLS] token (cf. Appendix C.3). Furthermore, using our Hidden Token Attribution method, one can track how context is aggregated for specific tokens (cf. Appendix C.4).  </p>
<h1>6 RELATED WORK</h1>
<p>Input-output mappings play a key role in NLP. For example, in machine translation, they were introduced in the form of explicit alignments between source and target words (Brown et al., 1993). Neural translation architectures re-introduced this concept in the form of attention (Bahdanau et al., 2015). The development of multi-head self-attention (Vaswani et al., 2017) has led to many impressive results in NLP. As a consequence, much work has been devoted to better understand what these models learn, with a particular focus on using attention to explain model decisions.  </p>
<p>Jain &amp; Wallace (2019) show that attention distributions of LSTM based encoder-decoder models are not unique, and that adversarial attention distributions that do not change the model's decision can be constructed. They further show that attention distributions only correlate weakly to moderately with dot-product based gradient attribution. Wiegreffe &amp; Pinter (2019) also find that adversarial attention distributions can be easily found, but that these alternative distributions perform worse on a simple diagnostic task. Serrano &amp; Smith (2019) find that zero-ing out attention weights based on gradient attribution changes the output of a multi-class prediction task more quickly than zero-ing out based on attention weights, thus showing that attention is not the best predictor of learned feature importance. Pruthi et al. (2019) demonstrate that self-attention models can be manipulated to produce different attention masks with very little cost in accuracy. These papers differ in their approaches, but they all provide empirical evidence showing that attention distributions are not unique with respect to downstream parts of the model (e.g., output) and hence should be interpreted with care. Here, we support these empirical findings by presenting a theoretical proof of the identifiability of attention weights. Further, while these works focus on RNN-based language models with a single layer of attention, we instead consider multi-head multi-layer self-attention models. Our token classification and token mixing experiments show that non-identifiable tokens increase with depth, further reinforcing the point that the factors that contribute to the mixing of information are complex and deserve further study.  </p>
<p>Voita et al. (2019) and Michel et al. (2019) find that only a small number of heads in BERT have a relevant effect on the output. These results are akin to ours about the non-identifiability of attention weights, showing that a significant part of attention weights do not affect downstream components. One line of work investigates the internal representations of transformers by attaching probing classifiers to different parts of the model. Tenney et al. (2019) find that BERT has learned to perform steps from the classical NLP pipeline. Similarly, Jawahar et al. (2019) show that lower layers of BERT learn syntactic features, while higher layers learn semantic features. They also argue that long-range features are learned in later layers, which agrees with our atribution-based experiments.  </p>
<h1>7 CONCLUSION</h1>
<p>We used the notion of identifiability to gain a better understanding of transformers from different yet complementary angles. We started by proving that attention weights are non-identifiable when the sequence length is longer than the attention head dimension. Thus, infinitely many attention distributions can lead to the same internal representation and model output. As an alternative, we propose effective attention, a method that improves the interpretability of attention weights by projecting out the null space. Second, we show that tokens remain largely identifiable through a learned linear transformation followed by a nearest neighbor lookup based on cosine similarity. However, input tokens gradually become less identifiable in later layers. Finally, we present Hidden Token Attribution, a gradient-based method to quantify information mixing. This method is general and can be used to investigate contextual embeddings in self-attention based models. In this work, we use it to demonstrate that input tokens mix heavily inside transformers. This result means that attention-based interpretations, which suggest that a word at some layer is attending to another word can be improved by accounting for how the tokens are mixed inside the model. We further show that context is progressively aggregated into the hidden embeddings while some identity information is preserved. Moreover, we show that context aggregation is mostly local and that distant dependencies become relevant only in the last layers, which highlights the importance of local information for natural language understanding. Our results suggest that some of the conclusions in prior work (Vaswani et al., 2017; Vig, 2019; Marecek &amp; Rosa, 2018; Clark et al.,2019; Raganato &amp; Tiedemann, 2018; Voita et al., 2019; Tang et al., 2018; Wangperawong, 2018; Padigela et al., 2019; Baan et al., 2019; Lin et al., 2019; Dehghani et al., 2019; Zenkel et al., 2019; Klein &amp; Nabi, 2019; Coenen et al., 2019) may be worth re-examining from this perspective.  </p>
<p>There are still many open questions for future research. For one, by constraining effective attention to the probability simplex, one could better compare it to standard attention, although in this case non-influencing parts would be included in the weights. More research is needed to better understand the differences between these formulations. Further, we find that tokens mix and remain largely identifiable. While these two conclusions are not necessarily at odds - a token can both gather context information and still retain the essence of the original input word - we believe that the relationship between mixing and identifiability warrants further investigation. Moreover, it is becoming increasingly difficult to compare all the new transformer variants, and it is hence important to gain a deeper understanding of this class of models. The concepts introduced in this paper could help in identifying fundamental differences and commonalities between variants of self-attention models.  </p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>We would like to thank the reviewers and area chairs for their thorough technical comments and valuable suggestions. We would also like to thank Jacob Devlin for feedback on an early draft. This research was supported with Cloud TPUs from Google's TensorFlow Research Cloud (TFRC).  </p>
<h1>REFERENCES</h1>
<p>Lnris Alperu, Kenton Lee, ana Micnael Colins. A BEKI Baseline Ior tne Natural Quesuons. https://arxiv.org/abs/1901.08634, 2019.  </p>
<p>Joris Baan, Maartje ter Hoeve, Marlies van der Wees, Anne Schuth, and Maarten de Rijke. Do transformer attention heads provide transparency in abstractive summarization?  CoRR, abs/1907.00570, 2019. <br />
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.0rg/abs/1409.0473. <br />
R. Bellman and Karl Johan Astrom. On structural identifiability. Mathematical Biosciences, 7: 329-339, 1970. <br />
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19 (2):263-311, 1993. URL https : //www .aclweb.org/anthology/J93-2003. <br />
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? an analysis of bert's attention. CoRR, abs/1906.04341, 2019. <br />
Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda B. Viegas, and Martin Wattenberg. Visualizing and measuring the geometry of BERT. CoRR, abs/1906.02715, 2019. <br />
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. <br />
Jacob Devlin, Ming-Wei Chang, Kenton Le, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the NorthAmericanChapterof the AssociationforComputational Linguistics:Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171-4186, 2019. <br />
William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005, Jeju Island, Korea, October 2005, 2005, 2005. <br />
Xavier Glorot and Yoshua Bengio. Understanding the dificulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Inteligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp. 249-256, 2010. URL http://proceedings.mlr.press/v9/giorot10a.html. <br />
Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415, 2016. URL http: / /arxiv.0rg/abs /1606. 08415. <br />
Sarthak Jain and Byron C. Wallace. Attention is not explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers),pp. 3543-3556, 2019. <br />
Ganesh Jawahar, Benoit Sagot, and Djame Seddah. What does BERT learn about the structure of language? In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL2019, Flrence, Italy, July 28- August 2, 2019, Volme 1: Long Papers, pp. 3651-3657, 2019. URL https: //www.aclweb.0rg/anthology/P19-1356/. <br />
Marcin Junczys-Dowmunt. Microsoft translator at wmt 2019: Towards large-scale document-level neural machine translation. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pp. 225-233, 2019.  </p>
<p>Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: / /arxiv .0rg/abs /1412 .6980.  </p>
<p>Tassilo Klein and Moin Nabi. Attention is (not) all you need for commonsense reasoning. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 4831-4836, 2019.  </p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020. URL https : / / openreview .net / forum? id $=$ HleA7AEtvS.  </p>
<p>Yongjie Lin, Yi Chern Tan, and Robert Frank. Open sesame: Getting inside bert's linguistic knowledge. arXiv preprint arXiv:1906.01698, 2019.  </p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.  </p>
<p>David Marecek and Rudolf Rosa.  Extracting syntactic trees from transformer encoder selfattentions. In Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP@ EMNLP 2018, Brussels, Belgium, November 1, 2018, pp. 347-349, 2018.  </p>
<p>Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? CoRR, abs/1905.10650, 2019. URL http: / /arxiv.0rg/abs /1905.10650.  </p>
<p>Harshith Padigela, Hamed Zamani, and W. Bruce Croft. Investigating the successes and failures of BERT for passage re-ranking. CoRR, abs/1905.01758, 2019.  </p>
<p>Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 -November 4, 2018, pp. 1499-1509, 2018.  </p>
<p>Nina Porner, Hinrich Schitze, and Benjamin Roth. Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pp. 340-350, 2018.  </p>
<p>Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C Lipton. Learning to deceive with attention-based explanations. arXiv preprint arXiv: 1909.07913, 2019.  </p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. URL htps://s3-us-west-2. amazonaws. com/openaiassets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.  </p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Hlya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.  </p>
<p>Alessandro Raganato and Jorg Tiedemann. An analysis of encoder representations in transformerbased machine translation. In Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP@ EMNLP 2018, Brussels, Belgium, November 1, 2018, pp. 287- 297,2018.  </p>
<p>Sofia Serrano and Noah A. Smith. Is attention interpretable? In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, Pp. 2931-2951, 2019. URL https: / /www.aclweb.org/ antho1ogy/P19-1282/.  </p>
<p>Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,Workshop Track Proceedings,2014.  </p>
<p>Gongbo Tang, Rico Sennrich, and Joakim Nivre. An analysis of attention mechanisms: The case of word sense disambiguation in neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 - November 1, 2018, pp. 26-35, 2018. <br />
Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, Pp. 4593-4601, 2019. <br />
Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL 2003, Edmonton, Canada, May 27 - June 1, 2003, 2003. <br />
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Mllia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 5998-6008, 2017. <br />
Jesse Vig. Visualizing attention in transformer-based language representation models. CoRR, abs/1904.02679,2019. <br />
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 5797-5808, 2019. <br />
Artit Wangperawong. Attending to mathematical language with transformers. CoRR, abs/1812.02825, 2018. <br />
Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv preprint arXiv: 1805.12471, 2018. <br />
Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. CoRR, abs/1908.04626, 2019. URL http: //arxiv.0rg/abs /1908.04626. <br />
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers),pp. 11i2-1122. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1101. <br />
Baosong Yang, Zhaopeng Tu, Derek F Wong, Fandong Meng, Lidia S. Chao, and Tong Zhang. Modeling localness for self-attention networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 4449-4458, 2018. URL https://aclanthology.info/papers/ D18-1475/d18-1475. <br />
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressve pretraining for language understanding. https://arxiv.org/abs/1906.08237, 2019a. <br />
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XInet: Generalized autoregressive pretrainingforlanguage understanding. InAdvances inneural information processing systems, pp. 5754-5764, 2019b. <br />
Thomas Zenkel, Joern Wuebker, and John DeNero. Adding interpretable attention to neural translation models improves word alignment. CoRR, abs/1901.11359, 2019.  </p>
<h1>AIDENTIFIABILITY OF SELF-ATTENTION</h1>
<h1>A.1 BACKGROUND ON ATTENTION IDENTIFIABILITY</h1>
<p>Often, the identifiability issue arises for a model with a large number of unknown parameters and limited observations. Taking a simple linear model $y\,=\,x_{1}\beta_{1}+x_{2}\beta_{2}$ as an example, when there is only one observation $(y,x_{1},x_{2})$ , model parameters $\beta_{1}$ and $\beta_{2}$ cannot be uniquely determined. Moreover, in the matrix form $Y\,=\,X\beta$ , by definition the parameter $\beta$ is identifiable only if $Y=$ $X\beta_{1}$ and $Y=X\beta_{2}$ imply $\beta_{1}=\beta_{2}$ . So if the null space contains only the zero solution ${\beta|X\beta=$ ${0}}={0}$ , i.e., Xβ1 $-\ X\beta_{2}=X(\beta_{1}-\beta_{2})=0\ \Longrightarrow\ \beta_{1}-\beta_{2}=0$ , the model is identifiable. Therefore, the identifiability of parameters in a linear model is linked to the dimension of the null space, which in the end is determined by the rank of $X$  </p>
<p>A.2 ADDITIONAL RESULTS OF THE EFFECTIVE ATTENTION VS. RAW ATTENTION RESULTS  </p>
<p>In Figure 5 we provide a recreation of the figure regarding the attention of tokens towards the [SEP] token found in (Clark et al., 2019, Figure 2) with average attention as well as average effective attention. Again, we see that most of the raw attention lies effectively in the null space, confirming the pattern of Figure 1. The figures are produced using the code from Clark et al. (2019).  </p>
<p><img alt="" src="images/fbd7663e5ad2a1b59221d608084051b3ecdef48793a30f986dcf260a78f7942b.jpg" /><br />
Figure 5: Effective attention (a) vs. raw attention (b). (a) Each point represents the average effective attention from a token type to a token type. Solid lines are the average effective attention of corresponding points in each layer. (b) is the corresponding figures using raw attention weights.  </p>
<h1>A.3A CLOSER LOOK AT EFFECTIVE ATTENTION WEIGHTS</h1>
<p>Here we discuss an example of how effective attention might lead to interpretive conclusions that differ from raw attention. Figure 6 plots the attention weights (raw, effective, null) from one of the attention heads in BERT's layer 4, for the following passage:  </p>
<p>"[CLS] research into military brats has consistently shown them to be better behaved than their civilian counterparts. [SEP] hypotheses as to why brats are better behaved: firstly, military parents have a lower threshold for misbehavior in their children; secondly, the mobility of teenagers might make them less likely to attract attention to themselves, as many want to fit in and are less secure with their surroundings; and thirdly, normative constraints are greater, with brats knowing that their behavior is under scrutiny and can affect the military member's career. teenage years are typically a period when people establish independence by taking some risks away from their [SEP]"'.  </p>
<p>For readability, on the y-axis, we consider just the sentence "'the mobility of teenagers might make them less likely to attract attention to themselves, as many want to fit in and are less secure with their surroundings'.  </p>
<h1>The following seems worth noticing:</h1>
<p>· Raw attention weights are by and large concentrated either on the structural components, [CLS] and [SEP], or on the semi-monotonic, near diagonal alignments.  </p>
<p>· Effective attention weights are more uniform, in general. They are still concentrated near the diagonal elements, although less so than in raw attention. However, the attention on [CLS] and [SEP] has vanished. The collapse of the [CLS] and [SEP] weights brings to the surface other interesting things. As an example, we point out the highest weight on the attention matrix, that is not on the diagonal. This involves (highlighted by means of the yellow lines) the main verb of the selected sentence, "make", whose object is "them" (teenagers), and the pronoun "'them” (the direct object of the first sentence, "'military brats", 48 positions away). The two are co-referential, as both refer to the main subject of the passage, military brats. <br />
· Null attention weights are also more uniform than raw attention ones. Interestingly, they seem to carry all the mass of the [CLS] and [SEP] tokens. There is a visible degree of redundancy between the null attention weights and the effective ones, but also clear complementary elements.  </p>
<p>One should not extrapolate too much from a single observation. Further research is needed on this topic. However, this example is a proof of concept that raw and effective attention can diverge qualitatively, in significant ways. It agrees with the hypothesis that the weights on the structural components may act as sinks, as observed in (Clark et al., 2019), but also tells us how this happens. Furthermore, it indicates that attention in the null space can obfuscate other valuable interactions that may be recoverable by inspecting effective attention.  </p>
<p><img alt="" src="images/fa2d4e111357c9c128916b606a34abf7d6a110a508b63d866c53a23fecd6ad16.jpg" /><br />
Figure 6: Raw attention weights (top), Effective attention weights (middle) and Null attention weights (bottom).  </p>
<h1>BTOKEN IDENTIFIABILITY EXPERIMENTS</h1>
<h1>B.1 EXPERIMENTAL SETUP AND TRAINING DETAILS</h1>
<p>The linear perceptron and MLP are both trained by either minimizing the L2 or cosine distance loss using the ADAM optimizer (Kingma &amp; Ba, 2015) with a learning rate of $\alpha=0.0001$ $\beta_{1}=0.9$ and $\beta_{2}\,=\,0.999$ . We use a batch size of 256. We monitor performance on the validation set and stop training if there is no improvement for 20 epochs. The input and output dimension of the models is $d=768$ ; the dimension of the contextual word embeddings. For both models we performed a learning rate search 0ver the values Q E [0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001, 0.000003]. The weights are initialized with the Glorot Uniform initializer (Glorot &amp; Bengio, 2010). The MLP has one hidden layer with 1oo0 neurons and uses the gelu activation function (Hendrycks &amp; Gimpel, 2016), following the feed-forward layers in BERT and GPT. We chose a hidden layer size of 1000 in order to avoid a bottleneck. We experimented with using a larger hidden layer of size 3072 and adding dropout to more closely match the feed-forward layers in BERT. This only resulted in increased training times and we hence deferred from further architecture search.  </p>
<p>We split the data by sentences into train/validation/test according to a 70/15/15 split. This way of splitting the data ensures that the models have never seen the test sentences (i.e., contexts) during training. In order to get a more robust estimate of performance we perform the experiments in Figure 2a using 10-fold cross validation. The variance, due to the random assignment of sentences to train/validation/test sets, is small, and hence not shown.  </p>
<h1>B.2 GENERALIZATION ERROR</h1>
<p>Figure 7 shows the token identifiability rate for train and test set for both models, linear and MLP, when using L2 distance. Both models are overfitting to the same degree. The fact that the linear model has about the same generalization error as the MLP suggests that more training data would not significantly increase performance on the test set. Further, we trained the MLP on layer 11 using $50\%$ $80\%$ $90\%$ and $100\%$ of the training data set. The MLP achieved the following token identifiability rate on the test set: 0.74, 0.8, 0.81, 0.82. This indicates that the MLP would not profit much from more data.  </p>
<p>We do not report the generalization error for the models trained to minimize cosine distance, as the linear and non-linear perceptrons perform almost equally.  </p>
<p><img alt="" src="images/577bf96de86ae07554e34927b243d4d24b46c87622e3b68807b455d489b69235.jpg" /><br />
Figure 7: Train and test token identifiability rates for the linear perceptron and MLP.  </p>
<p>Figure 2b in the main text only shows results of the linear perceptron trained to minimize cosine distanceonlayers $l\,=\,[1,6,11,12]$ and tested on all other layers. Figures 8, 9 and 10 show the corresponding results for the linear perceptron trained to minimize cosine distance, and for the MLP trained to minimize L2 and cosine distance respectively. Overall, all figures show the same qualitative trends as presented in Section 4 of the main text: Generalizing to later layers works considerably worse than the other way around. The linear perceptrons seem to generalize better across layers, likely due to the MLPs overfitting more to the particular layers they are trained on.  </p>
<p><img alt="" src="images/63320fe7b86015ae55adcab4135ff72468ba41e6c301ac90abd98664dc392fcf.jpg" /><br />
Figure 8: Linear Perceptron trained to minimize L2 distance generalizing to all layers.  </p>
<p><img alt="" src="images/36ee02cfa30713a5c3b0b15748590c5350f8a45ff36d499300e26c6e3b1c3477.jpg" /><br />
Figure 9: MLP trained to minimize L2 distance generalizing to all layers.  </p>
<p><img alt="" src="images/7071a0b2303aac1bf15a4d22571c28a83a699f34145efb2b9ee27019abfa88f9.jpg" /><br />
Figure 10: MLP trained to minimize cosine distance generalizing to all layers.  </p>
<h1>B.4 TOKEN IDENTITY - FROM HIDDEN TOKENS TO HIDDEN TOKENS</h1>
<p>Figure 11 shows results for identifying tokens across single layers of BERT, i.e., the input to $g$ is $(e_{i}^{1},x_{i})$ in the first layer, and subsequently $(e_{i}^{l},e_{i}^{l-1})$ ，where $l=[1,...,12]$ . This experiment gives further insight into what kinds of transformations are applied by each transformer layer separately, as opposed to the cumulative transformations shown in Section 4 of the main text. Interestingly, even the naive baselines perform well across single layers. This shows that BERT only applies small changes to the contextual word embeddings, whereas overall the angle (as indicated by the naive baseline using cosine distance) is affected less than the magnitude of the word embeddings (indicated by the naive baseline using L2 distance).  </p>
<p>Figure 11 shows that tokens are on average more difficult to identify across later layers. In the main text we hypothesize that the qualitative change seen in later layers could be due to a task-specific parameter adaptation during the second (next sentence prediction) pre-training phase. A possible reason is that during this pre-training-phase, BERT only needs the [CLS] token in the last layer, which is qualitatively very different form the first (masked language modeling) pre-training phase, where potentially all the tokens are needed in the last layer.  </p>
<p>To further verify this hypothesis we experimented with BERT fine-tuned on two datasets, MRPC and CoLA (Warstadt et al., 2018). During the fine-tuning phase, similar to the next sentence prediction pre-training phase, only the [CLS] token is needed at the last layer. If task-dependent parameter adaptation indeed has a different infuence on the last layer(s) than on earlier layers, then we should be able to see a difference between the finetuned and non-finetuned cases. Figures 12 and 13 compare the naive baselines across single layers for BERT finetuned on MRPC and CoLA, respectively. Indeed, one can see a remarkable decrease in identifiability across the last layer for L2-based nearest neighbour lookup, further indicating that the last layers are indeed more strongly affected by different fine-tuning objectives. Nearest neighbour lookup based on cosine distance is affected much less, indicating that in terms of token identifiability, the last layers are only slightly affected by fine-tuning.  </p>
<p><img alt="" src="images/ad66b199712c56377b21bf33b20dd370f5b9e274d2da0551d313e7a8b7239b0f.jpg" /><br />
Figure 11: Token identifiability across single layers. These results are for non fine-tuned BERT on MRPC.  </p>
<p><img alt="" src="images/c2aee1554d7ec0ee782668b658a21a0668c243b0211d0f52a2898fa4e77b1334.jpg" />  </p>
<p>Figure 12: Token identifiability across single layers, comparing non fine-tuned (dashed) BERT against BERT fine-tuned on MRPC (solid).  </p>
<p><img alt="" src="images/1a91bdbcfc03d8a874bcd895c46b15e3e7479255a34d37c3d6507bab7ffa7687.jpg" /><br />
Figure 13: Token identifiability across single layers, comparing non fine-tuned BERT (dashed) against BERT fine-tuned on CoLA (solid).  </p>
<h1>B.5TOKENIDENTITY-RECOVER NEIGHBOURINGINPUT TOKENS</h1>
<p>In Section 4 of the main text we show that tokens at position $i$ remain largely identifiable throughout the layers of BERT. In this section we show results of a related experiment, where we test how much information about tokens at neighbouring positions is contained in a contextual word embedding. More formally, the input to $g$ is $(e_{i}^{l},x_{i\pm k})$ , where $k\ \in\ {1,2,3}$ . Thus, we try to recover input token ri+t from hidden token e,. Figures 14, 15, 16 and 17 show the results of for glos,# 9,P * $\hat{g}_{L2,l}^{M L P}$ and $\hat{g}_{L2,l}^{l i n}$ ,respectivly In the figures, blue coresponds to *previous"'tokens and red to "next" tokens.  </p>
<p>From the figures we can see that tokens do contain information about neighbouring tokens that lets us recover the neighbouring tokens based on a transformation and subsequent nearest neighbour lookup. The identifiability rate drops both with increasing $k$ , but also with increasing depth. Interestingly, recovering left (blue) and right (red) neighbours shows different behaviour, indicating that BERT is treating left and right context differently, despite having been pre-trained using a bi-directional language modeling objective.  </p>
<p><img alt="" src="images/38a5062aace66e890fcf2f95ecf6127b5b8550e93842f9a601333a74037a8875.jpg" /><br />
Overall, neighbouring tokens can be recovered to a much lower degree than same-position tokens (cf. Section4). <br />
Figure 14: Recovering neighbouring input tokens using $\hat{g}_{c o s,l}^{l i n}$  </p>
<p><img alt="" src="images/b15399ca722011ae1114d5eb344a3bbaa3835b8e8b94db2cdf05d77bd42e5d32.jpg" /><br />
Figure 15:Rverng neigbng intkesuig $\hat{g}_{c o s,l}^{m l p}$  </p>
<p><img alt="" src="images/99a968fc826a88909fc168fab4397b6d208d3233589b3fd6246f7deca1389b4d.jpg" /><br />
Figure 16:Rvering neigburing int kesuig $\hat{g}_{L2,l}^{m l p}$  </p>
<p><img alt="" src="images/b2c82aa2676ebf6ff554551c99ffd04b2e0ce3634df9db64e8e1bf62517622b2.jpg" /><br />
Figure 17: Recovering neighbouring input tokens using $\hat{g}_{l2,l}^{l i n}$  </p>
<h1>C  CONTEXT CONTRIBUTION ANALYSIS</h1>
<h1>C.1 HIDDEN TOKEN ATTRIBUTION: DETAILS</h1>
<p>The attribution method proposed in Section 5.1 to calculate the contribution of input tokens to a given embedding does not look at the output of the model but at the intermediate hidden representations and therefore is task independent. Since the contribution values do not depend on the task that is evaluated, we can compare these values directly to attention distributions, which are also taskindependent. In this way, we can compare to other works in the literature (Vig, 2019; Clark et al., 2019; Klein &amp; Nabi, 2019; Coenen et al., 2019; Lin et al., 2019) by using the publicly available pretrained BERT model in our analyses without fine-tuning it to a specific task.  </p>
<p>Furthermore, since we are not interested in analysing how the input affects the output of the model but in quantifying the absolute contribution of the input tokens to the hidden embeddings, we use the $L_{2}$ norm of the gradients. If we were analyzing whether the input contributed positively or negatively to a given decision, the dot-product of the input token embedding with the gradient would be the natural attribution choice (Porner et al., 2018).  </p>
<h1>C.2 CONTEXT IDENTIFIABILITY: DETAILS</h1>
<p>To calculate the relative contribution values shown in Figure 4a we firstly calculate the mean of the left and right neighbours for each of the groups of neighbours, i.e., first, second, third, fourth and fifth, sixth to 10th and, from 11th onwards. Then we aggregate the values averaging over all the tokens in the MRPC evaluation set. Finally, we normalize for each group so that the sum of the contribution values of each group is one. In this way, we can observe in which layer the contribution of a given group of neighbours is the largest.  </p>
<p>Our results on context identifiability from Section 5.3 complement some of the studies in previous literature. In (Jawahar et al., 2019) the authors observe that transformers learn local syntactic tasks in the first layers and long range semantic tasks in the last layers. We explain this behavior from the point of view of context aggregation by showing that distant context acquires more importance in the last layers (semantic tasks) while the first layers aggregate local context (syntactic tasks). Furthermore, the results showing that the context aggregation is mainly local, specially in the first layers, provide an explanation for the increase in performance observed in (Yang et al., 2018). In that work, the authors enforce a locality constraint in the first layers of transformers, which pushes the model towards the local operators that it naturally tends to learn, as we show in Figure 4b, improving in this way the overall performance.  </p>
<h1>C.3 CONTEXT CONTRIBUTION TO CLS TOKEN</h1>
<p>In this section we use Hidden Token Attribution to look at the contribution of the context to the [CLS] token, which is added to the beginning of the input sequence by the BERT pre-processing pipeline. This is an especially interesting token to look at because the decision of BERT for a classification task is based on the output in the [CLS] token. Furthermore, like the [SEP] token, it does not correspond to a natural language word and its position in the input sequence does not have any meaning. Therefore, the conclusion that context is on average predominantly local (cf. Section 5.3), is likely to not hold for [CLS].  </p>
<p>The second and final pre-training task that BERT is trained on is next sentence prediction. During this task, BERT receives two sentences separated by a [SEP] token as input, and then has to predict whether the second sentence follows the first sentence or not. Therefore, it is expected that the context aggregated into the [CLS] token comes mainly from the tokens around the first [SEP] token, which marks the border between the first and second sentence in the input sequence. In Figure 18 we show the contribution to the [CLS] token from all of its neighbours averaged over all the examples in the MRPC evaluation set for the first, middle and last layers. In Figure 18a, the [CLS] token is placed at position O and we see how the context contribution comes mainly from the tokens around position 30, which is roughly the middle of the input examples. In Figure 18b we center the contribution around the first [SEP] token and indeed, it becomes clear that the [CLS] token is aggregating most of its context from the tokens around [SEP], i.e., from the junction between both sentences. In particular, the two tokens with the highest contribution are the tokens directly before and after [SEP]. Also, it seems that the second sentence contributes more to [CLS] than the first one.  </p>
<p>These results give an insight on what information BERT uses to solve next sentence prediction and serves as an illustrative example of how Hidden Token Attribution can be used to analyze transformers.  </p>
<p><img alt="" src="images/e5b8b832c30cf77d965f5317aec3c1f82f8d7652ba6164b05321603d8ed975d7.jpg" /><br />
Figure 18: Normalized total contribution to the [CLS] token (a) centered around [CLS] at position O (b) centered around [SEP].  </p>
<h1>C.4 TRACKING CONTEXT CONTRIBUTION</h1>
<p>Here we show examples of how Hidden Token Attribution can track how context is aggregated for a given word at each layer. For reasons of space we show only few words of a randomly picked sentence of the MRPC evaluation set, which is tokenized as follows:  </p>
<p>[CLS] he said the foods ##er ##vic ##e pie business doesn ’ t fit the company"' s long - term growth strategy . [SEP] " the foods ##er ##vic ##e pie business does not fit our long - term growth strategy.[SEP]  </p>
<p><img alt="" src="images/8705e0d553b6ff392af9e5eef92321da9e6f055c413c764319db4d786365d154.jpg" /><br />
Figure 19: [CLS]: Aggregates context from all tokens but more strongly from those around the first [SEP] token. We hypothesize that this is due to the Next Sentence Prediction pre-training.  </p>
<p><img alt="" src="images/2f9ac45a0312108cb457d017bcd7d5b02455ceab8eb70f5829d591617cb7d0e3.jpg" /><br />
Figure 20: he: Aggregates most context from the main verb of the sentence, "'said".  </p>
<p><img alt="" src="images/ec8829d9225c2f62319c3d23083cccb80f0b17642bc2a5a29345f70cdd8d52f2.jpg" /><br />
Figure 21: said: Aggregates context mainly from its neighborhood, the main verb of the subordinate sentence and the border between the two input sentences.  </p>
<p><img alt="" src="images/a22fa926fe4b9bc1c86dbaf72926ca1349100a5c914b384abebf79bd63dd5a55.jpg" /><br />
Figure 22: fit: In the first layers it aggregates most context from its neighborhood and towards the last layers it gets the context from its direct object (strategy) and from the token with the same meaning in the second sentence.  </p>
<p><img alt="" src="images/b9788a124b9ebd88d1ab11b19da9abf0247a6d5e84863f00f064f2004f73a179.jpg" /><br />
Figure 23: long: It is part of a composed adjective (long-term) and aggregates most of its context from the other part of the adjective (term) as well as from the same tokens in the second sentence. Interestingly, it mostly ignores the hyphen.  </p>
<p><img alt="" src="images/20096c449a985c0e64eadc6c361a40b21ccc12b1eae9a472998f60991b149486.jpg" /><br />
Figure 24: strategy: Aggregates context from the word growth, which is the first one of the noun phrase "growth strategy".  </p>
<p><img alt="" src="images/e3c5dd7e9456a9efe25d26787421f81b73626aa93663d24a6aa1e0e927e436d5.jpg" /><br />
Figure 25: [SEP]: This token that has no semantic meaning aggregates context mostly from [CLS] and its own neighborhood.  </p>
<h1>C.5 TOKEN CONTRIBUTIONS BY POS TAG</h1>
<p>Here we show the contribution of input tokens to hidden representations in all layers split by part-ofspeech (POS) tag (Toutanova et al., 2003). The POS tags are ordered according to the contribution in layer 12.  </p>
<p><img alt="" src="images/5467e43b5fea3a2c802ab49f20d4af59ff012a2a5b71e6e622dfb8c0f92aa4c8.jpg" /><br />
Figure 26: Layer 1: Most token types are equally mixed and have already less than $35\%$ median contribution from their corresponding input. The only exception are the [CLS] tokens, which remain withover $40\%$ median original contribution.  </p>
<p><img alt="" src="images/01dee125ae21acfe4ccee2a804410a33f5be7f18658637fcb6aa09da7baa1550.jpg" /><br />
Figure 27: Layer 2: Similar to the previous layer with less contribution over all and [SEP] behaving similarly to [CLS].  </p>
<p><img alt="" src="images/f55b0f99a979f190d949f22a0bf889dd93b4a12a06f6b65b7907673b9d8fc05c.jpg" /><br />
Figure 28: Layer 3: Similar to layer 2 with decreasing contribution overall  </p>
<p><img alt="" src="images/1d77e243cf7c6e9838f1a15c6cea4462d5cd2590250e0dbe8252c6c808df333d.jpg" />  </p>
<p>Figure 29: Layer 4: The original input contribution to [CLS] and [SEP] falls significantly. The trend that the word types will follow until the last layer is already clear: Most nouns (NNP, NNS, NN), verbs (VBN, VB, VBD, VBP), adjectives (JJ, JS) and adverbs (RBR, RBS) keep more contribution from their corresponding input embeddings than words with “less" semantic meaning like Whpronouns and determiners (WP, WDT), prepositons (IN), coordinating conjunctions (CC), symbols (SYM),possessives $({\mathrm{PRPS}}$ , POS) or determiners (DT).  </p>
<p><img alt="" src="images/0a979b5d41c5965e79c4fc877d66cec94624e2765e15dedf5c7364bfe04278ff.jpg" />  </p>
<p>Figure 30: Layer 5: The trend started in the previous layer continues, with a reduction of internal variability within those word types with less original contribution.  </p>
<p><img alt="" src="images/87b5ae549852f749913dcab9969a508e1f82f478406a80e7d0ab1300e5d47154.jpg" /><br />
Figure 31: Layer 6: Similar behavior as in the previous layer with minor evolution.  </p>
<p><img alt="" src="images/b670a9349c5958d4bb4f8759d4b95619f5afaddddda6a9015d76291111710c91.jpg" /><br />
Figure 32: Layer 7: Minor changes with respect to Layer 6.  </p>
<p><img alt="" src="images/e523f91973d7552538834845ef67809de24a7a2dd64605b933eb0917bd604853.jpg" /><br />
Figure 33: Layer 8: At this point there is clearly a different behavior between the tokens with most contribution which present more intra-class variability, and those with less contribution, which are moreuniform.  </p>
<p><img alt="" src="images/ec519819ffb7e0802fbc8a9ddceb2df4248cfd2747e1993aab901beb817fe6a0.jpg" /><br />
Figure 34: Layer 9: SEP changes increasing the contribution, while the rest stays similar.  </p>
<p><img alt="" src="images/7f741d71d5713e113a0f1dd8871ed0e216ba6596a0370e3d779c7a649450f969.jpg" /><br />
Figure 35: Layer 10: The contribution evolves with the same pattern as in previous layers.  </p>
<p><img alt="" src="images/824b3ececd78dafc48ce7cdcf7e05c70d3eca3f1862c6b8fed02c762d9c43127.jpg" /><br />
Figure 36: Layer 11:The contribution evolves with the same pattern as in previous layers.  </p>
<p><img alt="" src="images/2eb30a24f051764ca68afde0e6ff04d484b44bdb53d80226349bc5a6088dd626.jpg" />  </p>
<p>Figure 37: Layer 12: Finally, nouns, verbs, adjectives, adverbs, receive more contribution from their corresponding input than determiners, prepositions, pronouns, 'to" words and symbols.  </p>
<h1>D GENERALIZATION TO OTHER DATASETS</h1>
<p>In this appendix we reproduce several experiments from the main text using the development sets of two additional datasets from the GLUE benchmark: The Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2018), and the matched Multi-Genre Natural Language Inference corpus (MNLI-matched) (Williams et al., 2018). CoLA is a dataset about grammatical acceptability of sentences and MNLI consists of pairs of sentences where the second sentence entails, contradicts or is neutral about the first one. These datasets differ significantly from MRPC. The development set of CoLa has 1043 examples with sequence length $d_{s}$ between 5 and 35 tokens, and 11 tokens on average. The development set of MNLI-m consits of 9815 examples although we restrict the experiments to the first 4000 examples without loss of generality. These contain a total of 155964 tokens, with a sequence length comprised between 6 and 128 tokens and an average of 39 tokens perexample.  </p>
<p>The results presented in this appendix are qualitatively similar to those presented in the main text, which shows that our empirical conclusions about BERT are general across data domains.  </p>
<h1>D.1 TOKEN IDENTIFIABILITY</h1>
<p>Here we reproduce the main token identifiability results of Section 4 on two additional datsets: CoLA and MNLI. Qualitatively, the results are in line with those for MRPC. Note that a random classifier would achieve an accuracy of $1/\bar{d}_{s}$ ,where $\bar{d_{s}}$ denotes the average sentence length. Thus, the random guessing baselines for MRPC, CoLA and MNLI are $1.7\%$ ， $9\bar{/}\bar{}$ and $2.6\%$ respectively.  </p>
<h1>D.1.1 CoLA</h1>
<p>Figure 38 shows the token identifiability results for CoLA  </p>
<p><img alt="" src="images/9010157e6368818d4e3957387dce28b1cf5c114cd7e2f771b890a32d8ffc075b.jpg" /><br />
Figure 38: Identifiability of contextual word embeddings at different layers on CoLA.  </p>
<h1>D.1.2 MNLI</h1>
<p>Figure 39 shows the token identifiability results for the first 500 sentences (19,839 tokens) of MNLImatched.  </p>
<p><img alt="" src="images/7e02c083b2f972383cab168ff2550dd64026954a5bcba406243c096f1bca0172.jpg" />  </p>
<p>Figure 39: Identifiability of contextual word embeddings at different layers on a the first 500 sentences of MNLI-matched (19,839 tokens).  </p>
<h1>D.2 ATTRIBUTION ANALYSIS</h1>
<h1>D.2.1 COLA EXPERIMENTS</h1>
<p>Figure D.2.1 shows the token mixing analysis for the CoLA dataset. The behavior is very similar to MRPC with the only difference that both, the contribution of the original token and the percentage of tokens that are not maximum contributors to their embeddings are slightly larger across layers. However, this increase is explained by CoLA consisting of much shorter sequences on average than MRPC.  </p>
<p><img alt="" src="images/b95fb7fe25c6f40a04d779f82ff27912a54eb0de26965c5b6f4ebcbe782c0544.jpg" /><br />
Figure 40: (a) Contribution of the input token to the embedding at the same position. (b) Percentage of tokens $\tilde{P}$ that are not the main contributors to their corresponding contextual embedding at each layer.  </p>
<p>Figure D.2.1 presents the context aggregation for the CoLA development set. We observe the same general trend as for MRPC, with the context being aggregated mostly locally and long range dependencies increasing in the later layers. The fact that examples in CoLA have an average sequence length of 11 tokens explains the smaller relative contribution of tokens beyond the 10th neighbour.  </p>
<p><img alt="" src="images/9d919ad73957284624ab2ca27d8f59e8eda95e0d6ddd73df859341b70ff88ead.jpg" /><br />
Figure 41: (a) Relative contribution per layer of neighbours at different positions. (b) Total contribution per neighbour for the first, middle and last layers.  </p>
<h1>D.2.2 MNLIEXPERIMENTS</h1>
<p>As shown by Figures D.2.2 and D.2.2, the results with the MNLI matched dataset are very similar to the ones presented in the main text. No meaningful discrepancy exists in this case.  </p>
<p><img alt="" src="images/7479ae47678ee54de68eab8f8f0f4f6efc22879405e5f1bd52647f04a36e7e96.jpg" /><br />
Figure 42: (a) Contribution of the input token to the embedding at the same position. (b) Percentage of tokens $\tilde{P}$ that are not the main contributors to their corresponding contextual embedding at each layer.  </p>
<p><img alt="" src="images/7b1a9ebbd6fffb55bb9c6b9e22d5cf57695dc3364f46145b19ad119a7ecacc22.jpg" /><br />
Figure 43: (a) Relative contribution per layer of neighbours at different positions. (b) Total contribution per neighbour for the first, middle and last layers.  </p>
    </body>
    </html>