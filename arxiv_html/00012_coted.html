<!doctype html>
    <html>
    <head>
        <meta charset="UTF-8">
        <link rel="stylesheet" media="all" href="normalize.css">
        <link rel="stylesheet" media="all" href="core.css">
        <link rel="stylesheet" media="all" href="style.css">
        <script type="text/javascript"
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
    
        <script type="text/javascript">
          MathJax.Hub.Config({
            tex2jax: {
              inlineMath: [['$', '$'], ['\(', '\)']],
              displayMath: [['$$', '$$'], ['\[', '\]']],
              processEscapes: true
            }
          });
        </script>
        
</head>
    <body>
    <h1>TRANSFORMER-XH: MULTI-EVIDENCE REASONING WITH EXTRA HOP ATTENTION</h1>
<p>Chen Zhao* <br />
University of Maryland, College Park <br />
chenz@ cs.umd.edu <br />
Chenyan Xiong, Corby Rosset, Xia Song, <br />
Paul Bennett, and Saurabh Tiwary <br />
Microsoft AI &amp; Research <br />
cxiong, corosset, xiaso, <br />
pauben, satiwary $@$ microsoft.com  </p>
<h1>ABSTRACT</h1>
<p>Transformers have achieved new heights modeling natural language as a sequence of text tokens. However, in many real world scenarios, textual data inherently exhibits structures beyond a linear sequence such as trees and graphs; many tasks require reasoning with evidence scattered across multiple pieces of texts. This paper presents Transformer-XH, which uses eXtra Hop attention to enable intrinsic modeling of structured texts in a fully data-driven way. Its new attention mechanism naturally ^hops" across the connected text sequences in addition to attending over tokens within each sequence. Thus, Transformer-XH better conducts joint multi-evidence reasoning by propagating information between documents and constructing global contextualized representations. On multi-hop question answering, Transformer-XH leads to a simpler multi-hop QA system which outperforms previous state-of-the-art on the HotpotQA FullWiki setting. On FEVER fact verification, applying Transformer-XH provides state-of-the-art accuracy and excels on claims whose verification requires multiple evidence.  </p>
<h1>1 INTRODUCTION</h1>
<p>Transformers effectively model natural language in sequential form (Vaswani et al., 2017; Dai et al., 2019; Devlin et al., 2019; Yang et al., 2019). Nevertheless, in many NLP tasks, text does not simply appear as a linear sequence of tokens but rather carries meaningful structure in the form of paragraphs, headings, and hyperlinks. These structures can be represented abstractly as trees or graphs with nodes and edges; and the tasks can be performed as joint reasoning on these more general structures as input. Multi-hop question answering (Yang et al., 2018) is one such task in which structure plays an important role, since the evidence required to formulate the answer is scattered across multiple documents, requiring systems to jointly reason across links between them.  </p>
<p>Recent approaches leverage pre-trained Transformers (e.g., BERT) for multi-hop question answering (QA) by converting the structural reasoning task into sub-tasks that model flat sequences. For example, Min et al. (2019b) decompose a multi-hop question into a series of single-hop questions; Ding et al. (2019) conduct several steps of single-hop reading comprehension to simulate the multihop reasoning. The hope is that additional processing to fuse the outputs of the sub-models can recover all the necessary information from the original structure. While pre-trained Transformer language models have shown improvements on multi-hop QA, manipulating the inherent structure of the problem to fit the rigid requirements of out-of-the-box models can introduce problematic assumptions or information loss.  </p>
<p>This paper presents Transformer-XH (meaning eXtra Hop), which upgrades Transformers with the ability to natively represent structured texts. Transformer-XH introduces extra hop attention in its layers that connects different text pieces following their inherent structure while also maintaining the powerful pre-trained Transformer abilities over each textual piece individually. Our extra hop attention enables 1) a more global representation of the evidence contributed by each piece of text as it relates to the other evidence, and 2) a more natural way to jointly reason over an evidence graph by propagating information along edges necessary to complete the task at hand.  </p>
<p>We apply Transformer-XH to two multi-evidence reasoning tasks: Hotpot QA, the multi-hop question answering task (Yang et al., 2018), and FEVER, the fact verification benchmark whose claims often require multiple pieces of evidence to support (Thorne et al., 2018). Rather than decomposing the task into a series of sub-tasks to fit the constraints of pre-trained Transformers, TransformerXH is a solution that fits the problem as it naturally occurs. It is a single model that represents and combines evidence from multiple documents to conduct the reasoning process. On HotpotQA's FullWiki setting, which requires strong multi-hop reasoning capability (Min et al., 2019b; Jiang &amp; Bansal, 2019), Transformer-XH outperforms CogQA (Ding et al., 2019), the previous start-of-theart, by 12 points on answer F1. On FEVER 1.0 shared task, Transformer-XH outperforms GEAR, the Graph Neural Network based approach significantly. On both applications, Transformer-XH beats the contemporary BERT based pipeline SR-MRS (Nie et al., 2019), by 2-3 points.  </p>
<p>The results follow from our simple yet effective design, with one unified model operating over the inherent structure of the task, rather than melding the outputs from disparate sub-tasks adapted to the sequential constraints of pre-trained Transformers. Our ablation studies demonstrate TransformerXH's efficacy on questions that are known to require multi-hop reasoning (Min et al., 2019b) and on verifying multi-evidence claims (Liu et al., 2019b). Our analyses confirm that the source of Transformer-XH's effectiveness success is due to the eXtra Hop attention's ability to fuse and propagate information across multiple documents.  </p>
<h1>2 MODEL</h1>
<p>This section first discusses preliminaries on sequential Transformers, then we show how we incorporate eXtra hop attention to create Transformer-XH.  </p>
<h1>2.1 PRELIMINARIES</h1>
<p>Transformers represent a sequence of input text tokens $X\,=\,{x_{1},...,x_{i},...,x_{n}}$ as contextualized distributed representations $H\,=\,{h_{1},...,h_{i},...,h_{n}}$ (Vaswani et al., 2017). This process involves multiple stacked self-attention layers that converts the input $X$ into ${H^{0},\dot{H}^{1},...,\dot{H}^{l},...H^{L}}$ , starting from $H^{0}$ , the embeddings, to the final layer of depth $L$  </p>
<p>The key idea of Transformer is its attention mechanism, which calculates the $l$ -th layer output $H^{l}$ using the input $H^{l-1}$ from the previous layer:  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: To derive the equation for the self-attention mechanism in the Transformer model, we start by considering the input to the current layer, $H^{l-1}$, which is the output from the previous layer. We then apply linear transformations to $H^{l-1}$ to obtain the query ($Q$), key ($K$), and value ($V$) matrices. The attention mechanism calculates the output of the current layer, $H^{l}$, by taking the dot product of $Q$ and the transpose of $K$ (denoted as $K^{T}$), normalizing the result using a softmax function, and then multiplying it by the transpose of $V$ (denoted as $V^{T}$)., 
 before perplexity: 1.2172454293100488, after perplexity: 1.1912925282597346</p>
<p>Completions End </p>
<p></span>$$
\begin{array}{c}{H^{l}=\mathrm{softmax}(\cfrac{Q\cdot K^{T}}{\sqrt{d_{k}}})\cdot V^{T},}\ {Q^{T};K^{T};V^{T}=W^{q}\cdot H^{l-1};W^{k}\cdot H^{l-1};W^{v}\cdot H^{l-1}.}\end{array}
$$  </p>
<p>It includes three projections on the input $H^{l-1}$ : Query (Q), Key (K), and Value (V).  </p>
<p>Specifically, the slices of token $h_{i}^{l}$ in Eqn.(2) is:  </p>
<p>$$
h_{i}^{l}=\sum_{j}\mathrm{softmax}_{j}(\frac{q_{i}^{T}\cdot k_{j}}{\sqrt{d_{k}}})\cdot v_{j},
$$  </p>
<p>which first calculates its attention to all other tokens $j$ in the sequence and then combines the token values $v_{j}$ into a new representation $h_{i}^{l}$ , using the normalized attention weights. Multiple attentions can be used in one Transformer layer and concatenated as multi-head attention (Vaswani et al., 2017). The architecture is stacked to form rather deep networks, which leads to significant success of large pre-trained Transformer models (Devlin et al., 2019; Liu et al., 2019a).  </p>
<p>A challenge of Transformer is that its attention is calculated over all token pairs (Eqn. 3), which is hard to scale to long text sequences. Transformer-XL (eXtra Long) addresses this challenge by breaking down longer texts, e.g., a multi-paragraph document, into a sequence of text segments: ${X_{1},...,X_{\tau},...,X_{\zeta}}$ , and propagates the information between adjacent text segments using the following attention:  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We observe that the traditional Transformer architecture calculates attention over all token pairs, which can be challenging to scale to long text sequences. To address this limitation, we consider breaking down longer texts into a sequence of text segments, ${X_{1},...,X_{\tau},...,X_{\zeta}}$. We then aim to propagate information between adjacent text segments using a novel attention mechanism. This involves combining the frozen context from the previous segment with the current segment's representation, effectively creating a contextualized representation that captures the relationships between adjacent segments., 
 before perplexity: 1.6552634405820572, after perplexity: 1.4857908569509164</p>
<p>Completions End </p>
<p></span>$$
\tilde{H}_{\tau}^{l-1}=[\mathrm{Freeze}(H_{\tau-1}^{l-1})\circ H_{\tau}^{l-1}].
$$  </p>
<p><img alt="" src="images/19705f34d0f3c91e630308090ce4b20ce0a8f315c51ec3bac7f8d020a1924cb6.jpg" /><br />
Figure 1: The eXtra Hop attention in Transformer-XH (a) and its application to multi-hop QA (b).  </p>
<p>It concatenates (o) the representation of the previous segment $H_{\tau-1}^{l-1}$ to the current segment as segment level recurrences. The new representation $\tilde{H}_{\tau}^{l-1}$ includes the information from previous segment and is integrated in the new attention mechanism:  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We build upon the concept of segment-level recurrence introduced in Transformer-XL, where the representation of the previous segment is concatenated to the current segment. To enable the new attention mechanism, we need to update the projections of the input to the attention mechanism. We do this by incorporating the segment-level recurrence into the query, key, and value projections, allowing the model to attend to both the tokens within the current segment and the information from the previous segment., 
 before perplexity: 0.6213798790650729, after perplexity: 0.6194025644150921</p>
<p>Completions End </p>
<p></span>$$
\tilde{Q}^{T};\tilde{K}^{T};\tilde{V}^{T}=W^{q}\cdot H_{\tau}^{l-1};W^{k}\cdot\tilde{H}_{\tau}^{l-1};W^{v}\cdot\tilde{H}_{\tau}^{l-1}.
$$  </p>
<p>The attention over the previous segment allows Transformer-XL to effectively model long form text data recurrently as a sequence of text chunks (Dai et al., 2019).  </p>
<p>Nevertheless, in many scenarios, the text segments are organized in nontrivial structures beyond a linear sequence. For example, documents are connected by hyperlinks in a graphical structure that does not readily simplify to form a linear sequence, prohibiting Transformer-XL's recurrent approach.  </p>
<h1>2.2 TRANSFORMER-XH WITH EXTRA HOP ATTENTION</h1>
<p>Transformer-XH models structured text sequence by linking them with eXtra Hop attention following their original structure. As illustrated in Figure 1a, to model three connected documents $d_{2}\rightarrow d_{1}\rightarrow d_{3}$ , Transformer-XH uses eXtra Hop attention to propagate information along the graph edges, enabling information sharing between connected text sequence.  </p>
<p>Formally, the structured text data includes a set of nodes, $\mathcal{X}\,=\,{X_{1},...,X_{\tau},...X_{\zeta}}$ ,each corresponding to a text sequence, and an edge matrix $E$ , which includes the connections (e.g., links) between them. The goal is to learn representations $\mathcal{H}={\tilde{H}_{1},...,\tilde{H}_{\tau},...\tilde{H}_{\zeta}}$ , that incorporate not only the local information in each sequence $X$ , but also the global contexts on the entire structured text ${\mathcal{X},E}$  </p>
<p>Transformer-XH achieves this by two attention mechanisms: in-sequence attention and eXtra Hop attention. The in-sequence attention is the same as vanilla Transformer: in layer $l$ token $i$ gathers information from other tokens inside the same text piece $\tau$  </p>
<p>$$
h_{\tau,i}^{l}=\sum_{j}\mathrm{softmax}_{j}(\frac{q_{\tau,i}^{T}\cdot k_{\tau,j}}{\sqrt{d_{k}}})\cdot v_{\tau,j}.
$$  </p>
<p>The eXtra Hop attention uses the first token in each sequence - the added special token “ $\cdot[\mathrm{CLS}]^{\circ}-$ as an “attention hub", which attends on all other connected nodes' hub token. In layer $l$ ,the $\tau$ -th text sequence attends over another text sequence $\eta$ if there is an edge between them ( $e_{\tau\eta}=1$  </p>
<p>$$
\hat{h}_{\tau,0}^{l}=\sum_{\eta;e_{\tau\eta}=1}\mathrm{softmax}_{\eta}(\frac{\hat{q}_{\tau,0}^{T}\cdot\hat{k}_{\eta,0}}{\sqrt{d_{k}}})\cdot\hat{v}_{\eta,0}.
$$  </p>
<p>Node $\tau$ calculates the attention weight on its neighbor $\eta$ using hop query $\hat{q}_{\tau,0}$ and key $\hat{k}_{\eta,0}$ Then it uses the weights to combine its neighbors′ value $\hat{v}_{\eta,0}$ and forms a globalized representation $\hat{h}_{\tau,0}^{l}$  </p>
<p>The two attention mechanism are combined to form the new representation of layer $l$  </p>
<p>$$
\begin{array}{r l}&amp;{\tilde{h}_{\tau,0}^{l}=\mathrm{Linear}([h_{\tau,0}^{l}\circ\hat{h}_{\tau,0}^{l}]),}\ &amp;{\tilde{h}_{\tau,i}^{l}=h_{\tau,i}^{l};\forall i\neq0.}\end{array}
$$  </p>
<p>Note that the non-hub tokens $(i\neq0)$ still have access to the hop attention in the previous layer through Eqn. (6).  </p>
<p>One layer of eXtra Hop attention can be viewed as single-step of information propagation along edges $E$ . For example, in Figure 1la, the document node $d_{3}$ updates its representation by gathering information from its neighbor $d_{1}$ using the hop attention $d_{1}\rightarrow\,d_{3}$ .When multiple TransformerXH layers are stacked, this information in $d_{1}$ includes both $d_{1}$ 's local contexts from its in-sequence attention, and cross-sequence information from the hop attention $d_{2}\rightarrow d_{1}$ of the $l-1$ layer. Hence, an L-layer Transformer-XH can attend over information from up to L hops away.  </p>
<p>Together, three main properties equip Transformer-XH to effectively model raw structured text data: the propagation of information (values) along edges, the importance of that information (hop attention weights), and the balance of in-sequence and cross-sequence information (attention combination). The representations learned in $\mathcal{H}$ can innately express nuances in structured text that are required for complex reasoning tasks such as multi-hop QA and natural language inference.  </p>
<h1>3  APPLICATION TO MULTI-HOP QUESTION ANSWERING</h1>
<p>This section describes how Transformer-XH applies to multi-hop QA. Given a question $q$ ,thetask is to find an answer span $a$ in a large open-domain document corpus, e.g. the first paragraph of all Wikipedia pages. By design, the questions are complex and often require information from multiple documents to answer. For example, in the case shown in Figure 1b, the correct answer "Cambridge” requires combining the information from both the Wikipedia pages “Facebook” and "Harvard University". To apply Transformer-XH in the open domain multi-hop QA task, we first construct an evidence graph and then apply Transformer-XH on the graph to find the answer.  </p>
<p>Evidence Graph Construction.  The first step is to find the relevant candidate documents $D$ for the question $q$ and connect them with edges $E$ to form the graph $G$ . Our set $D$ consists of three sources. The first two sources are from canonical information retrieval and entity linking techniques:  </p>
<p>$D_{i r}$ : the top 100 documents retrieved by DrQA's TF-IDF on the question (Chen et al., 2017).  </p>
<p>$D_{e l}$ : the Wikipedia documents associated with the entities that appear in the question, annotated by entity linking systems: TagMe (Ferragina &amp; Scaiella, 2010) and CMNS (Hasibi et al., 2017).  </p>
<p>For better retrieval quality, we use a BERT ranker (Nogueira &amp; Cho, 2019) on the set $D_{i r}\cup D_{e l}$ and keep the top two ranked ones in $D_{i r}$ and top one per question entity in $D_{e l}$ . Then the third source $D_{e x p}$ includes all documents connected to or from any top ranked documents via Wikipedia hyperlinks (e.g., "Facebook" $\rightarrow$ "Harvard University").  </p>
<p>The final graph comprises all documents from the three sources as nodes $\mathcal{X}$ . The edge matrix $E$ is flexible. We experiment with various edge matrix settings, including directed edges along Wikipedia links, i.e. $e_{i j}=1$ if there is a hyperlink from document $i$ to $j$ , bidirectional edges along Wiki links, and fully-connected graphs, which rely on Transformer-XH to learns the edge importance.  </p>
<p>Similar to previous work (Ding et al., 2019), the textual representation for each node in the graph is the [SEP]-delimited concatenation of the question, anchor text (the text in the hyperlink in parent nodes pointing to the child node), and the paragraph itself. More details on the evidence graph construction are in Appendix A.1.  </p>
<p>Transformer-XH on Evidence Graph.β Transformer-XH takes the input nodes $\mathcal{X}$ and edges $E$ and produces the global representation of all text sequences:  </p>
<p>$$
\mathcal{H}^{L}=\mathrm{Transformer}–\mathrm{XH}(\mathcal{X},E).
$$  </p>
<p>Then we add two task-specific layers upon the last layer's representation $\mathcal{H}^{L}$ : one auxiliary layer to predict the relevance score of the evidence node, and one layer to extract the answer span within it:  </p>
<p>$$
\begin{array}{r l}&amp;{p(\mathrm{relevance}|\tau)=\mathrm{softmax}(\mathrm{Linear}(\tilde{h}_{\tau,0}^{L}));}\ &amp;{p(\mathrm{start}|\tau,i),p(\mathrm{end}|\tau,j)=\mathrm{softmax}(\mathrm{Linear}(\tilde{h}_{\tau,i}^{L})),\mathrm{softmax}(\mathrm{Linear}(\tilde{h}_{\tau,j}^{L})).}\end{array}
$$  </p>
<p>The final model is trained end-to-end with cross-entropy loss for both tasks in a multi-task setting. During inference, we first select the document with the highest relevance score, and then the start and end positions of the answer within that document.  </p>
<h1>4 APPLICATION TO FACT VERIFICATION</h1>
<p>This section describes how Transformer-XH applies to the fact verification task in FEVER Thorne et al. (2018). Given a claim and a trustworthy background corpus, i.e. Wikipedia, the task is to verify whether the evidence in the corpus SUPPORTS, REFUTES, or there is NOT ENOUGH INFO to verify the claim. Similar to multi-hop QA, the first step is to construct an evidence graph using the text pieces in the background corpus and then Transformer-XH can be easily applied to conduct reasoning on these evidence pieces.  </p>
<p>Evidence Graph Construction. Many previous FEVER systems first retrieve the evidence sentences for the claim and then reason verify it (Nie et al., 2019; Zhou et al., 2019). This first step is similar as the retrieval stage in Hotpot QA. And the second step is a multi-evidence reasoning task, where Transformer-XH is applied.  </p>
<p>We keep the evidence sentence retrieval step consistent with previous methods. The sentence retrieval results of SR-MRS is not yet released at the time of our experiments, thus we instead use the BERT-based retrieval results from another contemporary work (Liu et al., 2019b).  </p>
<p>We construct the evidence graph using the top five sentences from Liu et al. (2019b) as the nodes $\mathcal{X}$ and fullyconnected edges $E$ . Following Liu et al. (2019b), the representation of each node is the concatenation of the claim, the Wikipedia title (entity name) of the document that includes the sentence, and the evidence sentence.  </p>
<p>Transformer-XH on Evidence Graph. Transformer-XH takes the evidence graph ${X,E}$ and learns to verify the claim to three categories: $y\,\in\,{\mathrm{SUPPORT}$ , REFUSE, NOT ENOUGH EVIDENCE}. Similar to the application in Hotpot QA, it first produces the global representation of the graph:  </p>
<p>$$
\mathcal{H}^{L}=\mathrm{Transformer}–\mathrm{XH}(\mathcal{X},E).
$$  </p>
<p>Then two task-specific layers are added upon the last layer. The first layer conducts the fact prediction per node using the “[CLS]’ token:  </p>
<p>$$
p(y|\tau)=\mathrm{softmax}(\mathrm{Linear}(\tilde{h}_{\tau,0}^{L})).
$$  </p>
<p>The second layer learns to measure the importance of each node in the graph:  </p>
<p>$$
p(s|\tau)=\mathrm{softmax}(\mathrm{Linear}(\tilde{h}_{\tau,0}^{L})),
$$  </p>
<p>The node level predictions and node importance are combined to the final prediction for the claim:  </p>
<p>$$
p(y|\mathcal{X},E)=\sum_{\tau}p(s|\tau)\cdot p(y|\tau).
$$  </p>
<p>Similar to the Hotpot QA scenario, we use multi-task learning that combines the node prediction task and the claim verification task. The first task uses the evidence sentence label provided by FEVER and cross-entropy loss on Eqn. 15. The second task uses the final verification label with cross-entropy loss on Eqn. 16.  </p>
<h1>5  EXPERIMENTAL METHODOLOGIES</h1>
<p>Our experiments are conducted on Hotpot QA, the multi-hop question answering benchmark Yang et al. (2018), and FEVER, the fact verfication benchmark Thorne et al. (2018).  </p>
<p>Dataset.HotpotQA includes $112\mathbf{k}$ crowd-sourced questions designed to require multiple pieces of textual evidence, which are the first paragraphs of Wikipedia pages. It has two type of questions: bridge question require hopping via an outside entity, and comparison question compare a property of two entities. There are two settings in HotpotQA. The Distractor setting provides golden evidence paragraphs together with TF-IDF retrieved negatives. The FullWiki setting requires systems to retrieve evidence paragraphs from the full set of Wikipedia articles.  </p>
<p>We focus on FullWiki setting since previous research found that the negative documents in Distractor may be too weak and mitigate the need for multi-hop reasoning (Min et al., 2019b). There are 90k Train, 7k Dev and 7k Test questions. The ground truth answer and supporting evidence sentences in Train and Dev sets are provided. Test labels are hidden; only one submission is allowed to the leaderboardper $30\;\mathrm{days}^{2}$ . We evaluate our final model on Test and conduct ablations on Dev.  </p>
<p>Metrics. We use official evaluation metrics of HotpotQA: exact match (EM) and F1 on answer (Ans), supporting facts (Supp), and the combination (Joint). The supporting facts prediction is an auxiliary task that evaluates model's ability to find the evidence sentences. Joint EM is the product of the two EM result. Joint F1 first multiplies the precision and recall from Ans and Supp, then combines the Joint precision and recall to F1.  </p>
<p>Baseline. The main baselines include Cognitive QA (CogQA, Ding et al. (2019)) and Semantic Retrieval MRS (SR-MRS, Nie et al. (2019)). CogQA uses several fine-tuned BERT machine reading comprehension (MRC) models to find hop entities and candidate spans, and then uses a BERT based Graph Convolution Network to rank the candidate spans. SR-MRS is a contemporary work and was the previous leaderboard rank one. It is a BERT based pipeline and uses fine-tuned BERT models to first rank the documents (twice), then to rank sentences to find supporting facts, and finally conducts BERT MRC on the concatenated evidence sentences.  </p>
<p>We also re-implement CogQA and upgrade its IR with our BERT IR model (BERT on $D_{i r}\cup D_{e l}$ same as Transformer-XH), for fair comparisons. We include other approaches on the FullWiki setting: Official Baseline (Yang et al., 2018), MUPPET (Feldman &amp; El-Yaniv, 2019), QFE (Nishida et al., 2019), and DecompRC (Min et al., 2019a),  </p>
<p>Implementation Details.  The in-sequence attention and other standard Transformer components in Transformer-XH are initialized by the pre-trained BERT base model (Devlin et al., 2019). The extra hop attention parameters are initialized randomly and trained from scratch. The final model uses three hop steps. For bridge questions, we build the evidence graph described in Section 3. And for comparison questions, we build the fully-connected graph on the set $D_{i r}\cup D_{e l}$ and train Transformer-XH separately. We leave more implementation details in the Appendix.  </p>
<h1>5.2 FACT VERIFICATION ON FEVER</h1>
<p>Dataset. The FEVER task provides a claim sentence and requires the system to classify it into three categories: SUPPORTS, REFUTES, and NOT ENOUGH INFO, using the Wikipedia corpus as the evidence source. It provides 185,455 claims with manual labels and uses the Wikipedia dump in June 2017 which includes 5.4 million documents.  </p>
<p>Metrics. There are two official evaluation metrics in FEVER: Label Accuracy (LA), which evaluates the classification accuracy of the verification labels, and FEVER Score, which evaluates both the correctness of the evidence sentences used in verification and the LA. The latter is close to Joint EM in Hotpot QA and is the main metric. We use the official evaluation scripts from FEVER task and we refer to Thorne et al. (2018) for more details of this task.  </p>
<p>Experimental Setups. We follow the experiment settings used by previous research in FEVER 1.0 shared task, i.e. Nie et al. (2019), Zhou et al. (2019), and Liu et al. (2019b). Similar as Liu et al. (2019b), we also split the data into single and multi evidence categories and evaluate TransformerXH on the two splits.  </p>
<p><img alt="" src="images/4eb7f5a9378a8909cc969c173c43bc289c789de298091b61fe1818733b2a787d.jpg" />  </p>
<p>Table 1: Results $(\%)$ on HotpotQA FullWiki Setting. Dev results of previous methods are reported in their papers. Test results are from the leaderboard. Contemporary method is marked by *. <br />
Table 2:Dev Ans $(\%)$ on different scenarios. Reasoning Types are estimated by Min et al. (2019b) via whether single-hop BERT has non-zero Ans F1. The numbers of questions are shown in brackets.   </p>
<p><img alt="" src="images/fde9d8092565559497f662f47453670e48aad6a40ecb69bb47ed31c45a69ad50.jpg" />  </p>
<p>Baselines. The baselines include GEAR (Zhou et al., 2019) and two contemporary work, SRMRS (Nie et al., 2019) and KGAT (Liu et al., 2019b). SR-MRS uses similar adaptations as Transformer-XH from Hotpot QA to FEVER. GEAR is a graph attention network based approach specially designed for fact verification. KGAT further improves GEAR's GAT by adding the kernel information, and is the previous STOA with BERT base. We also include the BERT Concat baseline Liu et al. (2019b) which concatenates the evidence sentences to a text sequence and applies BERT on it.  </p>
<p>Implementation Details. We use the retrieval result from Liu et al. (2019b) and connect all sentences as a fully connected graph. We follow similar parameter settings as Hothot QA. We use pre-trained BERT base model to initialize the Transformer components. The extra hop attention parameters are initialized randomly and trained from scratch, and three hop steps are used. We train Transformer-XH for two epochs.  </p>
<h1>6 EVALUATION RESULTS</h1>
<p>This section first presents the evaluation results on HotpotQA and FEVER. Then it conducts ablation studies, analyses, and case studies on HotpotQA to understand the effectiveness of Transformer-XH.  </p>
<h1>6.1 OVERALL RESULT</h1>
<p>HotpotQA FullWiki results are presented in Table 1. Transformer-XH outperforms all previous methods by significant margins. Besides strong results, Transformer-XH's ability to natively represent structured data leads to much simpler QA system. Previously, in order to utilize pre-trained BERT, Hotpot QA approaches adapted the multi-hop reasoning task to comprise multiple sub-tasks. For example, given the retrieved documents, CogQA (w. BERT IR) first leverages one BERT MRC model to find hop entities and then another BERT MRC to find candidate answer spans. After that, it ranks the candidate spans using a BERT based GAT, which is the only structure modeling step. In comparison, Transformer-XH is a unified model which directly represents structured texts and integratesBERT weights.  </p>
<p><img alt="" src="images/20572708b100188f8b6b59b22905a4df2ac094930211c70f90b467ad33ebb832.jpg" /><br />
Table 3: FEVER Results. Contemporary work is marked by *. Single and Multi Evidence are results on Dev claims on which one or multiple sentences are labeled as evidence.  </p>
<p>Table 2 further inspects model performances on the Dev set by question types and reasoning types. Transformer-XH significantly outperforms all baselines on bridge questions which require more multi-hop reasoning. And on the “multi-hop” questions, Transformer-XH has higher relative gains $39\%$ Over CogQA on EM) than the “single-hop" questions $(27\%)$ , demonstrating its stronger multihop reasoning capability. We further study this in Section 6.3.  </p>
<p>To further investigate the reasoning ability of Transformer-XH, we replace our retrieval pipeline with the top retrieved documents from the SR-MRS pipeline. More specifically, we use the top retrieved documents from SR-MRS to construct Transformer-XH's evidence graph while keeping all else constant. The resulting system, Transformer-XH (w. SR-MRS), outperforms SR-MRS's multi-step BERT based reasoning on all metrics and question types. Transformer-XH's effectiveness is robust with multiple IR systems.  </p>
<p>FEVER fact verification results are shown in Table 3. Transformer-XH outperforms SR-MRS by 4 FEVER score on Dev and 1.8 on Test. It performs on par with KGAT. More importantly, Transformer-XH excels at verifying claims that require multiple pieces of evidence-outperforming the contemporary work KGAT by 20 FEVER scores on the multi-evidence claims, a $49\%$ relative improvement. Compared to KGAT, Transformer-XH mainly loses on the "not enough evidence" category which is neither single nor multi evidence. This is an artifact the FEVER task which our system is not specifically designed for.  </p>
<p>This result also demonstrates Transformer-XH's generality on tasks with multiple text inputs not in sequential formats. The only difference between Transformer-XH when applied on multi-hop QA and FEVER is the last (linear) task specific layer; it provides similar or better performances over contemporary approaches that were specifically designed for the fact verification task. Due to space constraints and the consistent effectiveness of Transformer-XH on the two applications, the rest experiments mainly used HotpotQA to analyze the behavior of Transformer-XH.  </p>
<h1>6.2ABLATION STUDIES</h1>
<p>Model Variations. We show the results of different model variations on the top left of Table 4. Single-Hop BERT uses BERT MRC model on each document individually, which significantly decreases the accuracy, confirming the importance of multi-hop reasoning in FullWiki setting (Min et al.,2019a). $G A T+B E R T$ first uses Graph Attention Network (Velickovic et al., 2018) on the evidence graph to predict the best node; then it uses BERT MRC on the best document. It is $10\%$ worse than Transformer-XH since the MRC model has no access to the information from other documents. No Node Prediction eliminates the node prediction task and only trains on span prediction task; the accuracy difference shows node prediction task helps the model training.  </p>
<p>Graph Structures. We show Transformer-XH's performance with different graph structures on the bottom left of Table 4. Bidirectional Edges adds reverse edges along the hyperlinks; Fully Connected Graph connects all document pairs; Node Sequence randomly permutes the documents and connects them into a sequence to simulate the Transformer-XL setting. Both Bidirectional Links and Fully Connected Graph have comparable performance with the original graph structure. Transformer-XH is able to learn meaningful connections using its hop attentions and is less dependent on the pre-existing graph structural. The fully connected graph can be used if there is no strong edge patterns available in the task. However, the performance drops significantly on Node Sequence, showing that structured texts cannot be treated as a linear sequence which cuts off many connections.  </p>
<p><img alt="" src="images/6083cf4adf04c0c13c492a8a45cc87688497c3cb000ffa7374f83f686e0d3e1a.jpg" /><br />
Table 4: Ablation studies on the bridge questions on Dev answer accuracy $(\%)$ , including model components (top left), graph structures (bottom left), and hop steps (right). Transformer-XH's full model uses three hop step and unidirectional Wiki link graph.  </p>
<p><img alt="" src="images/353274a12455c1232dbd4ebf988bfc133e576e0771fad78766c0cfa4a0148c8d.jpg" /><br />
Figure 2: Distributions of learned attention weights of three hops on three groups: From All (Node) $\rightarrow$ (to) All, $\mathrm{All}\rightarrow\mathrm{(to)}$ Ans (ground truth answer node), and Supp (nodes with the supporting facts) $\rightarrow$ (to) Ans. X-axes are attention values scaled by number of nodes.  </p>
<p>Hop Steps. Recall that a Transformer-XH layer with extra hop attention corresponds to one information propagation (hop) step in the graph. Thus Transformer-XH with last K layers conducts K-step attention hops in the graph. We show results with different K on the right side of Table 4. Transformer-XH reaches its peak performance with three hops (our full-model). This is expected as most Hotpot QA questions can be answered by two documents (Yang et al., 2018).  </p>
<h1>6.3 HOP ATTENTION ANALYSIS</h1>
<p>This experiment analyzes the hop attentions using our full-model (three-hop) on the fully connected graph to study their behavior without pre-defined structure. Figure 2 plots the distributions of the learned hop attentions on the Dev set. It shows a strong shift away from the normal distribution with more hops. Transformer-XH learns to distinguish different nodes after multi-hop attention: the attention score becomes a bimodal distribution after three hops, ignoring some non-useful nodes. Transformer-XH also learns to focus on meaningful edges: the score is higher on the path Supp $\rightarrow$ Ans thanAll $\rightarrow.$ Ans. And the margin is larger as the hop step increases from one to three.  </p>
<h1>6.4 CASE STUDY</h1>
<p>Table 5 lists two examples from Transformer-XH and CogQA (w. BERT IR). The first case has a clear evidence chain“2011/S/S" $\rightarrow$ "Winner' $\rightarrow$ "YG Entertainment'; both methods find the correct answer. However, the second case has too many distractors in the first document. Without additional clues from document 2, it is likely that the single-hop hop entity extraction component in CogQA (w. BERT IR) misses the correct answer document in its candidate sets; and the later structural reasoning component can not recover from this cascade error. In comparison, Transformer-XH finds the correct answer by combining the evidence with the hop attentions between the two evidence pieces. We leave more positive and negative cases in Appendix A.5.  </p>
<p>Table 5: Examples of Transformer-XH and BERT pipeline results in Hotpot QA   </p>
<p><img alt="" src="images/1f95a27b74c8201dbadc64a2b0485b8bb4d656ddb0d5f432f084e4f2078831f4.jpg" />  </p>
<h1>7 RELATED WORK</h1>
<p>HotpotQA's FullWiki task is a combination of open-domain QA (Chen et al., 2017) and multi-hop QA (Yang et al., 2018): the questions are designed to require multiple pieces of evidence and these evidence pieces are documents to retrieve from Wikipedia. It is a challenging combination: The retrieved documents are inevitably noisy and include much stronger distractors than the TF-IDF retrieved documents in the Distractor setting (Min et al., 2019a; Jiang &amp; Bansal, 2019).  </p>
<p>Various solutions have been proposed for Hotpot QA (Min et al., 2019b; Feldman &amp; El- Yaniv, 2019; Nishida et al., 2019). These solutions often use complicated pipelines to adapt the multi-hop task into a combination of single-hop tasks, in order to leverage the advantage of pre-trained models. For example, CogQA (Ding et al., 2019) uses two BERT based MRC model to find candidate spans and then another BERT initialized Graph Neural Network (GNN) to rank spans; SR-MRS (Nie et al., 2019) uses three BERT based rankers to find supporting sentences, and then another BERT MRC model on the concatenated sentences to get the answer span. Transformer-XH is a simpler model that directly represents and reasons with multiple pieces of evidence using extra hop attentions.  </p>
<p>Fact verification is a natural language inference task while also requires retrieving (*"open-domain") and reasoning with multiple text pieces ("multi-evidence") (Thorne et al., 2018; Nie et al., 2019; Liu et al., 2019b). Many recent FEVER systems leverage Graph Neural Networks to combine information from multiple text nodes, while each node text is represented by BERT encodings (Zhou et al., 2019; Liu et al., 2019b). Transformer-XH is a more unified solution that simply includes language modeling as part of its joint reasoning.  </p>
<p>In addition to Transformer-XL (Dai et al., 2019), other work is proposed to improve the Transformer architecture on long text sequence. For example, T-DMCA (Liu et al., 2018) splits the sequence into blocks and then the attention merges different blocks. Sparse Transformer (Child et al., 2019) introduces the sparse factorizations of the attention matrix. Transformer-XH shares similar motivation and focuses on multiple pieces of text that are not in sequential forms.  </p>
<p>Transformer-XH is also inspired by GNN (Kipf &amp; Welling, 2017; Schlichtkrull et al., 2017; Velickovic et al., 2018), which leverages neural networks to model graph structured data for downstream tasks (Sun et al., 2018; Zhao et al., 2020). The key difference is that a “node" in TransformerXH is a text sequence, and modeling of the structure is conducted jointly with the representation of the text. Transformer-XH combines the Transformer's advantages in understanding text with the power that GNN has in modeling structure.  </p>
<h1>8 CONCLUSION</h1>
<p>Transformer-XH and its eXtra Hop attention mechanism is a simple yet powerful adaptation of Transformer to learn better representations of structured text data as it naturally occurs. It innately integrates with pre-trained language models to allow for complex reasoning across multiple textual evidence pieces. When applied to HotpotQA, Transformer-XH significantly shrinks the typical multi-hop QA pipeline, eliminating many cascading errors that arise from the linear sequence input constraints of pre-trained Transformers. The same simplicity also applies to FEVER, with one Transformer-XH all we needed to obtain a much stronger answer accuracy. With its simplicity and efficacy, we envision Transformer-XH will benefit many applications in the near future.  </p>
<h1>REFERENCES</h1>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer OpenDomain Questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pp. 1870-1879, 2017. <br />
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating Long Sequences with Sparse Transformers. arXiv preprint arXiv: 1904.10509, 2019. <br />
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive Language Models beyond a Fixed-Length Context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978-2988, 2019. <br />
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Ppp. 4171-4186, 2019. <br />
Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. Cognitive Graph for Multi-Hop Reading Comprehension at Scale. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2694-2703, 2019. <br />
Yair Feldman and Ran El- Yaniv. Multi-Hop Paragraph Retrieval for Open-Domain Question Answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2296-2309, 2019. <br />
Paolo Ferragina and Ugo Scaiella. Tagme: On-the-fly Annotation of Short Text Fragments (by Wikipedia Entities). In Proceedings of the 19th ACM international conference on Information and knowledge management, pp. 1625-1628, 2010. <br />
Faegheh Hasibi, Krisztian Balog, and Svein Erik Bratsberg. Entity Linking in Queries: Effciency vs.Effectiveness. In European Conference on Information Retrieval, pp. 40-53, 2017. <br />
Yichen Jiang and Mohit Bansal. Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development for Multi-Hop QA. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2726-2736, 2019. <br />
Thomas N Kipf and Max Welling. Semi-supervised Classification with Graph Convolutional Networks. In International Conference on Learning Representations, 2017. <br />
Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating Wikipedia by Summarizing Long Sequences. In International Conference onLearningRepresentations,2018. <br />
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv: 1907.11692, 2019a. <br />
Zhenghao Liu, Chenyan Xiong, and Maosong Sun. Kernel Graph Attention Network for Fact Verification. arXiv preprint arXiv: 1910.09796, 2019b. <br />
Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. Compositional Questions Do Not Necessitate Multi-hop Reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4249-4257, 2019a. <br />
Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. Multi-hop Reading Comprehension through Question Decomposition and Rescoring. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6097-6109, 2019b. <br />
Yixin Nie, Songhe Wang, and Mohit Bansal. Revealing the Importance of Semantic Retrieval for Machine Reading at Scale. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, 2019. <br />
Kosuke Nishida, Kyosuke Nishida, Masaaki Nagata, Atsushi Otsuka, Itsumi Saito, Hisako Asano, and Junji Tomita. Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2335-2345, 2019. <br />
Rodrigo Nogueira and Kyunghyun Cho.  Passage Re-ranking with BERT.  arXiv preprint arXiv: 1901.04085, 2019. <br />
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling Relational Data with Graph Convolutional Networks. arXiv preprint arXiv:1703.06103, 2017. <br />
Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William Cohen. Open domain question answering using early fusion of knowledge bases and text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Pp. 4231-4242, 2018. <br />
James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos, and Arpit Mittal. The Fact Extraction and VERification (FEVER) Shared Task. In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pp. 1-9, 2018. <br />
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is All You Need. In Advances in neural information processing systems, pp. 5998-6008, 2017. <br />
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph Attention Networks. In International Conference on Learning Representations, 2018. <br />
Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou, Qi Huang, Chao Ma, Ziyue Huang, Qipeng Guo, Hao Zhang, Haibin Lin, Junbo Zhao, Jinyang Li, Alexander J Smola, and Zheng Zhang. Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. <br />
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Pp. 2369-2380, 2018. <br />
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet: Generalized Autoregressive Pretraining for Language Understanding. In Advances in Neural Information Processing Systems, pp. 5754-5764, 2019. <br />
Chen Zhao, Chenyan Xiong, Xin Qian, and Jordan Boyd-Graber. Complex factoid question answering with a free-text knowledge graph. In The Web Conference, 2020. <br />
Jie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 892-901, 2019.  </p>
<h1>A APPENDIX</h1>
<p>The appendix includes details of the evidence graph construction for Hotpot QA, ablation studies in the BERT IR component, more details and results on Hotpot QA.  </p>
<h1>A.1 HOTPOTQA EVIDENCE GRAPH CONSTRUCTION DETAILS</h1>
<p>The evidence graph construction includes two stages. The first stage is BERT IR, which extract related documents directly from question. The second stage expands the related documents along Wikipedia links. The first is applied on all questions while the second is only required by Bridge questions.  </p>
<p>The first stage uses two methods to find documents. The first method uses DrQA's retrieval system (Chen et al., 2017), which is unsupervised TF-IDF. We keep the top $100~\mathrm{DrQA}$ retrieved Ones $D_{i r}$ for each question. The second method uses TagMe (Ferragina &amp; Scaiella, 2010) and CMNS (Hasibi et al., 2017), two commonly used entity linkers, to annotate questions. We keep the TagMe output entity and three highest scored entities per surface form (a phrase in the question linked with entities) from CMNS, and use its corresponding Wikipedia document as $D_{e l}$  </p>
<p>We use BERT ranker (Nogueira &amp; Cho, 2019) to re-rank the initial set $D_{i r}\cup D_{e l}$ . The input to the BERT is the concatenation of question and first paragraph of document:  </p>
<p>Then a linear layer is added on the last layer's [CLS] representation to score the relevance of the document. We use BERT base and fine-tune it using the relevance label (from supporting facts) with cross-entropy loss. The top two highest scored documents from $D_{i r}$ and the top one document per entity position (surface form) in $D_{e l}$ are kept as the first stage BERT IR documents.  </p>
<p>The second stage expands the first stage BERT IR documents by Wikipedia hyperlinks to obtain $D_{e x p}$ . A document is included if it is linked to or links to a document in the first stage. We use the same BERT ranker to rank $D_{e x p}$ and keep the top 15 documents in $D_{e x p}$  </p>
<p>The final evidence graph nodes per question includes the top two highest ranked documents in $D_{i r}$ top one per entity name in $D_{e l}$ , and top 15 from the expanded documents $D_{e x p}$  </p>
<p>The comparison questions only require information from two question entities; thus when building the evidence graph we do not expand them (i.e. there is no $D_{e x p.}$  </p>
<p>The retrieval pipeline is a multi-stage retrieval enhanced with entity linking. It is close to the retrieval system used in SR-MRS (Nie et al., 2019). When using SR-MRS retrieved documents for documents, we use top 10 documents on bridge questions and top two documents on comparison questions.  </p>
<p>In the next section, we show that Transformer-XH is robust to different number of documents kept in the evidence graph and performs similarly using the documents retrieved from SR-MRS.  </p>
<h1>A.2 ABLATION STUDIES ON DOCUMENT RETRIEVAL</h1>
<p>This experiment studies the effectiveness and influence of different retrieval settings. We use different numbers of top K ranked documents from the BERT ranker, run Transformer-XH in the corresponding evidence graph, and evaluate its performance on Bridge questions in the Dev set. We also evaluate the Supporting facts Recall and Answer Recall. Supp Recall evaluates whether the document with the supporting fact is included in the first stage retrieved documents. Ans Recall evaluates whether their exists a document in the evidence graph that includes the ground truth answer. The results are in Table 6.  </p>
<p>Our BERT IR system performs better than CogQA's TF-IDF and on par with SR-MRS, as expected. The latter uses a similar retrieval pipeline with our BERT IR system; Transformer-XH is robust on different retrieval settings and keeps its effectiveness when applied on Top 10 documents from SR-MRS (including both stages).  </p>
<p><img alt="" src="images/cc4f3df378fe0f27e7af1b67c783ee2a8ad144f325c8299cd7fad71f31450ed8.jpg" />  </p>
<p>Table 6: Ablation study on the retrieval systems. Top-10 TFIDF is the one used by $\mathrm{CogQA}$ (Ding et al., 2019). BERT IR is the retrieval system used by CogQA (w. BERT IR) and Transformer-XH. Top K refers to using the 2/5/10 highest ranked documents from the BERT ranker in the first stage. SR-MRS Top 10 uses the 10 retrieved documents per question provided by Nie et al. (2019). All retrieval methods include entities linked in the question and are expanded along Wiki links, except when evaluating the Ist stage Supp Recall.  </p>
<h1>A.3 OTHER HOTPOT QA COMPONENTS</h1>
<p>This section describes the other components for HotpotQA dataset. The whole QA system starts with question type classification. We train transformer-XH separately on each type of questions over their evidence graph. Besides answer prediction, we also adopt BERT based model for predicting supportingsentences.  </p>
<h1>A.3.1QUESTION CLASSIFICATION</h1>
<p>The first component of our system is to classify the question to bridge and comparison types. We adopt BERT classification fine-tuning setting on HotpotQA questions using the question type labels provided in HotpotQA. The classifier achieves $99.1\%$ accuracy on the dev set. We use the classifier to split the questions into Comparison and Bridge.  </p>
<h1>A.3.2 SUPPORTING FACTS CLASSIFICATION</h1>
<p>The supporting facts prediction task is to extract all sentences that help get the answer. For bridge question, these sentences usually cover different pieces of questions. And for comparison questions, the supporting facts are the properties of two question entities. We design one model architecture for this task, but we train two models on each type to reflect the inherent difference.  </p>
<p>We use BERT as our base model and on top of BERT, we conduct multi-task learning scheme. The first task is document relevance prediction, similar as Transformer-XH, we add a linear layer on the [CLS] token of BERT to predict the relevance score. The other task is sentence binary classification, we concatenate the first and last token representation of each sentence in the document through a linear layer, the binary output decides whether this sentence is supporting sentence.  </p>
<p>Bridge question supporting facts prediction  For bridge questions, we predict supporting facts after answer prediction from Transformer-XH to resume the inference chain. We start by predicting supporting facts in the answer document. The other document is chosen from the parents of the answer document in the evidence graph. 3. Compare with the contemporary model Nie et al. (2019), which does not limit the search space along the inference chain (i.e., the answer document may not be relevant to the other supporting page), our method more naturally fits the task purpose.  </p>
<p>Comparison question supporting facts prediction  For comparison questions, after extracting the first step documents $D$ , we simply run this supporting facts prediction model to select the top-2 documents, and predict the corresponding supporting facts.  </p>
<h1>A.3.3 TRAINING DETAILS</h1>
<p>We use DGL (Wang et al., 2019) for implementing Transformer-XH and CogQA (w. BERT IR) with batch size 1 (i.e., one graph for each batch), and keep the other parameters same as default BERT setting. We train Transformer-XH separately on two different types of questions, following previous  </p>
<p><img alt="" src="images/5d839f3284b84319b28a9b043e8451253d98d60833a7baf83dc3163c380c87a9.jpg" /><br />
Table 7: Additional examples for model prediction on HotpotQA dataset, the first example is the correct prediction $(+)$ , the other two examples are the wrong predictions (-).  </p>
<p>research (Ding et al., 2019). We train Transformer-XH and the GNN of CogQA (w. BERT IR) for 2 epochs. All other BERT based models use the default BERT parameters and train the model for 1 epoch.  </p>
<h1>A.4 IMPLEMENTATION DETAILS OF COGQA (W. BERT IR)</h1>
<p>This section discusses our implementation of CogQA (W. BERT IR). We start with the same documents from BERT IR, the one used by Transformer-XH, and then implement the following steps:  </p>
<h1>A.4.1 HOP ENTITY EXTRACTION</h1>
<p>For each document from the previous step, we run BERT MRC model and limit the span candidates as hyperlinked entities for hop entity extraction (e.g., in Figure 1, “Harvard University” is a hop entity). Following Ding et al. (2019), we predict the top three entities that above the relative threshold that is the start span probability of [CLS] position.  </p>
<h1>A.4.2 ANSWER SPAN EXTRACTION</h1>
<p>For each document (add the hop entity document), following Ding et al. (2019), we run BERT MRC model to extract spans (e.g., "Combridge”" in Figure 1.). We predict the top one span that above the threshold that is the start span probability of [CLS] position.  </p>
<p>We train both hop entity extraction and span extraction tasks with same BERT model but different prediction layers. For each training example, we extract the link between two given supporting pages. The page includes the link (e.g., “Harvard University" in Figure 1.) is the supporting page  </p>
<p>for hop entity extraction, while the other page is the answer page (e.g., “Combridge” in Figure 1.) for answer span extraction.  </p>
<h1>A.4.3 GAT MODELING</h1>
<p>All the entities and answer spans form the final graph. The nodes are the entities and spans, and edges are the connections from the entities to the extracted hop entities or spans.  </p>
<p>We use BERT for each node representation with question, anchor sentences and context, following Ding et al. (2019). We run GAT (Velickovic et al., 2018) on top of BERT to predict the correct answer span node.  </p>
<p>A.4.4 COMPARISON QUESTIONS  </p>
<p>After predicting supporting facts, we concatenate the sentences and follow Min et al. (2019b) to run a BERT MRC model to predict either span or yes/no as the answer.  </p>
<h1>A.5 ADDITIONAL CASE STUDY</h1>
<p>We provide addition case studies in Table 7. The first case can be directly predicted through the clear evidence chain "'the 1925 Birthday Honours" $\rightarrow$ 'George $\mathrm{V}^{\circ}{\rightarrow}^{\prime}1865^{\circ}$ . In the second case, the first document ('Algeria at the FIFA World Cup") has no link to any other documents, therefore the model can not access the correct answer. The third case is more reading comprehension oriented, where the model can not distinguish the correct and wrong spans inside one sentence.  </p>
    </body>
    </html>