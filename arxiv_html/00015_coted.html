<!doctype html>
    <html>
    <head>
        <meta charset="UTF-8">
        <link rel="stylesheet" media="all" href="normalize.css">
        <link rel="stylesheet" media="all" href="core.css">
        <link rel="stylesheet" media="all" href="style.css">
        <script type="text/javascript"
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
    
        <script type="text/javascript">
          MathJax.Hub.Config({
            tex2jax: {
              inlineMath: [['$', '$'], ['\(', '\)']],
              displayMath: [['$$', '$$'], ['\[', '\]']],
              processEscapes: true
            }
          });
        </script>
        
</head>
    <body>
    <h1>CROSS-DIMENSIONAL SELF-ATTENTION FOR MULTIVARIATE, GEO-TAGGED TIME SERIES IMPUTATION</h1>
<p>Anonymous authors Paper under double-blind review  </p>
<h1>ABSTRACT</h1>
<p>Many real-world applications involve multivariate, geo-tagged time series data: at each location, multiple sensors record corresponding measurements. For example, air quality monitoring system records PM2.5, CO, etc. The resulting time-series data often has missing values due to device outages or communication errors. In order to impute the missing values, state-of-the-art methods are built on Recurrent Neural Networks (RNN), which process each time stamp sequentially, prohibiting the direct modeling of the relationship between distant time stamps. Recently, the self-attention mechanism has been proposed for sequence modeling tasks such as machine translation, significantly outperforming RNN because the relationship between each two time stamps can be modeled explicitly. In this paper, we are the first to adapt the self-attention mechanism for multivariate, geo-tagged time series data. In order to jointly capture the self-attention across different dimensions (i.e. time, location and sensor measurements) while keep the size of attention maps reasonable, we propose a novel approach called Cross-Dimensional Self-Attention (CDSA) to process each dimension sequentially, yet in an order-independent manner. On three real-world datasets, including one our newly collected NYCtraffic dataset, extensive experiments demonstrate the superiority of our approach compared to state-of-the-art methods for both imputation and forecasting tasks.  </p>
<h1>1 INTRODUCTION</h1>
<p>Various monitoring applications, such as those for air quality (Zheng et al. (2015)), health-care (Silva et al. (2012)) and traffic (Jagadish et al. (2014)), widely use networked observation stations to record multivariate, geo-tagged time series data. For example, air quality monitoring systems employ a collection of observation stations at different locations; at each location, multiple sensors concurrently record different measurements such as PM2.5 and CO over time. Such time series are important for advanced investigation and also are useful for future forecasting. However, due to unexpected sensor damages or communication errors, missing data is unavoidable. It is very challenging to impute the missing data because of the diversity of the missing patterns: sometimes almost random while sometimes following various characteristics.  </p>
<p>Traditional data imputation methods usually suffer from imposing strong statistical assumptions. For example, Scharf &amp; Demeure (1991) and Friedman et al. (2001) fit a smooth curve on observations in either time series (Ansley &amp; Kohn (1984); Shumway &amp; Stoffer (1982)) or spatial distribution (Friedman et al. (2001); Stein (2012)). Deep learning methods (Li et al. (2018); Che et al. (2018); Cao et al. (2018); Luo et al. (2018a) have been proposed to capture temporal relationship based on RNN (Cho et al. (2014b); Hochreiter &amp; Schmidhuber (1997); Cho et al. (2014a)). However, due to the constraint of sequential computation over time, the training of RNN cannot be parallelized and thus is usually time-consuming. Moreover, the relationship between each two distant time stamps cannot be directly modeled. Recently, the self-attention mechanism as shown in Fig. 1(b) has been proposed by the seminal work of Transformer (Vaswani et al. (2017)) to get rid of the limitation of sequential processing, accelerating the training time substantially and improving the performance significantly on seq-to-seq tasks in Natural Language Processing (NLP) because the relevance between each two time stamps is captured explicitly.  </p>
<p>In this paper, we are the first to adapt the self-attention mechanism to impute missing data in multivariate time series, which cover multiple geo-locations and contain multiple measurements as  </p>
<p><img alt="" src="images/2d5726a3342bf0df1748977b351ae9f9092b31f5a1097220c4f77e5f6f2658ea.jpg" />  </p>
<p>Figure 1: (a) Mlustration of the multivariate, geo-tagged time series imputation task: the input data has three dimensions (i.e. time, location, measurement) with some missing values (indicated by the orange dot); the output is of same shape as the input while the missing values have been imputed (indicated by the red dot). (b) Self-attention mechanism: the Attention Map is first computed using every pair of Query vector and Key vector and then guides the updating of Value vectors via weighted sum to take into account contextual information. (c) Traditional Self-Attention mechanism updates Value vector along the temporal dimension only vs. Cross-Dimensional Self-Attention mechanism updates Value vector according to data across all dimensions.  </p>
<p>shown in Fig. 1(a). In order to impute a missing value in such unique multi-dimensional data, it is very useful to look into available data in different dimensions (i.e. time, location and measurement), as shown in Fig. 1(c), to capture the intra-correlation individually. To this end, we investigate several choices of modeling self-attention across different dimensions. In particular, we propose a novel Cross-Dimensional Self-Attention (CDSA) mechanism to capture the attention crossing all dimension jointly yet in a decomposed manner. In summary, we make the following contributions:  </p>
<p>(i)  We are the first to apply the self-attention mechanism to the multivariate, geo-tagged time series data imputation task, replacing the conventional RNN-based models to speed up training and directly model the relationship between each two data values in the input data. <br />
(ii)  For such unique time series data of multiple dimensions (i.e. time, location, measurement), we comprehensively study several choices of modeling self-attention crossing different dimensions. Our proposed CDSA mechanism models self-attention crossing all dimensions jointly yet in a dimension-wise decomposed way, preventing the size of attention maps from being too large to be tractable. We show that CDSA is independent with the order of processing each dimension. <br />
(i) We extensively evaluate on two standard benchmarks and our newly collected traffic dataset. Experimental results show that our model outperforms the state-of-the-art models for both data imputation and forecasting tasks. We visualize the learned attention weights which validate the capability of CDSA to capture important cross-dimensional relationships.  </p>
<h1>2 RELATED WORK</h1>
<p>Statistical data imputation methods. Statistical methods (Ansley &amp; Kohn (1984); Zhang (2003); Shumway &amp; Stoffer (1982); Nelwamondo et al. (2007); Buuren &amp; Groothuis-Oudshoorn (2010)) often impose assumptions over data and reconstruct the missed value by fitting a smooth curve to the available values. For instance, Kriging variogram model (Stein (2012) was proposed to capture the variance in data w.r.t. the geodesic distance. Matrix completion methods (Acuna &amp; Rodriguez (2004); Yu et al. (2016); Friedman et al. (2001); Cai et al. (2010); Ji &amp; Ye (2009); Ma et al. (2011)) usually enforce low-rank constraint.  </p>
<p><img alt="" src="images/5451deb37f4ad4b965bff56cb8e27c37e1f1f3238f7edf4af5f52f221fc46df6.jpg" /><br />
Figure 2: Three choices of implementing our Cross-Dimensional Self-Attention mechanism  </p>
<p>RNN-based data imputation methods. Li et al. (2018) proposed DCGRU for seq-to-seq by adopting graph convolution (Chung &amp; Graham (1997); Shi (2009); Shuman et al. (2012)) to model spatial-temporal relationship. Luo et al. (2018a) built GRUI by incorporating RNN into a Generative Adversarial Network (GAN). Nevertheless, the spatiotemporal and measurements correlation are mixed and indistinguishable. so that the mediate back propagation from loss of available observation can contribute to the missing value updating. Nevertheless, these RNN-based models fundamentally suffer from the constraint of sequential processing, which leads to long training time and prohibits the direct modeling of the relationship between two distant data values.  </p>
<p>Self-attention. Recently, Vaswani et al. (2017) introduced the Transformer framework which relies on self-attention, learning the association between each two words in a sentence. Then self-attention has been widely applied in seq-to-seq tasks such as machine translation, image generation ( Yang et al. (2016); Zhang et al. (2018a) and graph-structured data (Velickovic et al. (2017)). In this paper, we are the first to apply self-attention for multi-dimensional data imputation and specifically we investigate several choices of modeling self-attention crossing different data dimensions.  </p>
<h1>3 APPROACH</h1>
<p>In Sec. 3.1, we first review the conventional self-attention mechanism in NLP. In Sec. 3.2, we propose three methods for computing attention map cross different dimension. In Sec. 3.3 and 3.4, we present details of using CDSA for missing data imputation.  </p>
<h1>3.1 CONVENTIONAL SELF-ATTENTION</h1>
<p>As shown in Fig. 1(b), for language translation task in NLP, given an input sentence, each word $\pmb{x}_{i}$ is mapped into a Query vector $\pmb q_{i}$ of $d$ -dim, a Key vector $k_{i}$ of $d$ -dim, and a Value vector $\pmb{v}_{i}$ of $v$ -dim. The attention from word $\pmb{x}_{j}$ to word $\pmb{x}_{i}$ is effectively the scaled dot-product of $\pmb q_{i}$ and $k_{j}$ after Softmax, which is defined as $\begin{array}{r}{\mathbf{}A(i,j)=\exp(S(i,j))\big(\sum_{j=1}^{T}\exp(S(q,j))\big)^{-1}}\end{array}$ where Then, $\pmb{v}_{i}$ is updated to $\pmb{v}_{i}^{\prime}$ as a weighted sum of allthe Value vectos, defined as $\begin{array}{r}{\pmb{v}_{i}^{\prime}=\sum_{j=1}^{T}{\pmb{A}}(i,j)\pmb{v}_{j}}\end{array}$ after which each $\pmb{v}_{i}^{\prime}$ is mapped to the layer output $\pmb{x}_{i}^{\prime}$ of the same size as $\pmb{x}_{i}$ . In order to adapt the self-attention from NLP to our multivariate, geo-tagged time series data, a straightforward way is to view all data in a time stamp as one word embedding and model the self-attention over time.  </p>
<h1>3.2CROSS-DIMENSIONAL SELF-ATTENTION</h1>
<p>In order to model Cross-Dimensional Self-Attention (CDSA), in this section we propose three solutions: (1) model attention within each dimension independently and perform late fusion; (2)  </p>
<p>model attention crossing all dimension jointly; (3) model attention crossing all dimension in a joint yet decomposedmannerWe assume thinput $\dot{\boldsymbol{\mathcal{X}}}\in\mathbb{R}^{T\times L\times M}$ has three dimensions corresponding time, location, measurement. $\mathcal{X}$ can be reshaped into 2-D matrices (i.e. $X_{\mathcal{T}}\in\mathbb{R}^{T\times L M}$ ， $\pmb{X}_{\mathcal{L}}^{\pmb{\ L}}\in\mathbb{R}^{\pmb{\breve{L}}\times\boldsymbol{M}T}$ ，， $X_{\mathcal{M}}\in\dot{\mathbb{R}}^{M\times T L})$ or an 1-D vector i.e. $\mathbf{\dot{X}}\in\mathbb{R}^{T L M\times1})$ imilarls iaya the Query, Key and Value, e.g.,. $\mathcal{Q}\in\mathbb{R}^{T\times L\times M\times d}$ ， $\b{Q}_{\mathcal{L}}\in\mathbb{R}^{L\times M\check{T}\dot{d}}$ and $Q\in\mathbb{R}^{\dot{T}L M\dot{\times}d}$  </p>
<h1>3.2.1 INDEPENDENT</h1>
<p>As shown in Fig. 2(a), the input $\mathcal{X}$ is reshaped into three input matrices $X_{T},X_{\mathcal{L}}$ and $X_{\mathcal{M}}$ . Three streams of self-attention layers are built to process each input matrix in parallel. Such as the first layer in stream on $X_{\mathcal{L}}$ , each vector $X_{\mathcal{L}}(l,:)$ of $M T$ -dim is viewed as a word vector in NLP. Following the steps in Sec. 3.1, $X_{\mathcal{L}}(l,:)$ is mapped to $Q_{L}(l,:)$ and $\kappa_{L}(l,:)$ of $d_{L}$ -dim, as well as $V_{L}(l,:)$ of $v_{L}$ -dim. The output of every stream's last layer are fused through element-wise addition, $\chi\prime\,=\,\alpha_{T}\chi_{T}^{\prime}+\alpha_{L}\chi_{L}^{\prime}+\alpha_{M}\chi_{M}^{\prime}$ where the weights $\alpha_{T}$ ， $\alpha_{L}$ and $\alpha_{M}$ are trainable parameters. Besides, the hyper-parameters for each stream such as the number of layers, are set separately.  </p>
<h1>3.2.2JOINT</h1>
<p>As shown in Fig. 2(b), the three-dimensional input $\mathcal{X}$ is reshaped as to $\mathbf{\deltaX}$ .Each unit $X(p)$ is mapped to $Q(p,:)$ and $\;K(p,:)$ of $d$ -dim as well as $V(p,:)$ of $v$ -dim, where $p=p(t,l,m)$ denotes the index mapping from the 3-D cube to the vector form. In this way, an attention map of dimension $T L M\times T L M$ is built to directly model the cross-dimensional interconnection.  </p>
<h1>3.2.3DECOMPOSED</h1>
<p>The Independent manner sets multiple attention sub-layers in each stream to model the dimensionspecific attention but fail in modeling cross-dimensional dependency. In contrast, the Joint manner learns the cross-dimensional relationship between units directly but results in huge computation workload. To capture both the dimension-specific and cross-dimensional attention in a distinguishable way, we propose a novel Decomposed manner.  </p>
<p>As shown in Fig. 2(c), the input $\mathcal{X}$ is reshaped as input matrices $X_{T}$ ， $X_{\mathcal{L}}$ ， $X_{\mathcal{M}}$ and $\mathbf{\deltaX}$ . Each unit $X(p)$ is mapped to vector $V(p,:)$ of $v$ -dim as in the Joint while $X_{T}$ $X_{\mathcal{L}}$ and $X_{\mathcal{M}}$ are used for building attention map $A_{T}$ ， $A_{L},A_{M}$ individually as in the Independent. The attention maps are applied on Value vector in order as,  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We first note that the Decomposed manner involves applying attention maps individually to the input matrices $X_T$, $X_L$, and $X_M$, resulting in attention maps $A_T$, $A_L$, and $A_M$. These attention maps are then applied sequentially to the Value vector $V$, with the output of each attention map serving as the input to the next., 
 before perplexity: 1.3973123750074936, after perplexity: 1.3858156904167613</p>
<p>Completions End </p>
<p></span>$$
V^{\prime}=A V=\tilde{A}_{M}V_{\mathrm{S},2}=\tilde{A}_{M}\tilde{A}_{L}V_{\mathrm{S},1}=\tilde{A}_{M}\tilde{A}_{L}\tilde{A}_{T}V.
$$  </p>
<p>The attention map with _\~ is reshaped from the original attention map and consistent with the calculation in (1), e.g, $\widetilde{A}_{T}\in\mathbb{R}^{T L\bar{M}\times T L M}$ is reshaped from $A_{T}\in\mathbb{R}^{T\times T}$ .More specifically,  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: To derive the equations for the Decomposed manner, we first consider the individual attention maps $A_T$, $A_L$, and $A_M$ that are applied to the value vector $V$ in sequence. We then need to reshape these attention maps to match the dimensions of the value vector, which is a 3D cube. To achieve this, we use the Kronecker product ($\otimes$) to reshape the 2D attention maps into 3D attention maps. Specifically, we reshape the attention map $A_T$ into a 3D attention map $\widetilde{A}_T$ by taking the Kronecker product with the identity matrices $I_L$ and $I_M$, and similarly for $A_L$ and $A_M$. This allows us to apply the attention maps in a sequential manner while maintaining the correct dimensions., 
 before perplexity: 0.4788473054340427, after perplexity: 0.4624411570016165</p>
<p>Completions End </p>
<p></span>$$
\begin{array}{r}{\widetilde{\pmb{A}}_{T}=\pmb{A}_{T}\otimes\pmb{I}_{L}\otimes\pmb{I}_{M},}\ {\widetilde{\pmb{A}}_{L}=\pmb{I}_{T}\otimes\pmb{A}_{L}\otimes\pmb{I}_{M},}\ {\widetilde{\pmb{A}}_{M}=\pmb{I}_{T}\otimes\pmb{I}_{L}\otimes\pmb{A}_{M},}\end{array}
$$  </p>
<p>where $\otimes$ denotes tensor product and $\boldsymbol{\mathit{I}}$ is the Identity matrix where the subscript indicates the size, e.g, $I_{T}\,\in\,\mathbb{R}^{T\times T}$ .Although the three reshaped atention maps are applied with acertain order, according to (2), we show that each unit in $\pmb{A}$ is effectively calculated as  </p>
<p>$$
\begin{array}{r}{\pmb{A}(p_{0},p_{1})=\pmb{A}_{T}(t_{0},t_{1})\pmb{A}_{L}(l_{0},l_{1})\pmb{A}_{M}(m_{0},m_{1}),}\end{array}
$$  </p>
<p>where $p_{0}\,=\,p(t_{0},l_{0},m_{0}),p_{1}\,=\,p(t_{1},l_{1},m_{1})$ . Following the associativity of tensor product, we demonstrate  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We observe that the reshaped attention maps $\widetilde{\mathcal{A}}_{T}$, $\widetilde{\mathcal{A}}_{L}$, and $\widetilde{\mathcal{A}}_{M}$ are applied in a certain order, and we note that the tensor product is associative. By leveraging this property, we can simplify the expression for $\widetilde{\mathcal{A}}_{\sigma(T)}\widetilde{\mathcal{A}}_{\sigma(L)}\widetilde{\mathcal{A}}_{\sigma(M)}$ to obtain the desired result., 
 before perplexity: 1.3469553066026116, after perplexity: 0.9404018966828884</p>
<p>Completions End </p>
<p></span>$$
\widetilde{\cal A}_{\sigma(T)}\widetilde{\cal A}_{\sigma(L)}\widetilde{\cal A}_{\sigma(M)}={\cal A}_{T}\otimes{\cal A}_{L}\otimes{\cal A}_{M},
$$  </p>
<p>where $\sigma=\sigma(\mathrm{T},!\mathrm{L},!\mathrm{M})$ denotes the arbitrary arrangement of sequence $_{(\mathrm{T},\mathrm{L},\mathrm{M})}$ ,e.g., $\sigma=\scriptstyle\left(\mathrm{L},\mathrm{T},\mathrm{M}\right)$ and $\sigma(\boldsymbol{\mathrm{T}})=\boldsymbol{\mathrm{L}}$ . Effectively, the arrangement $\sigma$ is the order of attention maps to update $V$ . As (3)- (4) shows that the weight in $\pmb{A}$ is decomposed as the product of weights in three dimension-specific attention maps, the output and gradient back propagation are order-independent. Furthermore, we show in Supp. B that the cross-dimensional attention map has the following property:  </p>
<p>$$
\sum_{p_{1}=1}^{T L M}A(p_{0},p_{1})=\sum_{t_{1}=1}^{T}\sum_{l_{1}=1}^{L}\sum_{m_{1}=1}^{M}A_{T}(t_{0},t_{1})A_{L}(l_{0},l_{1})A_{M}(m_{0},m_{1})=1.
$$  </p>
<p><img alt="" src="images/8b51c06041b004f48085ca67b80019cdc2234dc446118353abafff6e1178cb6b.jpg" /><br />
Figure 3: The framework employing CDSA for data imputation and forecasting.  </p>
<p>In summary, the Independent builds attention stream for each dimension while the Joint directly model the attention map among all the units. Our proposed CDSA is based on the Decomposed, which forms a cross-dimensional attention map, out of three dimension-specific maps. As an alternative of the Decomposed, the Shared maps unit $X(p)$ to $Q(p,:)$ and $K(p,:)$ of $d_{\cdot}$ -dim and calculates all three dimension-specific attention map, e.g., ${\cal A}_{L}=\mathrm{Softmax}(Q_{\cal C}K_{\mathscr{L}}^{\top}/\sqrt{M T d})$ . As shown in Table 1, by using Tensorflow profile and fixing the hyper-parameters with detailed explanations in Supp., the Decomposed significantly decreases the FLoating point OPerations (FLOPs) compared to the Joint and requires less variables than the Independent. Detailed comparisons are reported in Sec. 4.3.  </p>
<p>Table 1: Computational complexity of several methods to implement CDSA   </p>
<p><img alt="" src="images/e12d8049361d1a34e1e794f160d2cf6fd71a00f2b846840dace565bd40b9a66d.jpg" />  </p>
<h1>3.3FRAMEWORK</h1>
<p>Imputation: As shown in Fig. 3(a), we apply our CDSA mechanism in a Transformer Encoder, a stackof $N=8$ identical layers with residual connection (He et al. (2016)) and normalization (Lei Ba et al. (2016)) as employed by Vaswani et al. (2017). To reconstruct the missing (along with other) values of the input, we apply a fully connected Feed Forward network on the final Value tensor, which is trained jointly with the rest of the model.  </p>
<p>Forecasting: As shown in Fig. 3(b), we apply our CDSA mechanism in Transformer framework whereweset $N=9$ for both encoder and decoder. Similar to imputation, we use a fully connected feed forward network to generate the predicted values.  </p>
<h1>3.4IMPLEMENTATION DETAILS</h1>
<p>We normalize each measurement of the input by subtracting the mean and dividing by standard deviation across training data. Then the entries with missed value are set O. We use the Adam optimizer (Kingma &amp; Ba (2014)) to minimize the Root Mean Square Error (RMSE) between the prediction and ground truth. The model is trained on a single NVIDIA GTX 1080 Ti GPU. More details (e.g., network hyper-parameters, learning rate and batch size) can be found in Supp.  </p>
<h1>4 EXPERIMENTS</h1>
<h1>4.1 DATASETS, TASKS, EVALUATION METRICS</h1>
<p>NYC-Traffic. New York City Department of Transportation has set up various street cameras1. Each camera keeps taking a snapshot every a few seconds. The is collected around 1-month data for 186 cameras on Manhattan from 12/03/2015 to 12/26/2015. For each snapshot, we apply our trained faster-RCNN (Ren et al. (2015)) vehicle detection model to detect the number of vehicles (#vehicle) in each snapshot. To aggregate such raw data into time series, for every non-overlapping 5-minute window, we averaged #vehicle from each snapshot to obtain the average #vehicle as the only measurement. Finally, we obtained 186 time series and the gap between two consecutive time stamps is 5 minutes.  </p>
<p>The natural missing rate of the whole dataset is $8.43\%$ . In order to simulate experiments for imputation, we further remove some entries and hold them as ground truth for evaluation. The imputation task is to estimate values of these removed entries. To mimic the natural data missing pattern, we model our manual removal as a Burst Loss, which means at certain location the data is continuously missed for a certain period. More details about vehicle detection and burst loss are be found in Supp. To simulate various data missing extents, we vary the final missing rate after removal from $20\%$ to $90\%$ For each missing rate, we randomly select 432 consecutive time slots to train our model and evaluate the average RMSE of 5 trials. The dataset will be released publicly.  </p>
<p>KDD-2018 (Cup (2018) is an Air Quality and Meteorology dataset recorded hourly. As indicated in Luo et al. (2018a), 11 locations and 12 measurements are selected. The natural missing rate is $6.83\%$ In order to simulate experiments for imputation, we follow Luo et al. (2018a) to split the data to every 48 hours, randomly hold values of some available entries and vary the missing rate from $20\%$ to $90\%$ Mean Squared Error (MSE) is used for evaluation.  </p>
<p>METR-LA (Jagadish et al. (2014)). We follow Li et al. (2018) to use this dataset for traffic speed forecasting. This dataset contains traffic speed at 207 locations recorded every 5 minutes for 4 months. Following Li et al. (2018), $80\%$ of data at the beginning of these 4 months is used for training and the remaining $20\%$ is for testing. In order to simulate the forecasting scenario, within either training or testing set, every time series of consecutive 2 hours are enumerated. For each time series, data in the first hour is treated as input and data in the second hour is to be predicted. We respectively evaluate the forecasting results at 15-th, 30-th, 60-th minutes in the second 1 hour and also evalaute the average evaluation results within the total 1 hour. We use RMSE, Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) as evaluation metrics.  </p>
<h1>4.2 COMPARISONS WITH STATE-OF-THE-ART</h1>
<p>Table 2: RMSE on dataset NYC-Traffic for comparisons with SOTA   </p>
<p><img alt="" src="images/3f20fe26cac1ad52ab9a166509e8b90c883458fddc784ccc20cf4a8d47f7422a.jpg" />  </p>
<p>Table 3: MSE on dataset KDD-2018 for comparisons with SOTA   </p>
<p><img alt="" src="images/efd845047cc550742a633869ee2305064a74bab5ebac69ce7eae886fe91ecaf7.jpg" />  </p>
<p>Imputation (NYC-Traffic, KDD-2018) In Table 2 , our CDSA consistently outperforms traditional methods (i.e., Auto Regressive, Kriging expo, Kriging linear) and recent RNN-based methods (i.e. MTSI, BRITS, DCRNN) over a wide range of missing rate. Because CDSA leverages the self-attention mechanism to avoid sequential processing of RNN and directly model the relationship between distant data. Table 3 shows that our method again achieves significant improvements on cross-dimensional data imputation task. Detailed overview of baselines can be found in Supp.  </p>
<p>Forecasting (METR-LA). Table 4 shows that for the forecasting task, our CDSA method outperforms previous methods in most cases. In particular, our method demonstrates clear improvement at long-term forecasting such as $60\;\mathrm{min}$ . This again confirms that our CDSA cthe effectiveness of directly modeling the relationship between every two data values (could from different dimensions and of far distance). But RNN-based methods and methods that sequentially conduct spatial conv and temporal conv fail to model the distant spatio-temporal relationship explicitly.  </p>
<p>Table 4: MAE/RMSE/MAPE on dataset METR-LA for comparisons with SOTA   </p>
<p><img alt="" src="images/ac2e3fe1fa61b2b1f1b46594fa560729e737257a66959dcc4000d8c14ee4f5a7.jpg" /><br />
4.3 DISCUSSIONS  </p>
<p>The effects of different training losses: For the forecasting task in METR-LA, we compare the performance by setting different training loss in Table 5 and we can see the performance with RMSE as loss metric achieves the best performance.  </p>
<p>Table 5: Comparisons of different losses in CDSA on METR-LA   </p>
<p><img alt="" src="images/12779c3b5f39c921a6499213b98f5614a8778de62b16df4c0f5eb63eb4142e1f.jpg" />  </p>
<p>Ablation study of different cross-dimensional self-attention manners: We compare the performance for different solutions in CDSA mechanism on the three datasets listed above. 1) The way of attention modeling determines the computational complexity. As shown in Table 1, since the Independent calculates dimension-specific Value vectors in parallel, the number of variables and FLOPs are larger than those of the Decomposed. As the Joint and the Shared both share the variables for each dimension, the number of variables is small and basically equals with each other. As the Joint builds a huge attention map, its FLOPs is much larger than others. Since the Decomposed draws attention maps like the Independent but shares Value like the Joint, it reduces the computational complexity significantly. 2) As shown in Table 6 - 8, we evaluate these methods on three datasets and the Decomposed always achieves the best performance thanks to the better learning ability compared to the Joint and Shared. More discussions can be found in Supp.  </p>
<p>Study of using the imputed time series for forecasting. On NYC-Traffic of missing rate $50\%$ ,we impute missing values in historical data (using statistical methods and our CDSA respectively) and then feed the imputed historical data into traffic prediction model ARIMA. We compare performances in terms of RMSE: Mean Filling 1.953, Kriging expo 1.681, Kriging linear 1.733, MTSI 1.675, DCRNN 1.666, BRITS 1.579, CDSA 1.536. This indicate that when using the imputed time series for forecasting, our CDSA can achieve significant gains in the downstream forecasting task as well compared to traditional imputation methods. More details can be found in Supp. E.3.  </p>
<p><img alt="" src="images/faa4675239b71cec76a9b1744049582293852235330d32305d43cf6f381d1dd5.jpg" />  </p>
<p>Table 7: Comparisons of different manners to implement CDSA on dataset KDD-2018.   </p>
<p><img alt="" src="images/80bfd111d9f393f4db605fbe20f45755b2aab354897eb61573ad1eb4f8cb7eca.jpg" />  </p>
<p>Table 8: Comparisons of different manners to implement CDSA on dataset METR-LA   </p>
<p><img alt="" src="images/b1588424cfafd9529f727a8f64e9b870183e73e7d4c64c31993356718efc4e9e.jpg" />  </p>
<p>Attention Map Visualization: Fig. 4 shows an PM10 imputation example in location fangshan at $t_{2}$ Since the pattern of PM2.5 around $t_{2}$ is similar to that at $t_{1}$ , the attention in orange box is high. As we can see that PM2.5 and PM10 are strongly correlated , in order to impute PM10 at $t_{2}$ , our model utilizes PM10 at $t_{1}$ (green arrow) and PM2.5 at $t_{1}$ (blue arrow), which crosses dimensions. More visualization examples can be found in Supp.  </p>
<p><img alt="" src="images/1e91328c59ef6a69f06ae3ac0b0d629ee2036d1ed3ecae517ba252a5326fb3e4.jpg" />  </p>
<p>Figure 4: Visualization of the cross-dimensional self-attention on KDD-2018. (a) Part of TimeMeasurement attention map. (b) Two time series of PM2.5 and PM1o. The value at purple dot is missing and our model predicts its value based on other values. The arrow in (b) represents attention whose score is highlighted with bounding box in (a) of the same color.  </p>
<h1>5 CONCLUSION</h1>
<p>In this paper, we have proposed a cross-dimensional self-attention mechanism to impute the missing values in multivariate, geo-tagged time series data. We have proposed and investigated three methods to model the cross-dimensional self-attention. Experiments show that our proposed model achieves superior results to the state-of-the-art methods on both imputation and forecasting tasks. Given the encouraging results, in the future we plan to extend our CDsA mechanism from multivariate, geo-tagged time series to the input that has higher dimension and involves multiple data modalities.  </p>
<h1>BIBLIOGRAPHY</h1>
<p>Edgar Acuna and Caroline Rodriguez. The treatment of missing values and its effect on classifier accuracy. In Classification, clustering, and data mining applications, pp. 639-647. Springer, 2004.  </p>
<p>Hirotugu Akaike. Fitting autoregressive models for prediction. Annals of the institute of Statistical Mathematics, 21(1):243-247, 1969.  </p>
<p>Craig F Ansley and Robert Kohn. On the estimation of arima models with missing values. In Time series analysis of irregularly observed data, pp. 9-37. Springer, 1984.  </p>
<p>Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, Pp.1171-1179, 2015.  </p>
<p>S van Buuren and Karin Groothuis-Oudshoorn. mice: Multivariate imputation by chained equations in r. Journal of statistical software, pp. 1-68, 2010.  </p>
<p>Jian-Feng Cai, Emmanuel J Candes, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956-1982, 2010.  </p>
<p>Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. Brits: Bidirectional recurrent imputation for time series. In Advances in Neural Information Processing Systems, pp. 6775-6785, 2018.  </p>
<p>Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent neural networks for multivariate time series with missing values. Scientific reports, 8(1):6085, 2018.  </p>
<p>Kyunghyun Cho, Bart Van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv: 1409.1259, 2014a.  </p>
<p>Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014b.  </p>
<p>Fan RK Chung and Fan Chung Graham. Spectral graph theory. American Mathematical Soc., 1997.  </p>
<p>KDD Cup. Available on:. http: / /www .kdd. 0rg/kdd2018/, 2018.  </p>
<p>Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning, volume 1. Springer series in statistics New York, 2001.  </p>
<p>E Haddadi, M R. Shabghard, M M. Ettefagh, A Bhattacharyya, G Boothroyd, M Castejon, E Alegre, J Barreiro, LK Hernandez, SK Choudhury, et al. Modern spectral estimation: Theory and application. Journal of Applied Sciences, 8(21):pp-438, 1998.  </p>
<p>Trevor Hastie, Robert Tibshirani, Gavin Sherlock, Michael Eisen, Patrick Brown, and David Botstein. Imputing missing data for gene expression arrays, 1999.  </p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, Pp.770-778,2016.  </p>
<p>Sepp Hochreiter and Juirgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780,1997.  </p>
<p>Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359-366, 1989.  </p>
<p>HV Jagadish, Johannes Gehrke, Alexandros Labrinidis, Yannis Papakonstantinou, Jignesh M Patel, Raghu Ramakrishnan, and Cyrus Shahabi. Big data and its technical challenges. Communications of the ACM, 57(7):86-94, 2014.  </p>
<p>Shuiwang Ji and Jieping Ye. An accelerated gradient method for trace norm minimization. In Proceedings of the 26th annual international conference on machine learning, pp. 457-464. ACM, 2009. <br />
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv: 1412.6980, 2014. <br />
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. <br />
Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network: Data-driven traffc forecasting. In International Conference on Learning Representations (ICLR '18), 2018. <br />
Yonghong Luo, Xiangrui Cai, Ying Zhang, Jun Xu, et al. Multivariate time series imputation with generative adversarial networks. In Advances in Neural Information Processing Systems, pp. 1596-1607, 2018a. <br />
Zhiming Luo, Fredéric Branchaud-Charron, Carl Lemaire, Janusz Konrad, Shaozi Li, Akshaya Mishra, Andrew Achkar, Justin Eichel, and Pierre-Marc Jodoin. Mio-tcd: A new benchmark dataset for vehicle classification and localization. IEEE Transactions on Image Processing, 27(10): 5129-5141, 2018b. <br />
Shiqian Ma, Donald Goldfarb, and Lifeng Chen. Fixed point and bregman iterative methods for matrix rank minimization. Mathematical Programming, 128(1-2):321-353, 2011. <br />
Fulufhelo V Nelwamondo, Shakir Mohamed, and Tshilidzi Marwala. Missing data: A comparison of neural network and expectation maximization techniques. Current Science, pp. 1514-1521, 2007. <br />
Sophocles J Orfanidis. Optimum signal processing: an introduction. Macmillan publishing company, 1988. <br />
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, 2015. <br />
Louis L Scharf and Cédric Demeure. Statistical signal processing: detection, estimation, and time series analysis, volume 63. Addison-Wesley Reading, MA, 1991. <br />
Ling Shi. Kalman fltering over graphs: Theory and applications. IEEE transactions on automatic control, 54(9):2230-2234, 2009. <br />
David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. arXiv preprint arXiv: 1211.0053, 2012. <br />
Robert H Shumway and David S Stoffer. An approach to time series smoothing and forecasting using the em algorithm. Journal of time series analysis, 3(4):253-264, 1982. <br />
Ikaro Silva, George Moody, Danil J Scott, Leo A Celi, and Roger G Mark. Predicting inhospital mortality of icu patients: The physionet/computing in cardiology challenge 2012. In 2012 Computing in Cardiology, Pp. 245-248. IEEE, 2012. <br />
Terry Speed. Statistical analysis of gene expression microarray data. Chapman and Hall/CRC, 2003. <br />
MichaelL Stein. Interpolation of spatial data: some theoryforkriging. Springer Science &amp;Business Media, 2012. <br />
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104-3112, 2014. <br />
Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie, Robert Tibshirani, David Botstein, and Russ B Altman. Missing value estimation methods for dna microarrays. Bioinformatics, 17(6):520-525, 2001. <br />
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017. <br />
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. <br />
Menglin Wang, Baisheng Lai, Zhongming Jin, Xiaojin Gong, Jianqiang Huang, and Xiansheng Hua. Dynamic spatio-temporal graph-based cnns for traffic prediction. arXiv preprint arXiv: 1812.02019, 2018. <br />
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 21-29, 2016. <br />
Xiuwen Yi, Yu Zheng, Junbo Zhang, and Tianrui Li. St-mvl: filling missing values in geo-sensory time series data. In Proceedings of the 25th International Joint Conference on Artificial Intelligence, June 2016. <br />
Hsiang-Fu Yu, Nikhil Rao, and Inderjit S Dhillon. Temporal regularized matrix factorization for high-dimensional time series prediction. In Advances in neural information processing systems, pp. 847-855, 2016. <br />
G Peter Zhang. Time series forecasting using a hybrid arima and neural network model. Neurocomputing, 50:159-175, 2003. <br />
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks. arXiv preprint arXiv: 1805.08318, 2018a. <br />
Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv: 1803.07294, 2018b. <br />
Yu Zheng, Xiuwen Yi, Ming Li, Ruiyuan Li, Zhangqing Shan, Eric Chang, and Tianrui Li. Forecasting fine-grained air quality based on big data. In Proceedings of the 2lth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Pp. 2267-2276. ACM, 2015. <br />
Jingguang Zhou and Zili Huang. Recover missing sensor data with iterative imputing network. In Workshops at the Thirty-Second AAAI Conference on Artificial Intelligence, 2018.  </p>
<h1>A MODEL ARCHITECTURE</h1>
<h1>A.1 NORMALIZATION LAYER</h1>
<p>Under the NLP scenario, each word is embedded as a vector and normalized individually. However, in the Cross-Dimensional scenario, the normalization applied on each individual unit will always lead to a zero-output. As shown in Fig. 5, different measurements may exhibit different correlation, i.e., PM2.5 and $\mathrm{PM}\ 10\$ are significantly positively correlated $(\rho_{\mathrm{PM2.5,\,PM10}}=0.8278)$ while $\mathrm{NO_{2}}$ and $\mathrm{{O_{3}}}$ are negatively correlated $(\rho_{\mathrm{NO_{2},O_{3}}}\,=\,-0.5117)$ .As discussed in Fig. 4(b) in the paper, different measurement may be used as reference for imputation of other measurements. As such, the normalization cross multiple measurements is unreasonable and we choose to apply normalization for each measurement in parallel which presumes that the time series inside the spatial network is essentially drawn from a standard normal distribution. We subsequently add the trainable scalar and bias to scale the normalized value and the scalars (biases) for different measurements are trained individually.  </p>
<p><img alt="" src="images/aca7eaab682c65aa6f50c5837ebf5fc91b74467682bf211e32fa7c2fe6138fba.jpg" /><br />
Figure 5: Different correlation between different measurements. The time slots when both of the chosen measurements are available are selected and the value of the first 300 selected time slots are plotted. Upper: PM2.5 &amp; PM10 are significantly positively correlated and Lower: $\mathrm{NO_{2}}$ &amp; $\mathrm{{O_{3}}}$ are negativelycorrelated.  </p>
<h1>A.2 UNIT-WISE FEED-FORWARD NETWORK</h1>
<p>Making use of the approximation property of multi-linear layer Hornik et al. (1989), a fully connected feed-forward network (FFN) is applied to each unit separately and identically. This FFN consists of three fully connected layer while RuLU is set as the activation function.  </p>
<p>$$
\mathrm{FFN}(x)=\operatorname_{max}(\operatorname_{max}(0,x W_{1}+b_{1})W_{2}+b_{2})W_{3}+b_{3}
$$  </p>
<p>During experiment, since the FFN is applied on each unit individually, we found the improvement by simply increasing the size of weight and bias of each layer is not obvious while increasing the depth of FFN will lead to obvious improvement.  </p>
<p>For the self-attention sub-layer in imputation task NYC and KDD-2018, we modify the attention map with mask in (7) to prevent unit of available observation from contributing to the estimation of itself.  </p>
<p>$$
S(i,j)=\left{{-\infty\atop q_{i}k_{j}^{\top}}/{\sqrt{d}}\atop\mathrm{otherwise}}\right.
$$  </p>
<p>where $\pmb q_{i}$ and $k_{j}$ are $d$ -dim vectors. This masking, combined with fact that there is no offset between input and output, ensures that the estimation of unit ${\bf\cal{X}}(p(t,l,m))$ depends on all the units except for itself, including both available and complemented units. In this way, the gradient back-propagation can be used to update the missed value effectively. The Table 9 shows the performance improvement of imputation mask applied in our model and demonstrate that mask prevents the estimation of itself and improve the inference ability of the model.  </p>
<p>Table 9: Performance Improvement for Imputation Mask   </p>
<p><img alt="" src="images/f8c5efc9f03e89d42efa0c28fdab8006d503c558db902f72bd141b4ea8e45c24.jpg" />  </p>
<h1>A.4ATTENTION MAP CALCULATION</h1>
<p><img alt="" src="images/c35a46aac85cbfc281d35f5e44d5e5ea02cd731ca29b04862bf5b25e1db2bae7.jpg" /><br />
Figure 6: The effective attention map calculation in Joint and Shared.  </p>
<p>Joint As shown in Fig. 6, when we build the attention among different units in the Joint, two different kernels will be used to map each input unit ${\cal X}(p)=\mathcal{X}(t,l,m)$ to an 1-D Query vector and an 1-D Key vector individually. As attention map is a scaled dot-product between Query and Key (Fig. 6 Left) after Softmax, each value of attention map in Joint is essentially the scaled numerical multiplication between each two units of input (Fig. 6 Right) after Softmax. As such, the multiple parameters inside that kernels only perform as a single scalar and the learning ability/relationship representation in Joint is limited.  </p>
<p>Decomposed According to Sec. 3.2.3, to calculate the dimension-specific attention map, e.g., the attention map of Time $A_{T}$ , the input $\mathcal{X}$ will be reshaped into matrix $X_{T}$ . Thus, the units corresponding to the same timestamp, reshaped into one vector $X_{T}(t,:)$ , will be mapped into dimension-specific Query vector $Q_{T}(t,:)$ and Key vector $K_{T}(t,:)$ : As more parameters will be introduced into such vector-vector mapping, each dimensional-specific map can learn the intra-correlation and the crossdimensional attention map can model the relationship among each input units effectively.  </p>
<p>Shared Same with the Decomposed, the Shared will calculate the dimension-specific attention maps individually. Like Joint, the Shared will map each unit $X(p)$ into vector. To calculate the dimensionspecific attention map (e.g., $A_{T}$ ), the “vector-vector mapping"” is essentially the summation of the units corresponding to the same timestamp while the multiple parameters introduced in the mapping still perform as a single scalar. As a result, the learning ability of the intra-correlation is limited so that the cross-dimensional attention map cannot model the relationship among input units effectively.  </p>
<p>B ATTENTION MAP RESHAPE IN THE Decomposed  </p>
<p><img alt="" src="images/9a1a0acfdc822b07d466e4391322609937f2e61d94a0003737524b8cccf4f126.jpg" />  </p>
<p><img alt="" src="images/b17c6dbe4cd39b4e85100ba5a03b5774ad8abaefeecb462a218021cf23ae05d9.jpg" />  </p>
<p>Figure 7: Reshape Attention Map: The original attention map in (a) is reshaped into (b-d) respectively. The variable $(t,l,m)$ denotes the attention map units for Time, Location and Measurement where the subscript labels the attention between two units, i.e., $t_{12}=\mathrm{Softmax}(q_{1}k_{2}^{\top}/\sqrt{d})$ where $\pmb q_{1}$ and $k_{2}$ are $d_{\cdot}$ -dim vectors. Besides, the empty entry indicates $\boldsymbol{O}$  </p>
<p>As described in (2) in the paper, the original attention maps $A_{T},A_{L}$ and $A_{M}$ are reshaped to $A_{T}^{\prime}$ $A_{L}^{\prime}$ and $A_{M}^{\prime}$ By setting $T=L=M=2$ as an example, we draw the attention maps before and after reshape in Fig. 7. Making use of the matrix structure, we have  </p>
<p>$$
\begin{array}{r l}&amp;{\displaystyle\sum_{p_{1}=1}^{T L M}A(p_{0},p_{1})=\sum_{t_{1}=1}^{T}\sum_{l_{1}=1}^{L}\sum_{m_{1}=1}^{M}A_{T}(t_{0},t_{1})A_{L}(l_{0},l_{1})A_{M}(m_{0},m_{1})}\ &amp;{\displaystyle=\sum_{t_{1}=1}^{T}\left(A_{T}(t_{0},t_{1})\sum_{l_{1}=1}^{L}\left(A_{L}(l_{0},l_{1})\displaystyle\sum_{m_{1}=1}^{M}A_{M}(m_{0},m_{1})\right)\right)=1,}\end{array}
$$  </p>
<p>Where $\begin{array}{r}{\sum_{t_{1}=1}^{T}\mathbf{\calA}_{T}(t_{0},t_{1})=1,\sum_{l_{1}=1}^{L}\mathbf{\calA}_{L}(l_{0},l_{1})=1}\end{array}$ and mn=1 Am(mo,m1) = 1. Besides, the reshape operation for attention map on different dimension is only determined by the index mapping function, $p=p(t,l,m)=L M t+M l+m$ , from the 3-dimensional cube to the vector form.  </p>
<h1>C CDSA FRAMEWORK FOR DATA FORECASTING</h1>
<p>Different from time series imputation task, the time series forecasting task use the current observation to estimate the time series in the future. To begin with, We first introduce the framework for time series forecasting and we compare the performance for two different types of input.  </p>
<h1>C.1 PREDICTION FRAMEWORK</h1>
<p>As shown in Fig. 8, we apply our CDSA mechanism in Transformer framework and use the same Feed Forward structure as in Sec. 3.3 in the paper. Notably, we set ${N=9}$ for both encoder and decoder and no CDsA module is used to derive a complement input for prediction task where the missing value is replaced with global mean. The architecture detail is shown in Fig. 9  </p>
<p><img alt="" src="images/899257d561296e9a339b22ad0d7a5ae35831fd63046fe3c8bbb823b524af58e7.jpg" />  </p>
<p>Figure 8: The framework of using Crossing-Dimensional Self-Attention (CDSA) for data forecasting.  </p>
<p><img alt="" src="images/c2de1e85affa9112d513dd55220c00db3f0e5e9f49e604a88815f69cf9e55776.jpg" /><br />
Figure 9: Model Architecture  </p>
<p>The decoder in NLP task originally sets the shifted output as input. Take the German-to-English translation scenario as an example where the embedded word vectors of German are set as the Encoder input, the model will first send a [GO] vector into the decoder and generate the first word vector of the translated English sequence, then the predicted vector will be sent into the decoder to predict the next word vector and the decoder will complete the sentence translation by repeating this operation until the end.  </p>
<p>Mapping directly from this model setting in NLP task to our series forecasting scenario, we can also use the shifted ground truth as the decoder input, i.e., to forecast the speed of the next $T$ timestamps given the speed of the first $T$ time stamps, the data of $T\leq t\leq2T-1$ are sent into the decoder. Consequently, the Casual Mask in Vaswani et al. (2017) need to be modified to make sure that the leftward information flow is prevented.  </p>
<p>For data forecasting by CDSA in the Decomposed, the masking on Attention Map on Time is simply masking out (setting to $-\infty,$ 0 the values in the input of Softmax which corresponds to the leftward information fow. Same with Vaswani et al. (2017), the masking is only adopted in the Multi-head CDSA layer labeled as (Mask TLM) in Fig. 9. However, as shown in Fig. 10, to calculate the Attention Map of Location and Measurement for data forecasting at $2T-1$ , all illegal units of input corresponding to $t\geq2T-1$ have to be masked out (setting as O). Then, the Masked Input are mapped to Query, Key and Value to build the Attention Map and calculate the Updated Value.  </p>
<p><img alt="" src="images/58bc65ab26084d874aafeed33a19ef4e52903d4a67ed02fb65c88569a12a57c4.jpg" /><br />
Figure 10: Casual Mask Design for Attention Map on Location  </p>
<p>Besides, the decoder generates predictions given previous ground truth observations during training while the ground truth observations are replaced by predictions generated by the model itself during testing. As, the discrepancy between the input distributions of training and testing can cause degraded performance, We adopt the integrated sampling Bengio et al. (2015) as in Li et al. (2018) to mitigate this impact while this method is very time-consuming for the Transformer framework. During testing,  </p>
<p>Table 10: Comparisons of Prediction Performance on dataset METR-LA   </p>
<p><img alt="" src="images/a041029f94f584c4b92db832c5fce0d21448d5ac80084f8dfcc16bcd1223bbb7.jpg" />  </p>
<p>In summary, by setting shifted output as the Decoder input, multiple Attention Map are calculated for forecasting value of different time stamps which requires huge memory usage. Still, integrated sampling makes this framework suffer from an exhausted training time, since we need to send the predicted output back to decoder $(R u n)$ and repeat this Run for $T$ times. During testing, we can use the output corresponding to its own Run (Step) as the predicted result, as well as the output of the last run (Final). As shown in the first 2 columns in Table 10, the performance of outputs in the last run (Final) is better than that of Step mode, which means the leftward information fow still exists to break the auto-regressive property in data forecasting even though the mask is adopted on the input data. For fair comparison, the models for testing are trained in one GPU and the training time are all less than 50 hours.  </p>
<p>Typically, missing value still exists in the original dataset. During experiment, we use the global mean to replace the missing value (Mean). We also compare the prediction performance between the input with Mean Filling and Complemented Input of Sec. 3.3. and the results in Table 10 shows the Complemented Input does not lead to performance improvement but increase the training workload. Consequently, we make encoder and decoder share the same input to reduce the memory usage and training time while our model achieves better performance for long-term prediction.  </p>
<h1>D DATASET DESCRIPTION</h1>
<h1>D.1 DATA AGGREGATION ON KDD 2018 DATASET</h1>
<p>Table 11: Selection of Common Locations between Air Quality and Meteorology   </p>
<p><img alt="" src="images/7c67d8a89e82f22832c9ab01be72b0be24498d1c1e9c845a2104cc547dffb29b.jpg" />  </p>
<p>The original KDD 2018 dataset consists of an Air Quality data of 35 locations and an Meteorology dataset of 18 locations. each dataset contains 6 different measurements. During experiment, Luo et al. (2018a) select 11 common locations between the two datasets and the measurements of paired locations are concatenated. The location pairs are described in Table 11. Since the unit of some measurements are label-based, e.g., the measurement weather denotes the types of weather including sunny, rainy and etc, these label are replaced with value such as $1,2,...,9$ .As the range of different measurements varies, Luo et al. (2018a) first apply $Z$ -score normalization for each measurement and the MSE is calculated based on normalized data while the metrics calculation for other dataset are based on the original data.  </p>
<p><img alt="" src="images/36c22cf1cbf6a62f208b74fc27fc9ddfa6e84d04c65f511db7669a2ac0868ec9.jpg" /><br />
D.2 NYC TRAFFIC DATASET  </p>
<p>Figure 11: Loss condition of NYC dataset: The horizontal axis represents time line while the vertical axis represents sensors. Each unit indicates whether the data is missed in a 5-min window. The white area indicates available observation. The blue area indicates the Burst Loss. The red area indicates the time slots when the data of all the sensors are missed. The green area denotes as Abnormal for a certain location the data is continuously missed for a very long period.  </p>
<p>Traffic volume extraction: We extract the traffic volume from images using a Faster R-CNN with VGG16 backbone, trained on the MIO-TCD Luo et al. (2018b) dataset. The dataset contains $110\mathbf{k}$ training images, with bounding box annotation for 11 vehicle categories (articulated truck, bicycle, bus, car, motorcycle, motorized vehicle, non-motorized vehicle, pedestrian, pick-up truck, single-unit truck, work van). On the NYC-traffic dataset, we manually annotated bounding boxes for a portion of images to evaluate the vehicle detector. Our model achieves $73\%$ precision and $54\%$ recall with an IoU of O.5. To construct the NYC traffic time series, we use the model to extract and then sum up the #cars of 11 different types (articulated truck, bicycle, bus, car, motorcycle, motorized vehicle, non-motorized vehicle, pedestrian, pick-up truck, single-unit truck, work van), in non-overlapping 5-minute intervals.  </p>
<p>Burst loss simulation: As shown in Fig. 11, we term the loss area marked as blue as the Burst loss area where for a certain camera, the data is continuously missed for a $\delta$ time slots. After statistics and analysis, we found the the length of time slots $2\leq\delta\leq134$ . Then, for those time slots of burst loss, we calculate the mean $\mu=6.350773$ and standard deviation $\delta=9.809643$ . With the mean and standard deviation, We model the generation of burst loss as Gaussian process.  </p>
<h1>E DISCUSSION</h1>
<p>E.1VISUALIZATION OF DIMENSION-SPECIFIC ATTENTION MAP  </p>
<p><img alt="" src="images/c711d0506aab7489cb99f2c83f0b65f91b26820318fd7913bfad75714737ea96.jpg" /><br />
Figure 12: Attention map example of the last CDSA layer  </p>
<p><img alt="" src="images/c9f02b72a361cca559a23fc6088041b9dd24bd0a620a37c714317fb7348532fa.jpg" /><br />
Figure 13: KDD-2015 Visualization of Location Correlation (arrow with darker color indicates a higher weight)  </p>
<p>We provide the attention map examples extracted from the last CDSA layer. As shown in Fig. 4(b), correlation exists between different measurement, i.e., PM2.5 and PM10 are highly correlated. As shown in Fig. 12(a), the estimation of PM2.5 and PM10 is also highly relied on each other, i.e., for the estimation of PM2.5, the color in second unit, representing the weight of PM10, is darker than the rest in the first row.  </p>
<p>As shown in Fig. 12(b) and Fig. 13, the arrow/unit with deeper color indicates a higher weight and the index of location can be found in Table 11. According to the map in Fig. 13, in most cases, neighbouring locations often share higher attention weight, e.g., the estimation at location 1 is mainly relied on the available data from location 2, location 3 and location 4. However, the estimation of location 11 is not relied on its neighbor (location 6), instead, it is mainly relied on the location 8. We think this relation is induced since both location 11 and location 8 are the center of express way while they are away from the urban area. Thus, the air condition from those two location my highly correlated.  </p>
<p><img alt="" src="images/7113e460364446db2785534810e6cd870ee6ba806968d093237e80e1c5b0fc97.jpg" /><br />
E.2 VISUALIZATION OF ATTENTION MAP FOR CROSS-DIMENSIONAL IMPUTATION <br />
Figure 14: Visualization of prediction of missing point A, our model not only attends to available points (e.g. C, D) but also attends to missing points (e.g. B).  </p>
<p>Besides the sample in Fig. 4 where the missing value can be estimated from the cross-dimensional available data, Fig. 14 visualizes another example and further shows that when predicting missing value A, our model pays strong attention to available values C and D while also some attention to another missingvalueB.  </p>
<p><img alt="" src="images/d6de273314a412f6fee38c71008df4ed5a68713b51bb6c9b470bb28e2336e475.jpg" />  </p>
<p>Following the model hyper-parameter setting in Tabls. 1, we further compare the average running time for one segment during testing. As the way of attention modeling determines the computational efficiency, computation method with higher FLOPs also leads to longer running time. As shown in Table. 12, the running time of Joint is much higher than the rest 3 methods. Since the computation schemes of Shared is similar with the Decomposed, while the number of trainable variables of Shared is much less that of Decomposed, the average processing time of Shared is a bit smaller than the running time of the Decomposed.  </p>
<p><img alt="" src="images/1fc84d1c57ab91f85d331561b45d6a21a3d0ce5fe77316282d4fafb5801e92a3.jpg" /><br />
E.4 DETAIL OF FORECASTING BY USING THE IMPUTED TIME SERIES <br />
Figure 15: RMSE comparison for Downstream Forecasting on NYC-Traffic  </p>
<p>As described in the main paper, we use the 23-day data of NYC-Traffic for further forecasting. We split the data into two segments, one segment contained the data of the first 20 days and the other contained the data of the rest 3 days. We used the imputed data from the first segment to forecast the value of the second segment. To provide comprehensive comparison, according to different missing ratio (i.e., $30\%$ $50\%$ $70\%$ ), we remove the value of some units in the first segment according to burst loss and then feed the segment with missing value into the data statistical imputation model (i.e., Mean filling, Kriging expo, Kriging linear) and deep learning methods (MTSI, DCRNN, BRITS, CDSA(ours)). Then, we feed the imputed data in to prediction model (ARIMA, Random Forest Friedman et al. (2001)) and evaluate the forecasting performance in terms of RMSE. According to the Fig.15, we can find our proposed model always outperforms than other method.  </p>
<h1>F EXPERIMENT SETTING</h1>
<p>F.1 BASELINES  </p>
<p>We compare our method with both deep learning based methods and statistical methods while all statistical methods are adopted for each measurement individually.  </p>
<p>● Mean Filling: Replace the missing data with global mean. <br />
● Auto Regressive Akaike (1969); Orfanidis (1988); Haddadi et al. (1998): Aggregate both forward and backward auto regressive on each time series data by weighted average and replace the missed value, implemented in MATLAB. <br />
· Kriging: Fit linear (Exponential) function between data variance and geodesic distance and replace the missing value w.r.t. the geo-location. Applied on each time slot and implemented in PyKrige2. <br />
● Multi-Imputation by Chained Equation (MICE) Buuren &amp; Groothuis-Oudshoorn (2010): Replace missing data by creating multiple imputations with chained equations. $^k$ -nearest neighbor (KNN) Speed (2003); Hastie et al. (1999); Troyanskaya et al. (2001): Use normalized Euclidean distance to find similar samples and impute the missing values with weighted average of its neighbors. <br />
● Matrix Factorization (MF) Yu et al. (2016): Factorizes the incomplete matrix into two low-rank matrices and fill the missing values by $l_{1}$ sparsity and $l_{2}$ penalty. <br />
● MTSI Luo et al. (2018a): Treat each measurement at different location as variables identically, align the time series of all the variables into one matrix and impute the missed data through RNN-based GAN. <br />
· ST-MVL Yi et al. (2016): Replace the missed value by using geo-location information <br />
· BRITS Cao et al. (2018): Treat the missing data as variable of the bidirectional RNN and impute by getting the delayed gradients for missing values in both forward and backward directions. <br />
● IN Zhou &amp; Huang (2018): Connect three bi-directional RNN in turn while the output of each RNN are adopted in loss calculation. <br />
● FC-LSTM Sutskever et al. (2014): Apply fully connected LSTM hidden units in EncoderDecoder RNN structure to model the temporal dependency and forecast the traffic speed. <br />
● DCRNN Li et al. (2018): Model traffic speed as signals diffused over bidirectional graph and modify the gated recurrent unit in encoder-decoder RNN structure to capture the spatiotemporal dependency. <br />
· DST-GCNN Wang et al. (2018): Build a two-stream framework for model the condition stream and flow evolution while the model use the predicted condition to forecast the fow. <br />
● GaAN Zhang et al. (2018b): Use attention mechanism for each location and then graph aggregator to assemble the neighbor nodes and impute the missing value.  </p>
<p>Table 13: Statistics of Dataset   </p>
<p><img alt="" src="images/4caca54e0dad9a323049cb4745f199ba7d19ca5d43071de0096cf3354d3f29c4.jpg" />  </p>
<p>The dataset in METR-LA also has missing data while the missing rate is $91\%$ .Thus, the segment sample whose all units are zero, i.e., all-zero sample, exists. During training, the all-zero sample (in training set) essentially has no contribution for the model training. During testing and validation, the evaluation metric will of such samples will not be counted.  </p>
<p>Data Pre-processing We apply $Z_{\cdot}$ -score normalization on each measurement as (9) respectively and fill the missing value with 0.  </p>
<p>$$
\begin{array}{r l}&amp;{{\mathbf X}(t,l)^{\prime}=\frac{{\mathbf X}\left(t,l\right)-\mu}{\sigma},}\ &amp;{\ \ \ \ \ \ \ \mu=\frac{\sum_{t=1}^{T}\sum_{l=1}^{L}{\mathbf X}\left(t,l\right)}{T L},}\ &amp;{\phi=\sqrt{\frac{\sum_{t=1}^{T}\sum_{l=1}^{L}({\mathbf X}\left(t,l\right)-\mu)^{2}}{T L}}.}\end{array}
$$  </p>
<p>Optimizer We use the Adam optimizer Kingma &amp; Ba (2014) while the initial learning rate in each epoch isset as  </p>
<p>$$
l r(e)=r_{0}\times\alpha^{c e i l(\mathrm{max}(0,e-d)/i)}.
$$  </p>
<h1>F.3 KDD-2015</h1>
<p>For KDD-2018, Luo et al. (2018a) adopts content loss in a GAN-based model to train the random noise and then estimate the missing value, i.e., for one data segment, according to the specified missing rate, some available data will be held to evaluate the imputation performance, while the remaining available data will be set as groundtruth to calculate the content loss. Our experiment on  </p>
<p>NYC-Traffic follows the same experiment setup as KDD-2018 while the noise is replaced with the remaining available data and the model parameter is trained according to the content loss. Thus, there is no division of training, validation or testing since the training loss is not calculated from the held availabledata.  </p>
<p>To comprehensively develop our experiment, we also adopting our method on KDD-2015 and follow the experiment setup in Yi et al. (2016); Cao et al. (2018) while the available data will be trained to predict the held data directly.  </p>
<p>KDD-2015 (Zheng et al. (2015). This dataset focuses on air quality and meteorology. It contains data recorded hourly, ending up with totally 8,759 time stamps. PM2.5 measurement is recorded at 36 locations and Temperature and Humidity are recorded at 16 locations in Beijing from 05/01/2014 to 04/30/2015, with natural missing rate $13.3\%$ $21.1\%$ and $28.9\%$ respectively. We treat those two subsets as two separate tasks and evaluate our method on each task separately. Following Yi et al. (2016), data in $3^{r\dot{d}}$ ， $6^{t h},9^{t h}$ and $12^{t h}$ months are for testing and the remaining months are for training. We randomly select 36 consecutive time slots to train our model and evaluate Mean Absolute Error (MAE) as well as Mean Relative Error (MRE).  </p>
<p>In order to simulate experiments for imputation, besides the natural missing data, for PM2.5 we follow the strategy used in (Yi et al. (2016); Cao et al. (2018); Zhou &amp; Huang (2018)) to further manually remove entries and hold the corresponding value as ground truth. The imputation task is to predict values of these manually removed entries. For Temperature and Humidity, we follow Zhou &amp; Huang (2018) to randomly hold $20\%$ of available data.  </p>
<p>KDD-2015. Table 14 shows that for PM2.5, our method outperforms the traditional methods significantly and achieves comparable MAE as IN (Zhou &amp; Huang (2018)) while better MRE than IN (Zhou &amp; Huang (2018)). For Temperature and Humidity, our method consistently outperforms state-of-the-artmethods.  </p>
<p>Table 14: MAE/MRE on dataset KDD-2015 for comparisons with SOTA   </p>
<p><img alt="" src="images/763b22b99cbb04d5ef12d20a9cc8fcc45295e39f3bdddbeddc57149fd6dcadee.jpg" />  </p>
<p>Table 15: Comparisons of different manners to implement CDSA on dataset KDD-2015.   </p>
<p><img alt="" src="images/37b205ebac47966bb1a24c192e80128a3482909cac287d24bc34d695d8b745ec.jpg" />  </p>
<p>Since the Decomposed draws attention maps as the Independent but shares Value as the Joint, it reduces the computational complexity significantly. As shown in Table 15, we also evaluate these methods on KDD-2015 datasets and the Decomposed achieves the best performance.  </p>
<h1>F.4METRICS</h1>
<p>Supose $\pmb{x}\,=\,[x_{1},x_{2},...x_{N}]\,\in\,\mathcal{R}^{N}$ represents the ground truth and $\pmb{\hat{x}}\,=\,[\hat{x}_{1},\hat{x}_{2},...\hat{x}_{N}]\,\in\,\mathcal{R}^{N}$ represents the predicted value.  </p>
<p>Root Mean Square Error (RMSE)  </p>
<p>$$
\mathrm{RMSE}(\pmb{x},\hat{\pmb{x}})=\sqrt{\frac{1}{N}\sum_{n=1}^{N}(x_{n}-\hat{x}_{n})^{2}}
$$  </p>
<p>Mean Squared Error (MSE)  </p>
<p>$$
\mathrm{MSE}(\pmb{x},\hat{\pmb{x}})=\frac{1}{N}\sum_{n=1}^{N}(x_{n}-\hat{x}_{n})^{2}
$$  </p>
<p>Mean Absolute Percentage Error (MAPE)  </p>
<p>$$
\mathrm{MAPE}({\boldsymbol{x}},{\hat{\boldsymbol{x}}})={\frac{1}{N}}\sum_{n=1}^{N}\left|{\frac{x_{n}-{\hat{x}}_{n}}{x_{n}}}\right|
$$  </p>
<p>Mean Absolute Error (MAE)  </p>
<p>$$
\mathrm{MAE}(\pmb{x},\hat{\pmb{x}})=\frac{1}{N}\sum_{n=1}^{N}|x_{n}-\hat{x}_{n}|
$$  </p>
<p>Mean Relative Error (MRE)  </p>
<p>$$
\mathrm{MRE}({\bf x},\hat{\bf x})=\frac{\sum_{n=1}^{N}|x_{n}-\hat{x}_{n}|}{\sum_{n=1}^{N}x_{n}}
$$  </p>
<p>F.5MODEL HYPER-PARAMETER  </p>
<p>Since there are missing data (Naturally missing data) in the original dataset, to evaluate the model performance, we manually remove some of the available observation (Manually removed data) and hold those entries’′ value as ground truth for evaluation. The rest data are termed as Available data. Thus, as a counterpart of Naturally missing data in the original dataset, the Naturally available data consists of Manually removed data and Available data.  </p>
<p>KDD 2015: Following the setting in Yi et al. (2016), we split the data into training set and testing set. During training, we send the Available data into the model to estimate the missing data while the loss is calculated based on the Manually removed data. During testing, we send the Available data into the model and the metric is calculated based on the Manually removed data. For model structure on Air Quality, we set $(d_{L},d_{T},v)$ as $(6,12,3)$ and 12 heads in each layer. We set $(r_{0},\alpha,d$ , batch size) as (0.003, 0.2, 60, 23) for at most 100 epoch. The model hyper-parameter on Meteorology is same with that for Air quality except for $d_{M}=15$  </p>
<p>KDD-2018: Following the setting in Luo et al. (2018a), we don't split the data for testing or validation while the imputation task assumes the completed data has no label. Thus, we send the Available data into the model while the loss is calculated based on Available data while the evaluation metric is calculated based on the Manually removed data. For the model structure, we set $(d_{T},d_{L},d_{M},v)$ as (30, 6, 14, 3) and 12 heads in each layer. We set $(r_{0},\,\alpha,\,d$ ,batchsize)as $(0.003,0.2,60,20)$ for at most 100 epoch.  </p>
<p>NYC: Like KDD-2018, the imputation task assumes the completed data has no label. For the estimated data, the loss is calculated on the Available data part while the evaluation metric is calculated on the Manually removed data part.  </p>
<p>METR-LA: Following the setting in Li et al. (2018), we split the data into training set and testing set where this prediction task assumes the predicted data has labels. For model structure, we set $(d_{T},d_{L},v)$ as $(14,6,3)$ and there are 16 heads in each layer. During training, we set $(r_{0},\alpha,d$ batch size) as (0.008, 0.5, 40, 16).  </p>
    </body>
    </html>