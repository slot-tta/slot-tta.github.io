<!doctype html>
    <html>
    <head>
        <meta charset="UTF-8">
        <link rel="stylesheet" media="all" href="normalize.css">
        <link rel="stylesheet" media="all" href="core.css">
        <link rel="stylesheet" media="all" href="style.css">
        <script type="text/javascript"
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
    
        <script type="text/javascript">
          MathJax.Hub.Config({
            tex2jax: {
              inlineMath: [['$', '$'], ['\(', '\)']],
              displayMath: [['$$', '$$'], ['\[', '\]']],
              processEscapes: true
            }
          });
        </script>
        
</head>
    <body>
    <h1>KRONECKER ATTENTION NETWORKS</h1>
<p>Anonymous authors Paper under double-blind review  </p>
<h1>ABSTRACT</h1>
<p>Attention operators have been applied on both 1-D data like texts and higher-order data such as images and videos. Use of attention operators on high-order data requires flattening of the spatial or spatial-temporal dimensions into a vector, which is assumed to follow a multivariate normal distribution. This not only incurs excessive requirements on computational resources, but also fails to preserve structures in data. In this work, we propose to avoid flattening by developing Kronecker attention operators (KAOs) that operate on high-order tensor data directly. KAOs lead to dramatic reductions in computational resources. Moreover, we analyze KAOs theoretically from a probabilistic perspective and point out that KAOs assume the data follow matrix-variate normal distributions. Experimental results show that KAOs reduce the amount of required computational resources by a factor of hundreds, with larger factors for higher-dimensional and higher-order data. Results also show that networks with KAOs outperform models without attention, while achieving competitive performance as those with original attention operators.  </p>
<h1>1 INTRODUCTION</h1>
<p>Deep learning networks with attention operators have demonstrated great capabilities of solving challenging problems in various tasks such as computer vision (Xu et al., 2015; Lu et al., 2016), natural language processing (Bahdanau et al., 2015; Vaswani et al., 2017), and network embedding (Velickovic et al., 2017). Attention operators are capable of capturing long-range relationships and brings significant performance boosts (Li et al., 2018; Malinowski et al., 2018). The application scenarios of attention operators range from 1-D data like texts to high-order and high-dimensional data such as images and videos. However, attention operators suffer from the excessive usage of computational resources when applied on high-order or high-dimensional data. The memory and computational cost increases dramatically with the increase of input orders and dimensions. This prevents attention operators from being applied in broader scenarios. To address this limitation, some studies focus on reducing spatial sizes of inputs such as down-sampling input data (Wang et al., 2018) or attending selected part of data (Huang et al., 2018). However, such kind of methods inevitably results in information and performance loss.  </p>
<p>In this work, we propose novel and effcient attention operators, known as Kronecker attention operators (KAOs), for high-order data, which avoid flattening and operate on high-order data directly. Experimental results show that KAOs are as effective as original attention operators, while dramatically reducing the amount of required computational resources. In particular, we employ KAOs to design a family of efficient modules, leading to our compact deep models known as Kronecker attention networks (KANets). KANets significantly outperform prior compact models on the image classification task, with fewer parameters and less computational cost. We also investigate the above problem from a probabilistic perspective. Specifically, regular attention operators flatten the data and assume the flattened data follow multivariate normal distributions. This assumption not only results in high computational cost and memory usage, but also fails to preserve the spatial or spatial-temporal structures of data. Our KAOs, instead, use matrix-variate normal distributions to model the data, where the Kronecker covariance structure is able to capture relationships among spatial or spatial-temporal dimensions.  </p>
<h1>2  BACKGROUND AND RELATED WORK</h1>
<p>In this section, we describe the attention and related non-local operators.  </p>
<p>2.1ATTENTION OPERATOR  </p>
<p>The inputs to an attention operator include a query matrix $\mathbf{\calQ}\;=\;\left[\mathbf{q}_{1},\mathbf{q}_{2},\cdots,\mathbf{q}_{m}\right]\;\in\;\mathbb{R}^{d\times m}$ , a key matrix ${\cal K}=\left[\mathbf{k}_{1},\mathbf{k}_{2},\cdots,\mathbf{k}_{n}\right]\in\mathbb{R}^{d\times n}$ , and a value matrix $V=\left[\mathbf{v}_{1},\mathbf{v}_{2},\cdots,\mathbf{v}_{n}\right]\in\mathbb{R}^{p\times n}$ . The attention operation computes the responses of a query vector $\mathbf{q}_{i}$ by attending it to all key vectors in $\kappa$ and uses the results to take a weighted sum over value vectors in $V$ . The layer-wise forward-propagation operation of an attention operator can be expressed as $O=V\mathrm{softmax}(\dot{K}^{T}Q)$ . Matrix multiplication between $K^{T}$ and $Q$ results in a coefficient matrix ${\pmb E}={\pmb K}^{T}{\pmb Q}$ , in which each element $e_{i j}$ is calculated by the inner product between $\mathbf{k}_{i}^{T}$ and ${\bf q}_{j}$ . This coefficient matrix $\pmb{E}$ computes similarity scores between every query vector $\mathbf{q}_{i}$ , and every key vector ${\bf k}_{j}$ and is normalized by a column-wise softmax operator to make every column sum to 1. The output $\b{O}\in\mathbb{R}^{p\times m}$ is obtained by multiplying $V$ with the normalized $\boldsymbol{E}$ . In self-attention operators (Vaswani et al., 2017), we have $Q=K=V$ . The computational cost in attention operator is $O(m\times n\times(d+p))$ . The memory required for storing the intermediate coefficient matrix $\boldsymbol{E}$ is $O(m n)$ . If $d=p$ and $m=n$ , the time and space complexities become $O(m^{2}\times d)$ and $O(m^{2})$ , respectively. There are several other ways to compute $\pmb{E}$ from $Q$ and $\kappa$ , including Gaussian function, dot product, concatenation, and embedded Gaussian function. It has been shown that dot product is the simplest but most effective one (Wang et al., 2018). Therefore, we focus on the dot product similarity function in this work. In practice, we can first perform separate linear transformations on each input matrix, resulting in the following attention operator: O = WV vSoftmax $(W^{K}K)^{T}W^{Q}Q)$ , where $W^{V}\in\mathbb{R}^{p^{\prime}\times p}$ $W^{K}\in\mathbb{R}^{d^{\prime}\times d}$ , and $W^{Q}\in\mathbb{R}^{d^{\prime}\times d}$ . For notational simplicity, we omit linear transformations in the following discussion.  </p>
<h1>2.2 NON-LOCAL OPERATOR</h1>
<p>Non-local operators (Wang et al., 2018) apply self-attention operators on higher-order data such as images and videos. Taking 2-D data as an example, the input to the non-local operator is a third-order tensor $\mathbf{\mathcal{X}}\in\mathbb{R}^{h\times w\times c}$ , where $h,w.$ and $c$ denote the height, width, and number of channels, respectively. The tensor is first converted into a matrix $\dot{X_{(3)}}\in\mathbb{R}^{c\times h w}$ by unfolding along mode-3 (Kolda &amp; Bader, 2009), as illus  </p>
<p><img alt="" src="images/1960cc66634ae306d211a0f6b34769e3ce19c4476dbee042dff5661ea4ef4fbe.jpg" /><br />
Figure 1: Conversion of a third-order tensor into a matrix by unfolding along mode-3. In this example, $h\times w\times c$ tensor is unfolded into a $c\times h w$ matrix.  </p>
<p>trated in Figure 1. Then we perform the attention operation by setting $Q=K=V=X_{(3)}$ The output of the attention operator is converted back to a third-order tensor as the final output. One practical challenge of the non-local operator is its excessive usage of computational resources. If $h=w$ , the computational cost of a 2-D non-local operator is $O(h^{4}\,{\overline{{\times}}}\,c)$ . The memory used to store the coefficient matrix incurs $O(h^{4})$ space complexity. The time and space complexities are prohibitively high for high-dimensional and high-order data.  </p>
<h1>3 KRONECKER ATTENTION NETWORKS</h1>
<p>In this section, we describe our proposed Kronecker attention operators, which are efficient and effective attention operators on high-order data.  </p>
<h1>3.1  KRONECKER ATTENTION OPERATORS</h1>
<p>We describe the Kronecker attention operators (KAO) in the context of self-attention on 2-D data, but they can be easily generalized to generic attentions. In this case, the input to the Cth layer is athird-ordertensor $\mathbf{\bar{x}}^{(\bar{\ell})}\in\mathbb{R}^{h\times w\times c}$ We propose to use horizontal and lateral average matrices to represent original mode-3 unfolding without much information loss. The horizontal average matrix $\pmb{H}$ and the lateral average matrix $\textbf{\emph{L}}$ arecomputed as  </p>
<p>$$
\pmb{H}=\frac{1}{h}\sum_{i=1}^{h}\pmb{X}_{i:\cdot}^{(\ell)}\in\mathbb{R}^{w\times c},\qquad\pmb{L}=\frac{1}{w}\sum_{j=1}^{w}\pmb{X}_{:j:}^{(\ell)}\in\mathbb{R}^{h\times c},
$$  </p>
<p>where X() and x() are the horizontal and lateral slices (Kolda &amp; Bader, 2009) of tensor $\mathbf{\mathcal{X}}^{(\ell)}$ respectively. We then form a matrix $_{C}$ by juxtaposing $H^{T}$ and $L^{T}$ as $C=[H^{T},L^{T}]\in\mathbb{R}^{c\times(h+w)}$  </p>
<p><img alt="" src="images/b5c36b1f9e9d157c7d621819c25184242f18d20b2a4714f6e966bbffa27e1bb1.jpg" /><br />
Figure 2: Mlustrations of regular attention operator (a), $\mathrm{KAO}_{K V}$ (b) and $\mathrm{KAO}_{Q K V}$ (c) on 2-D data. In the regular attention operator (a), the input tensor is unfolded into a mode-3 matrix and fed into the attention operator. The output of the attention operator is folded back to a tensor as the final output. In $\mathrm{KAO}_{K V}$ (b), we juxtapose the horizontal and lateral average matrices derived from the input tensor as the key and value matrices. We keep the mode-3 unfolding of input tensor as the query matrix. In $\mathrm{KAO}_{Q K V}$ (c), all three input matrices use the juxtaposition of two average matrices. In contrast to $\mathrm{KAO}_{K V}$ , we use an outer-sum operation to generate the output third-order tensor.  </p>
<p>Based on the horizontal and lateral average matrices contained in $_{C}$ , we propose two Kronecker attention operators (KAOs), i.e., $\mathrm{KAO}_{K V}$ and $\mathrm{KAO}_{Q K V}$ . In $\mathrm{KAO}_{K V}$ as shown in Figure 2 (b), we Luse $X_{(3)}^{(\ell)}$ $_{C}$  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We derived the equation for the Kronecker attention operator $\mathrm{KAO}_{K V}$ by leveraging the horizontal and lateral average matrices contained in the matrix $_{C}$, which is formed by juxtaposing the transpose of these average matrices. By using these average matrices as the key and value matrices, we can compute the output of the attention operator, which is given by the attention function $\mathrm{attn}$ applied to the mode-3 unfolding of the input tensor and the matrix $_{C}$., 
 before perplexity: 1.6601413799799047, after perplexity: 1.546520499676164</p>
<p>Completions End </p>
<p></span>$$
O=\mathrm{attn}(X_{(3)}^{(\ell)},C,C)\in\mathbb{R}^{c\times h w}.
$$  </p>
<p>Note that the number of columns in $^o$ depends on the number of query vectors. Thus, we obtain hw output vectors from the attention operation. Similar to the regular attention operator, $o$ is folded back to a third-order tensor $\mathbf{y}^{(\ell)}\in\mathbb{R}^{h\times w\times c}$ by considering the column vectors in $^o$ as mode-3 fibers of $\mathbf{y}^{(\ell)}$ $\mathrm{KAO}_{K V}$ uses $\mathbf{y}^{(\ell)}$ as the output of layer $\ell$ If $h\,=\,w$ , the time and space complexities of $\mathrm{KAO}_{K V}$ are $O(h w\times c\times(h+w))\stackrel{_}{=}O(h^{3}\stackrel{_}{\times}c)$ and $O(h w\times(h+w))\,=\,O\bar{(}h^{3})$ , respectively. Compared to the original local operator on 2-D data, $\mathrm{KAO}_{K V}$ reduces time and space complexities by a factor of $h$  </p>
<p>In order to reduce the time and space complexities further, we propose another operator known as $\mathrm{KAO}_{Q K V}$ .In $\mathrm{KAO}_{Q K V}$ as shown in Figure 2(c), we use $_{C}$ as the query, key, and value matrices as  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We propose to use the juxtaposition of the horizontal and lateral average matrices $H^{T}$ and $L^{T}$ as the query, key, and value matrices in $\mathrm{KAO}_{Q K V}$. To obtain the output of the attention operator, we apply the attention function to these three matrices, resulting in a matrix $O$ with dimensions $c\times(h+w)$., 
 before perplexity: 1.418134785270294, after perplexity: 1.3254618496172375</p>
<p>Completions End </p>
<p></span>$$
[\underbrace{\tilde{H}}_{h},\underbrace{\tilde{L}}_{w}]=O=\mathrm{attn}(C,C,C)\in\mathbb{R}^{c\times(h+w)}.
$$  </p>
<p>The final output tensor $\mathbf{y}^{(\ell)}\in\mathbb{R}^{h\times w\times c}$ is obtained as ${\bf Y}_{::i}^{\left(\ell\right)}=\tilde{\pmb{H}}_{i:}^{T}\oplus\tilde{\pmb{L}}_{i:}^{T}$ where $\tilde{\pmb{H}}_{i}$ and ${\tilde{L}}_{i}$ are the ith rows of the corresponding matrices. That is, the ith frontal slice of $\mathbf{y}^{(\ell)}$ is obtained by computing the outer sum of the ith rows of $\tilde{H}$ and $\tilde{L}$ If $h=w$ , the time and space complexities of $\mathrm{KAO}_{Q K V}$ are $O((h+w)\times c\times(h+w))=O(h^{2}\times c)$ and $O((h+w)\times(h+w))=O(h^{2})$ , respectively. Thus, the time and space complexities have been reduced by a factor of $h^{2}$ as compared to the original local operator, and by a factor of $h$ as compared to $\mathrm{KAO}_{K V}$ . Note that we do not consider linear transformations in our description, but they can be applied to all three input matrices in $\mathrm{KAO}_{K V}$ and $\mathrm{KAO}_{Q K V}$ as shown in Figure 2.  </p>
<p>Attention models have not been used in compact deep models to date, primarily due to their high computational cost. In this section, we design a family of efficient Kronecker attention modules based on KAOs as illustrated in Figure 3 in the appendix.  </p>
<p>BaseModule: MobileNetV2 (Sandler et al., 2018) is mainly composed of bottleneck blocks with inverted residuals. Each bottleneck block consists of three convolutional layers; those are, $1\times1$ convolutional layer, $3\times3$ depth-wise convolutional layer, and another $1\times1$ convolutional layer. Suppose the expansion factor is $r$ and stride is $s$ . Given input $\mathbf{\mathcal{X}}^{(\ell)}\in\mathbb{R}^{h\times w\times c}$ for the lth block, the first $1\times1$ convolutional layer outputs $r c$ feature maps $\tilde{\boldsymbol{x}}^{(\ell)}\in\mathbb{R}^{h\times w\times r c}$ The depth-wise convolutional layer uses a stride of $s$ and outputs $r c$ feature maps $\bar{\mathbf{x}}^{(\ell)}\in\mathbb{R}^{\frac{h}{s}\times\frac{w}{s}\times r c}$ The last $1\times1$ convolutional layer produces $d$ feature maps $\mathbf{y}^{(\ell)}\in\mathbb{R}^{\frac{h}{s}\times\frac{w}{s}\times d}$ . When $s=1$ and $c=d$ , a skip connection is added between $\mathbf{\mathcal{X}}^{(\ell)}$ and $\mathbf{y}^{(\ell)}$  </p>
<p>BaseSkipModule: To facilitate feature reuse and gradient back-propagation in deep models, we improve the BaseModule by adding a skip connection. Given input $\mathbf{\mathcal{X}}^{(\ell)}$ , we use an expansion factor of $r-1$ for the first $1\times1$ convolutional layer, instead of $r$ as in BaseModule. We then concatenate the output with the original input,resulting in $\tilde{\boldsymbol{x}}^{(\ell)}\in\mathbb{R}^{h\times w\times r c}$ The other parts of theBaseSkipModule are the same as those of the BaseModule. Compared to the BaseModule, the BaseSkipModule reduces the number of parameters by $c\times c$ and computational cost by $h\times w\times c$ . It achieves better feature reuse and gradient back-propagation.  </p>
<p>AttnModule: We propose to add an attention operator into the BaseModule to enable the capture of global features. We reduce the expansion factor of the BaseModule by 1 and add a new parallel path with an attention operator that outputs $c$ feature maps. Concretely, after the depth-wise convolutional layer, the original path outputs $\bar{\mathfrak{X}}_{a}^{(\ell)}\in\mathbb{R}^{\frac{h}{s}\times\frac{w}{s}\times(r-1)c}$ .The atention operator, optionally followed by an average pooling of stride $s$ $s&gt;1$ ,produces $\bar{\mathfrak{X}}_{b}^{(\ell)}\,\in\,\mathbb{R}^{\frac{h}{s}\times\frac{w}{s}\times c}$ . Concatenating them gives () e Rx xre. The final 1 × 1 convolutional layer remains the same. Within the attention operator, we only apply the linear transformation on the value matrix $V$ to limit the number of parameters and required computational resources. In this module, the original path acts as locality-based feature extractors, while the new path with an attention operator computes global features. This enables the module to incorporate both local and global information. Note that we can use any attention operator in this module, including the regular attention operator and our KAOs.  </p>
<p>AttnSkipModule: We propose to add an additional skip connection in the AttnModule. This skip connection can always be added unless $s&gt;1$ . The AttnSkipModule has the same amount of parameters and computational cost as the AttnModule.  </p>
<p>In the following sections, we perform some theoretical analysis on the proposed methods.  </p>
<p>3.3 FROM MULTIVARIATE TO MATRIX-VARIATE DISTRIBUTIONS  </p>
<p>We analyze our solutions for attention operators on high-order data from a probabilistic perspective. We take the non-local operator on 2-D data as an example. Formally, consider a self-attention operator with $Q=K=V=X_{(3)}$ ,where $\mathbf{\cal{X}}_{(3)}\in\mathbb{R}^{c\times h w}$ is the mode-3 unfolding of a third-order input tensor $\mathbf{\mathcal{X}}\in\mathbb{R}^{h\times w\times c}$ , a illutrated in Figure 1. The $i$ th row of $\pmb{X}_{(3)}$ corresponds to $\mathrm{vec}(X_{::i})^{T}\,\in\,\mathbb{R}^{h w}$ where $\pmb{X}_{::i}\in\mathbb{R}^{h\times w}$ denotes the ith frontal slice of $\boldsymbol{\mathfrak{X}}$ (Kolda &amp; Bader, 2009), and $\operatorname{vec}(\cdot)$ denotes the vectorization of a matrix by concatenating its columns (Gupta &amp; Nagar, 2018).  </p>
<p>The frontal slices $X_{::1},X_{::2},\allowbreak\dots,X_{::c}\in\mathbb{R}^{h\times w}$ of $\boldsymbol{\mathbf{\mathcal{X}}}$ are usually known as $c$ feature maps. In this view, the mode-3 unfolding is equivalent to the vectorization of each feature map independently. It is worth noting that, in addition to $\bar{\mathrm{vec}}(\cdot)$ , any other operation that transforms each feature map into a vector leads to the same output from the non-local operator, as long as a corresponding reverse operation is performed to fold the output into a tensor. This fact indicates that unfolding of $\boldsymbol{\mathbf{\mathcal{X}}}$ in local operators ignores the structural information within each feature map, i.e., the relationships among rows and columns. In addition, such unfolding results in excessive requirements on computational resources, as explained in Section 2.2.  </p>
<p>In the following discussions, we focus on one feature map $X\in\left{X_{::1},X_{::2},\ldots,X_{::c}\right}$ by assuming feature maps are conditionally independent of each other, given feature maps of previous layers. This assumption is shared by many deep learning techniques that process each feature map independently, including the unfolding mentioned above, batch normalization, instance normalization (Ulyanov et al., 2016), and pooling operations (LeCun et al., 1998). To view the problem above from a probabilistic perspective (Ioffe &amp; Szegedy, 2015; Ulyanov et al., 2016), the unfolding yields the assumption that $\operatorname{vec}(X)$ follows a multivariate normal distribution as $\mathrm{vec}(\boldsymbol{X})\sim\mathcal{N}_{h w}(\boldsymbol{\mu},\boldsymbol{\Omega})$ ,where $\pmb{\mu}\in\mathbb{R}^{h w}$ and $\Omega\in\mathbb{R}^{h w\times h w}$ . Apparently, the multivariate normal distribution does not explicitly model relationships among rows and columns in $\mathbf{\deltaX}$ . To address this limitation, we propose to model $\mathbf{\deltaX}$ using a matrix-variate normal distribution (Gupta &amp; Nagar, 2018), defined as below.  </p>
<p>Definition 1. A random matrix $A\in\mathbb{R}^{m\times n}$ is said to follow a matrix-variate normal distribution $\mathcal{M}\mathcal{N}_{m\times n}(M,\Omega\otimes\Psi)$ with mean matrix $M\in\mathbb{R}^{m\times n}$ and covariance matrix $\Omega\otimes\Psi$ , where $\Omega\in$ $\mathbb{R}^{m\times m}\;\succ\;0$ and $\boldsymbol{\Psi}\,\in\,\mathbb{R}^{n\times n}\,\succ\,0$ if $\mathrm{vec}(A^{T})\,\sim\,{\mathcal{N}}_{m n}\bigl(\mathrm{vec}(M^{T}),\Omega\otimes\Psi\bigr)$ . Here, $\otimes$ denotes the Kronecker product (Van Loan, 2000; Graham, 2018).  </p>
<p>The matrix-variate normal distribution has separate covariance matrices for rows and columns. They interact through the Kronecker product to produce the covariance matrix for the original distribution. Specifically, for two elements $X_{i j}$ and $X_{i^{\prime}j^{\prime}}$ from different rows and columns in $\mathbf{\deltaX}$ , the relationship between $X_{i j}$ and $X_{i^{\prime}j^{\prime}}$ is modeled by the interactions between the ith and $\,\mathrm{\nabla}\,{\mathrm{\cdot}}\,!!\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,\,!\,!\,\,!\,\,!\,\,!\,!\,\,!\,\,!\,!\,\,!\,!\,\,!\,!\,\,!\,!\,\,!\,!\,\,!\,!\,!\,\,!\,!\,!\,\,!\,!\,!\,\,!\,!\,!\,\,!\,!\,!\,\,!\,!\,!\,\,!\,!\,!\,\,!\,!\,!\,\,!\,!\,\,!\,!\,\,!\,!\,\,!\,!\,\,!\,!\,\,}$ rows and the $j$ th and $j^{\prime}$ th columns. Therefore, the matrix-variate normal distribution can incorporate relationships among rows and columns.  </p>
<h1>3.4 THE PROPOSED MEAN AND COVARIANCE STRUCTURES</h1>
<p>In machine learning, (Kalaitzis et al., 2013) proposed to use the Kronecker sum to form covariance matrices, instead of the Kronecker product. Based on the above observations and studies, we propose to model $\mathbf{\deltaX}$ as $X\sim{\mathcal{M N}}_{h\times w}({\bar{M}},\Omega\oplus\Psi)$ , where $M\in\mathbb{R}^{h\times w}$ ， $\Omega\in\mathbb{R}^{h\times h}\succ0$ $\Psi\in\mathbb{R}^{w\times w}\succ0$ $\oplus$ denotes the Kronecker sum (Kalaitzis et al., 2013), defined as $\Omega\oplus\Psi=\Omega\otimes I_{[w]}+I_{[h]}\otimes\Psi$ , and $I_{[n]}$ denotes an $n\times n$ identity matrix. Covariance matrices following the Kronecker sum structure can still capture the relationships among rows and columns (Kalaitzis et al., 2013). It also follows from (Allen &amp; Tibshirani, 2010; Wang et al., 2017) that constraining the mean matrix $_M$ allows a more direct modeling of the structural information within a feature map. Following these studies, we assume $\mathbf{\deltaX}$ follows a variant of the matrix-variate normal distribution as  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We propose to model the feature map $X$ as a matrix-variate normal distribution, which can capture the relationships among rows and columns. This distribution is characterized by a mean matrix $M$ and a covariance matrix that follows a Kronecker sum structure, which is defined as the sum of the Kronecker product of two matrices and the identity matrix., 
 before perplexity: 1.381194113675143, after perplexity: 1.3238225929536416</p>
<p>Completions End </p>
<p></span>$$
X\sim{\mathcal{M N}}_{h\times w}(M,\Omega\oplus\Psi),
$$  </p>
<p>where the mean matrix $M\in\mathbb{R}^{h\times w}$ is restricted to be the outer sum of two vectors, defined as $M=\pmb{\mu}\oplus\pmb{v}=\pmb{\mu1}_{[w]}^{T}+\mathbf{1}_{[h]}\pmb{v}^{T}$ where $\pmb{\mu}\in\mathbb{R}^{h},\pmb{v}\in\mathbb{R}^{w},$ and ${\mathbf1}_{[n]}$ denotes an all-one vector of size $n$  </p>
<p>Under this model, the marginal distributions of rows and columns are both multivariate normal (Allen &amp; Tibshirani, 2010). Specifically, the ith row vector $X_{i}$ $\in\mathbb{R}^{1\times w}$ follows $\pmb{X}_{i:}^{T}\sim\mathcal{N}_{w}(\mu_{i}!+!\pmb{v}^{T},\Omega_{i i}!+!\pmb{\Psi})$ and the $j$ th column vector $\mathbf{\boldsymbol{X}}_{:j}\in\mathbb{R}^{h\times1}$ follows $\pmb{X}_{:j}\sim\mathcal{N}_{h}(v_{i}!+!\pmb{\mu},\Psi_{i i}!+!\pmb{\Omega})$ . In the following discussion, we assume that $\Omega$ and $\Psi$ are diagonal, implying that any pair of variables in $\mathbf{\deltaX}$ are uncorrelated. Note that, although the variables in $\mathbf{\deltaX}$ are independent, their covariance matrix still follows the Kronecker covariance structure, thus capturing the relationships among rows and columns (Allen &amp; Tibshirani, 2010; Wang et al., 2017).  </p>
<h1>3.5  MAIN TECHNICAL RESULTS</h1>
<p>Let $\begin{array}{r}{\overline{{\mathbf{X}}}_{r o w}=(\sum_{i=1}^{h}{X_{i:}^{T}})/h\in\mathbb{R}^{w}}\end{array}$ and $\begin{array}{r}{\overline{{\mathbf{X}}}_{c o l}=\big(\sum_{j=1}^{w}X_{:j}\big)/w\in\mathbb{R}^{h}}\end{array}$ be the average of row and column vectors, respectively. Under the assumption above, $\overline{{X}}_{r o w}$ and $\overline{{\mathbf{X}}}_{c o l}$ follow multivariate normal distributions as  </p>
<p>$$
\overline{{{X}}}_{r o w}\sim\mathcal{N}_{w}(\overline{{{\mu}}}+v,\frac{\overline{{{\Omega}}}+\Psi}{h})\qquad(5)\qquad\qquad\overline{{{X}}}_{c o l}\sim\mathcal{N}_{h}(\overline{{{v}}}+\mu,\frac{\overline{{{\Psi}}}+\Omega}{w}),
$$  </p>
<p>Wwhere $\textstyle\overline{{\pmb{\mu}}}=(\sum_{i=1}^{h}{\mu_{i}})/h$ $\overline{{\Omega}}=\bigl(\sum_{i=1}^{h}{\Omega_{i i}}\bigr)/h$ $\textstyle\overline{{\pmb{v}}}=\bigl(\sum_{j=1}^{w}v_{j}\bigr)/w$ and $\begin{array}{r}{\overline{{\Psi}}=\big(\sum_{j=1}^{w}\Psi_{j j}\big)/w}\end{array}$ . Our main technical results can be summarized in the following theorem.  </p>
<p>Table 1: Comparisons between the regular attention operator, the regular attention operator with a pooling operation (Wang et al., 2018), and our proposed $\mathrm{KAO}_{K V}$ and $\mathsf{K A O}_{Q K V}$ in terms of the number of parameters, MAdd, memory usage, and CPU inference time on data of different sizes. The input sizes are given in the format of “batch size $\times$ spatial sizes $\times$ number of channels". “Attn" denotes the regular attention operator. “Attn+Pool’ denotes the regular attention operator which employs a $2\times2$ pooling operation on $\kappa$ and $V$ input matrices to reduce required computational resources.  </p>
<p><img alt="" src="images/8524a2eb6a074a63f4e87a24b099f20f2ee0e5bd289d48515fa07c71d7965458.jpg" />  </p>
<p>Theorem 1. Given the multivariate normal distributions in Eqs. (5) and (6) with diagonal $\Omega$ and $\Psi$ if $(a)\,\mathbf{r}_{1},\mathbf{r}_{2},\ldots,\mathbf{r}_{h}$ are independent and identically distributed (i.i.d.) random vectors that follow the distribution in Eq. (5), $(b)\,\mathbf{c}_{1},\mathbf{c}_{2},\ldots,\mathbf{c}_{w}$ are i.i.d. random vectors that follow the distribution in Eq. (6), $\cdot)\ \mathbf{r}_{1},\mathbf{r}_{2},\ldots,\mathbf{r}_{h}$ $\mathbf{c}_{1},\mathbf{c}_{2},\ldots,\mathbf{c}_{w}$ $\begin{array}{r}{\tilde{X}\sim\mathcal{M}\mathcal{N}_{h\times w}\left(\tilde{M},\frac{\overline{{\Psi}}+\Omega}{w}\oplus\frac{\overline{{\Omega}}+\Psi}{h}\right)}\end{array}$ where $\tilde{\cal X}=[{\bf r}_{1},{\bf r}_{2},\ldots,{\bf r}_{h}]^{T}+[{\bf c}_{1},{\bf c}_{2},\ldots,{\bf c}_{w}]$ $\tilde{M}=(\mu\oplus v)+(\overline{{\mu}}+\overline{{v}})$ . In particular, if $h=w$ , the covariancematri satisies $\begin{array}{r}{\mathrm{tr}\left(\frac{\overline{{\Psi}}+\Omega}{w}\oplus\frac{\overline{{\Omega}}+\Psi}{h}\right)=\frac{2}{h}\,\mathrm{tr}\left(\Omega\oplus\Psi\right)}\end{array}$ where $\operatorname{tr}(\cdot)$ denotes matrix trace.  </p>
<p>The proof of Theorem 1 can be found in the appendix. With certain normalization on $\mathbf{\deltaX}$ , we can have $\overline{{\pmb{\mu}}}+\overline{{\pmb{v}}}=0$ , resulting in ${\tilde{M}}\,=\,\mu\oplus v$ . As the trace of a covariance matrix measures the total variation, Theorem 1 implies that $\tilde{X}$ follows a matrix-variate normal distribution with the same mean and scaled covariance as the distribution of $\mathbf{\deltaX}$ in Eq. (4). We build KAOs based on this conclusion and the process to obtain $\tilde{X}$ from $\mathbf{\deltaX}$  </p>
<h1>4 EXPERIMENTAL STUDIES</h1>
<p>In this section, we evaluate our methods and networks on image classification and segmentation tasks.  </p>
<h1>4.1 COMPARISON OF COMPUTATIONAL EFFICIENCY</h1>
<p>According to the theoretical analysis in Section 3.1, our KAOs have efficiency advantages over regular attention operators on high-order data, especially for inputs with large spatial sizes. We conduct simulated experiments to evaluate the theoretical results. To reduce the influence of external factors, we build networks composed of a single attention operator, and apply the TensorFlow profile tool (Abadi et al., 2016) to report the multiply-adds (MAdd), required memory, and time consumed on 2-D simulated data. For the simulated input data, we set the batch size and number of channels both to 8, and test three spatial sizes; those are, $56\times56$ $28\times28$ , and $14\times14$ . The number of output channels is also set to 8. Table 1 summarizes the comparison results. On simulated data of spatial sizes $56\times56$ , our $\mathrm{KAO}_{K V}$ and $\mathrm{KAO}_{Q K V}$ achieve 31.1 and 305.8 times speedup, and $96.18\%$ and $99.73\%$ memory saving compared to the regular attention operator, respectively. Our proposed KAOs show significant improvements over regular atention operators in terms of computational resources, which is consistent with the theoretical analysis. In particular, the amount of improvement increases as the spatial sizes increase. These results show that the proposed KAOs are efficient attention operators on high-dimensional and high-order data.  </p>
<p>With the high efficiency of our KAOs, we have proposed several efficient Kronecker attention modules for compact CNNs in Section 3.2. To further show the effectiveness of KAOs and the modules, we build novel compact CNNs known as Kronecker attention networks (KANets). Following the practices in (Wang et al., 2018), we apply these modules on inputs of spatial sizes $28!\times!28$ $14!\times!14$ and $7\times7$ . The detailed network architecture is described in Table 6 in the appendix due to space constraint. We compare KANets with other CNNs on the ImageNet ILSVRC 2012 image classification dataset, which serves as the benchmark for compact CNNs (Howard et al., 2017; Zhang et al., 2017; Gao et al., 2018; Sandler et al., 2018). Details of the experimental setups are provided in the appendix.  </p>
<p>The comparison results between our KANets and other CNNs in terms of the top-1 accuracy, number of parameters, and MAdd are reported in Table 2.SqueezeNet (Iandola et al.,2016) has the least number of parameters, but uses the most MAdd and does not obtain competitive performance as compared to other compact CNNs. Among compact CNNs, MobileNetV2 (Sandler et al., 2018) is the previous state-of-theart model, which achieves the best trade-off between effectiveness and efficiency. According to the results, our KANets significantly outperform MobileNetV2 with 0.03 million fewer parameters. Specifically, our $\mathrm{KANet}_{K V}$ and $\mathsf{K A N e t}_{Q K V}$ outperform MobileNetV2 by margins of $0.9\%$ and $0.8\%$ , respectively. More importantly, our KANets has the least computational cost. These results demonstrate the effectiveness and efficiency of our proposed KAOs.  </p>
<p>Table 2: Comparisons between KANets and other CNNs in terms of the top-1 accuracy on the ImageNet validation set, the number of total parameters, and MAdd. We use $\mathrm{KANet}_{K V}$ and $\mathsf{K A N e t}_{Q K V}$ to denote KANets using $\mathrm{KAO}_{K V}$ and $\mathrm{KAO}_{Q K V}$ , respectively.  </p>
<p><img alt="" src="images/617a0d08f758265237ab7791f0659e3a0cb9f54a9da55b2c6a608259909eac09.jpg" />  </p>
<p>The performance of KANets indicates that our proposed methods are promising, since we only make small modifications to the architecture of MobileNetV2 to include KAOs. Compared to modules with the regular convolutional layers only, our proposed modules with KAOs achieve better performance without using excessive computational resources. Thus, our methods can be used widely for designing compact deep models. Next, we show that our proposed KAOs are as effective as regular attention operators.  </p>
<h1>4.3 COMPARISON WITH REGULAR ATTENTION OPERATORS</h1>
<p>We perform experiments to compare our proposed KAOs with regular attention operators. We consider the regular attention operator and the one with a pooling operation in (Wang et al., 2018). For the attention operator with pooling operation, the spatial sizes of the key matrix $\kappa$ and value matrix $V$ are reduced by $2\times2$ pooling operations to save computation cost. To compare these operators in fair settings, we replace all KAOs in KANets with regular attention operators and regular attention operators with a pooling operation, denoted as AttnNet and AttnNet+Pool, respectively.  </p>
<p>Table 3:Comparisons between KANets with regular attention operators (denoted as AttnNet), KANets with regular attention operators with a pooling operation (denoted as AttnNet+Pool) and KANets with KAOs in terms of the top-1 accuracy on the ImageNet validation set, the number of total parameters, and MAdd.  </p>
<p><img alt="" src="images/5d0fd36dec5b018a134d2012cebf1a3143f89622e8b01e85da4ad5ae35be2882.jpg" />  </p>
<p>The comparison results are summarized in Table 3. Note that all these models have the same number of parameters. We can see that $\mathrm{KANet}_{K V}$ and $\mathsf{K A N e t}_{Q K V}$ achieve similar performance as AttnNet and AttnNet+Pool with dramatic reductions of computational cost. The results indicate that our proposed KAOs are as effective as regular attention operators while being much more efficient. In addition, our KAOs are better than regular attention operators that uses a pooling operation to increase efficiency in (Wang et al., 2018).  </p>
<p>To show how our KAOs benefit entire networks in different settings, we conduct ablation studies on MobileNetV2 and $\mathrm{KANet}_{K V}$ . For MobileNetV2, we replace BaseModules with AttnModules as described in Section 3.2, resulting in a new model denoted asMobileNet $\mathrm{V}2{+}\mathrm{KAO}$ On the contrary, based on $\mathrm{KANet}_{K V}$ ,we replace all AttnSkipModules by BaseModules. The resulting model is denoted as KANet w/o KAO.  </p>
<p>Table 4 reports the comparison results. By employing $\mathrm{KAO}_{K V}$ , MobileNetV2+KAO gains a performance boost of $0.6\%$ with fewer parameters than MobileNetV2. On the other hand,  </p>
<p>Table  4:  Comparisons among $\mathrm{KANet}_{K V}$ MobileNetV2, MobileNetV2 with $\mathrm{KAOs}_{K V}$ (MobileNet $\mathsf{V}2!+!\mathsf{K A O}_{K V}$ )， and KANet without KAO (KANet w/o KAO) in terms of the top-1 accuracy on the ImageNet validation set, the number of total parameters, and MAdd.  </p>
<p><img alt="" src="images/fbae8db5ad0b457734234959c4b52c49b238ea1035b1806dbf8d98d1797ae33c.jpg" />  </p>
<p>$\mathrm{KANet}_{K V}$ outperforms KANet w/o KAO by a margin of $0.8\%$ , while KANet w/o KAO has more parameters than $\mathrm{KANet}_{K V}$ $\mathrm{KANet}_{K V}$ achieves thebest performance while costing the least computational resources. The results indicate that our proposed KAOs are effective and efficient, which is independent of specific network architectures.  </p>
<h1>4.5  RESULTS ON IMAGE SEGMENTATION</h1>
<p>In order to show the efficiency and effectiveness of our KAOs in broader application scenarios, we perform additional experiments on image segmentation tasks using the PASCAL 2012 dataset (Everingham et al., 2010). With the extra annotations provided by (Hariharan et al., 2011), the augmented dataset contains 10,582 training, 1,449 validation, and 1,456 testing images. Each pixel of the images is labeled by one of 21 classes with 20 foreground classes and 1 background class. We re-implement the DeepLabV2 model (Chen et al., 2018) as our baseline. Following (Wang &amp; Ji, 2018), using attention operators as the output layer, instead  </p>
<p>Table 5:  Comparisons among DeepLabV2, DeepLabV2 with the regular attention operator (DeepLab $\mathsf{V2+A t t n})$ ，DeepLabV2 with $\mathrm{KAO}_{K V}$ (DeepLab $\scriptstyle{\mathrm{V}}2+{\mathrm{KAO}}_{K V}$ )， and DeepLabV2 with $\mathrm{KAO}_{Q K V}$ (DeepLab $\mathsf{V}2\mathrm{+}\mathsf{K A O}_{\mathit{Q K V}}\,,$ ) in terms of the pixel-wise accuracy, and mean IOU on the PASCAL VOC 2012 validation dataset.   </p>
<p><img alt="" src="images/0040919aa8b1b9496a4c3c229790d3d4f60d5a0efd98826c14e7aab5764a54d9.jpg" />  </p>
<p>of atrous spatial pyramid pooling (ASPP), results in a significant performance improvement. In our experiments, we replace ASPP with the regular attention operator and our proposed KAOs, respectively, and compare the results. For all attention operators, linear transformations are applied On $Q,K$ and $V$ . Details of the experimental setups are provided in the appendix.  </p>
<p>Table 5 shows the evaluation results in terms of pixel accuracy and mean intersection over union (IoU) on the PASCAL VOC 2012 validation set. Clearly, models with attention operators outperform the baseline model with ASPP. Compared with the regular attention operator, KAOs result in similar pixel-wise accuracy but slightly lower mean IoU. From the pixel-wise accuracy, results indicate that KAOs are as effective as the regular attention operator. The decrease in mean IoU may be caused by the strong structural assumption behind KAOs. Overall, the experimental results demonstrate the efficiency and effectiveness of our KAOs in broader application scenarios.  </p>
<h1>5 CONCLUSIONS</h1>
<p>In this work, we propose Kronecker attention operators to address the practical challenge of applying attention operators on high-order data. We investigate the problem from a probabilistic perspective and use matrix-variate normal distributions with Kronecker covariance structure. Experimental results show that our KAOs reduce the amount of required computational resources by a factor of hundreds, with larger factors for higher-dimensional and higher-order data. We employ KAOs to design a family of efficient modules, leading to our KANets. KANets significantly outperform the previous state-of-the-art compact models on image classification tasks, with fewer parameters and less computational cost.  </p>
<h1>REFERENCES</h1>
<p>Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for large-scale machine learning. In OSDI, volume 16, pp. 265-283, 2016. <br />
Genevera I Allen and Robert Tibshirani. Transposable regularized covariance models with an application to missing data imputation. The Annals of Applied Statistics, 4(2):764, 2010. <br />
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. International Conference on Learning Representations, 2015. <br />
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834-848, 2018. <br />
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,2009. <br />
Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2): 303-338, 2010. <br />
Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Channelnets: Compact and efficient convolutional neural networks via channel-wise convolutions. In Advances in Neural Information Processing Systems, pp. 5203-5211, 2018. <br />
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, Pp. 249-256, 2010. <br />
Alexander Graham. Kronecker products and matrix calculus with applications. Courier Dover Publications, 2018. <br />
Arjun K Gupta and Daya K Nagar. Matrix variate distributions. Chapman and HallCRC, 2018. <br />
Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In Computer Vision (ICCV), 2011 IEEE International Conference on, Pp. 991-998. IEEE, 2011. <br />
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016. <br />
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv: 1704.04861, 2017. <br />
Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. arXiv preprint arXiv:1811.11721, 2018. <br />
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with $50\mathrm{x}$ fewer parameters and; $0.5\;\mathrm{mb}$ model size. arXiv preprint arXiv:1602.07360, 2016. <br />
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, Pp. 448-456, 2015. <br />
Alfredo Kalaitzis, John Lafferty, Neil Lawrence, and Shuheng Zhou. The bigraphical lasso. In International Conference on Machine Learning, pp. 1229-1237, 2013. <br />
Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM Review, 51(3): 455-500, 2009. <br />
Yann LeCun, Lon Bottou, Yoshua Bengio, and Parick Haffner Gradint-based learning applid to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. <br />
Guanbin Li, Xiang He, Wei Zhang, Huiyou Chang, Le Dong, and Liang Lin. Non-locally enhanced encoder-decoder network for single image de-raining. In 2018 ACM Multimedia Conference on Multimedia Conference, pp. 1056-1064. ACM, 2018. <br />
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740-755. Springer, 2014. <br />
Wei Liu, Andrew Rabinovich, and Alexander C Berg. Parsenet: Looking wider to see better. arXiv preprint arXiv:1506.04579, 2015. <br />
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for visual question answering. In Advances In Neural Information Processing Systems, pp. 289-297, 2016. <br />
Mateusz Malinowski, Carl Doersch, Adam Santoro, and Peter Battaglia. Learning visual question answering by bootstrapping hard attention. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 3-20, 2018. <br />
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4510-4520. IEEE, 2018. <br />
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929-1958, 2014. <br />
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pp. 1139-1147, 2013. <br />
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky, Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. <br />
Charles F Van Loan. The ubiquitous kronecker product. Journal of computational and applied mathematics, 123(1-2):85-100, 2000. <br />
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Mllia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998-6008, 2017. <br />
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv: 1710.10903, 2017. <br />
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, pp. 4, 2018. <br />
Zhengyang Wang and Shuiwang Ji. Smoothed dilated convolutions for improved dense prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, Pp. 2486-2495. ACM, 2018. <br />
Zhengyang Wang, Hao Yuan, and Shuiwang Ji. Spatial variational auto-encoding via matrix-variate normal distributions. arXiv preprint arXiv: 1705.06821, 2017. <br />
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning, pp. 2048-2057, 2015. <br />
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely effcient convolutional neural network for mobile devices. arXiv preprint arXiv: 1707.01083, 2017.  </p>
<h1>Appendix</h1>
<p>1 EXPERIMENTAL SETUP FOR IMAGE CLASSIFICATION  </p>
<p>As a common practice on this dataset, we use the same data augmentation scheme in (He et al., 2016). Specifically, during training, we scale each image to $256\times256$ and then randomly crop a $224\times224$ patch. During inference, the center-cropped patches are used. We train our KANets using the same settings as MobileNetV2 (Sandler et al., 2018) with minor changes. We perform batch normalization (Ioffe &amp; Szegedy, 2015) on the coefficient matrices in KAOs to stabilize the training. All trainable parameters are initialized with the Xavier initialization (Glorot &amp; Bengio, 2010). We use the standard stochastic gradient descent optimizer with a momentum of 0.9 (Sutskever et al., 2013) to train models for 150 epochs in total. The initial learning rate is 0.1 and it decays by 0.1 at the 80th, 105th, and 120th epoch. Dropout (Srivastava et al., 2014) with a keep rate of 0.8 is applied after the global average pooling layer. We use 8 TITAN Xp GPUs and a batch size of 512 for training, which takes about 1.5 days. Since labels of the test dataset are not available, we train our networks on training dataset and report accuracies on the validation dataset.  </p>
<h1>2  EXPERIMENTAL SETUP FOR IMAGE SEGMENTATION</h1>
<p>We train all the models with randomly cropped patches of size $321\times321$ and a batch size of 8. Data augmentation by randomly scaling the inputs for training is employed. We adopt the “poly" learning rate policy (Liu et al., 2015) with $p o w e r=0.9$ , and set the initial learning rate to 0.00025. Following DeepLabV2, we use the ResNet-101 model pre-trained on ImageNet (Deng et al., 2009) and MS-COCO (Lin et al., 2014) for initialization. The models are then trained for 25,000 iterations with a momentum of 0.9 and a weight decay of 0.0005. We perform no post-processing such as conditional random fields and do not use multi-scale inputs due to limited GPU memory. All the models are trained on the training set and evaluated on the validation set.  </p>
<p>3ILLUSTRATION OF KRONECKER ATTENTION MODULES  </p>
<p><img alt="" src="images/20571d05d99f7203a3f645fbd87e7d8b9f878d137d2767ac991d894a1c466ada.jpg" /><br />
Figure 3: Architectures of the BaseModule (a), BaseSkipModule (b), AttnModule (c), and AttnSkipModule (d) as described in Section 3.2. The skip connections indicated by single dashed paths are notusedwhen $s&gt;1$ Or $c\neq d$ . Those indicated by double dashed paths are not used when $s&gt;1$  </p>
<h1>4 THE KANETS ARCHITECTURE</h1>
<p>Table 6 describes the detailed KANets architecture. We use KAOs in every AttnSkipModule. In KAOs, we use multi-head attention with 4 heads and concatenate results for output. The linear  </p>
<p>Table 6: Details of the KANets architecture. Each line describes a sequence of operators in the format of “input size / operator name / expansion rate $\pmb{r}$ / number of output channels $c/$ number of operators in the sequence $^{\textit{n}/}$ stride $s^{\flat}$ .“Conv2D" denotes the regular 2D convolutional layer. “AvgPool" and “FC denote the global average pooling layer and the fully-connected layer, respectively. All depth-wise convolutions use the kernel size of $3\times3$ . For multiple operators in a sequence denoted in the same line, all operators produce $^c$ output channels. And the first operator applies the stride of $\pmb{s}$ while the following operators applies the stride of 1. $k$ denotes the class number in the task.  </p>
<p><img alt="" src="images/2675330375a6b7cc5b2886cf0f98578b494a38195bc8f8a0ca2f44399e27e0f5.jpg" />  </p>
<p>transformation is only performed on the value matrix $V$ to limit the number of parameters and computational resources.  </p>
<h1>5 PROOF OF THEOREM 1</h1>
<p>The fact that $\Omega$ and $\Psi$ are diagonal implies independence in the case of multivariate normal distributions. Therefore, it follows from assumptions (a) and (b) that  </p>
<p>$$
[\mathbf{r}_{1},\mathbf{r}_{2},\ldots,\mathbf{r}_{h}]^{T}\sim{\mathcal{M N}}_{h\times w}\left(M_{r},I_{[h]}\otimes{\frac{{\overline{{\Omega}}}+\Psi}{h}}\right),
$$  </p>
<p>where $M_{r}={\overline{{\pmb{\mu}}}}+[{\pmb v},{\pmb v},\ldots,{\pmb v}]^{T}={\overline{{\pmb{\mu}}}}+\mathbf{1}_{[h]}{\pmb v}^{T}$ and  </p>
<p>$$
\left[\mathbf{c}_{1},\mathbf{c}_{2},\ldots,\mathbf{c}_{w}\right]\sim\mathcal{M N}_{h\times w}\left(M_{c},\frac{\overline{{\Psi}}+\Omega}{w}\otimes I_{[w]}\right),
$$  </p>
<p>where $M_{c}=\overline{{\boldsymbol{v}}}+\left[\mu,\mu,\ldots,\mu\right]=\overline{{\boldsymbol{v}}}+\mu\mathbf{1}_{\left[w\right]}^{T}.$  </p>
<p>Given assumption (c) and $\tilde{\cal X}=[{\bf r}_{1},{\bf r}_{2},\ldots,{\bf r}_{h}]^{T}+[{\bf c}_{1},{\bf c}_{2},\ldots,{\bf c}_{w}].$ we have  </p>
<p>$$
\tilde{\boldsymbol{X}}\sim\mathcal{M N}_{h\times w}\left(\tilde{M},\frac{\overline{{\boldsymbol{\Psi}}}+\boldsymbol{\Omega}}{w}\oplus\frac{\overline{{\boldsymbol{\Omega}}}+\boldsymbol{\Psi}}{h}\right),
$$  </p>
<p>where $\tilde{M}=M_{r}+M_{c}=(\mu\oplus v)+(\overline{{\mu}}+\overline{{v}}).$  </p>
<p>If $h=w$ , we have  </p>
<p>$$
\mathrm{tr}\big(\Omega\oplus\Psi\big)=h\left(\sum\Omega_{i i}+\sum\Psi_{j j}\right),
$$  </p>
<p>and  </p>
<p>$$
\begin{array}{r l}&amp;{\mathrm{tr}\left(\overline{{\frac{\Psi}{w}}}+\Omega\right.}\ &amp;{=\mathrm{\i}\mathrm{\tr}\left(\frac{1}{h}(\Omega\oplus\Psi)+\frac{1}{h}(\overline{{\Psi}}+\overline{{\Omega}})\right)}\ {=}&amp;{\left(\sum\Omega_{i i}+\sum\Psi_{j j}\right)+h(\overline{{\Psi}}+\overline{{\Omega}})}\ {=}&amp;{\mathrm{\i}2(\sum\Omega_{i i}+\sum\Psi_{j j})}\ {=}&amp;{\frac{2}{h}\cdot\mathrm{tr}\left(\Omega\oplus\Psi\right).}\end{array}
$$  </p>
<p>This completes the proof of the theorem.  </p>
<h1>REFERENCES</h1>
<p>All citations refer to the references in the main paper.  </p>
    </body>
    </html>