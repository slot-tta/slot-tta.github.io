<!doctype html>
    <html>
    <head>
        <meta charset="UTF-8">
        <link rel="stylesheet" media="all" href="normalize.css">
        <link rel="stylesheet" media="all" href="core.css">
        <link rel="stylesheet" media="all" href="style.css">
        <script type="text/javascript"
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
    
        <script type="text/javascript">
          MathJax.Hub.Config({
            tex2jax: {
              inlineMath: [['$', '$'], ['\(', '\)']],
              displayMath: [['$$', '$$'], ['\[', '\]']],
              processEscapes: true
            }
          });
        </script>
        
</head>
    <body>
    <h1>ADAPTIVE STRUCTURAL FINGERPRINTS FOR GRAPH ATTENTION NETWORKS</h1>
<p>Kai Zhang <br />
Department of Computer &amp; Information Sciences <br />
Temple University <br />
Philadelphia PA 19122, USA <br />
kzhang980@gmail.com  </p>
<p>Yaokang Zhu &amp; Jun Wang* School of Computer Science and Technology East Chine Normal University, Shanghai China 52184501026@stu.ecnu.edu.cn jwang@sei.ecnu.edu.cn  </p>
<p>Jie Zhangt Institute of Brain-Inspired Intelligence Fudan University, Shanghai China jzhang080@gmail.com  </p>
<h1>ABSTRACT</h1>
<p>Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect performance, refecting the over-smoothing risk of GAT (or graph neural networks in general), and the ineffectiveness in exploiting graph structural details. In this paper, we propose an “adaptive structural fingerprint" (ADSF) model to fully exploit graph topological details in graph attention network. The key idea is to contextualize each node with a weighted, learnable receptive field encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can be inferred accurately, thus significantly improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform for different subspaces of node features and various scales of graph structures to “cross-talk" with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data. Empirical results demonstrate the power of our approach in exploiting rich structural information in GAT and in alleviating the intrinsic oversmoothing problem in graph neural networks.  </p>
<h1>1 INTRODUCTION</h1>
<p>Many real-world data set are represented naturally as graphs. For example, citation networks specify the citation links among scientific papers; social media often need to explore the significant amount of connections between users; biological processes typically involve complex interactions such as protein-protein-interaction (PPI). In these scenarios, the complex structures such as the graph topology or connectivities encode crucial domain-specific knowledge for the learning and prediction tasks. Examples include node embedding or classification, graph classification, and so on.  </p>
<p>The complexity of graph-structured data makes it non-trivial to employ traditional convolutional neural networks (CNN's). The CNN architecture was originally designed for images whose pixels are located on a uniform grids, and so the convolutional filters can be reused everywhere without having to accommodate local structure changes (LeCun &amp; Kavukcuoglu, 2010) . However, extending CNN to deal with arbitrary structured graphs can be non-trivial. To solve this problem, graph neural networks (GNN) were early proposed by Gori et al. (2005) and Sperduti (1997), which adopt an iterative process to propagate the state of each node, followed by a neural network module to generate the output until an equilibrium state is reached. Recent development of GNN can be categorized into spectral and non-spectral approaches. Spectral approaches employ the tools in signal processing and transform the convolutional operation in the graph domain to much simpler operations of the Laplacian spectrum (Bruna et al., 2014), and various approaches have been proposed to localize the convolution in either the graph or spectral domain (Henaff et al., 2015; Defferrard et al., 2016; Kipf &amp; Welling, 2017). Non-spectral approaches define convolutions directly on the neighboring nodes. As a result, varying node structures have to be accommodated through various processing steps such as fixed-neighborhood size sampling (Hamilton et al., 2017), neighborhood normalization (Niepert et al., 2016), or learning a weight matrix for each node degree (Duvenaud et al., 2015) or neighborhood size (Hamilton et al., 2017). Recently, residual network is also introduced to graph neural networks (Zhang &amp; Meng, 2019).  </p>
<p>Recently, graph attention network (GAT) proves a promising framework by combining graph neural networks with attention mechanism (Velickovic et al., 2017). The attention mechanism allows dealing with variable sized input while focusing on the most relevant parts, and has been widely used in sequence modelling (Bahdanau et al., 2015), machine translation (Luong et al., 2015), and visual processing (Xu et al., 2015). The GAT model further introduces attention module into graphs, where the hidden representation of the nodes are computed by repeatedly attending over their neighbors' features, and the weighting coefficients are calculated inductively based on a self-attention strategy.  </p>
<p>Despite the numerous success, how to exploit structural information in GAT remains an challenge. Note that attention scores in GAT are computed mainly based on the content of the nodes; the structures of the graph are simply used to mask the attention, e.g., only one-hop neighbors will be attended. When considering attention among higher order neighbors, however, the performance of GAT deteriorates (see experimental section for details). This is closely related to over-smoothing of GNN's (Li et al., 2018) and reflects the weakness of GAT in effectively exploiting graph structural information. In this paper, we believe that the topology or “shapes” of local edge connections scan provide a valuable guidance on how to exploit rich structural information of graphs. For example, in social networks or biological networks, a community or pathway may be composed of nodes that are densely inter-connected with each other but several hops away. Therefore, it can be beneficial attend neighbors from the same community, even they show no direct connections. To achieve this, simplychecking $\mathrm{k}$ -hop neighbors would seem insufficient and a thorough exploration of structural landscapes of the graph becomes necessary.  </p>
<p>In order to fully exploit rich, high-order structural details in graph attention networks, we propose a new model called “adaptive structural fingerprints". The key idea is to contextualize each node within a local receptive field composed of its high-order neighbors. Each node in the neighborhood will be assigned a non-negative, closed-form weighting based on local information propagation procedures, and so the domain (or shape) of the receptive field will adapt automatically to local graph structures and the learning task. We call this weighted, tunable receptive field for each node its “structural fingerprint'. The structural fingerprint encodes important structural details and will be used in conjunction with the node feature to compute an improved attention layer. Furthermore, our approach provides a useful platform for different subspaces of the node features and various scales of local graph structures to coordinate with each other in learning multi-head attention, being particularly beneficial in handling complex real-world graph data sets.  </p>
<p>In Section 2, we introduce the proposed method, including limitation of content-based attention, construction of adaptive structural fingerprints, and the algorithm workflow. In Section 3, we discuss related work. Section 4 reports empirical evaluations and the last section concludes the paper.  </p>
<h1>2  EXPLOITING GRAPH STRUCTURAL DETAILS IN ATTENTION</h1>
<h1>2.1 LIMITATIONS OF CONTENT-BASED ATTENTION</h1>
<p>The “closeness" between two nodes should be determined from both their content and structure. Here we illustrate the importance of detailed graph structures in determining node similarities. In Figure 1(a), suppose the feature similarity of node-pairs (A,B) and (A,C) are similar. Namely, content-based attention will be similar for this two node pairs. However, from structural point of view, the attention between (A,B) should be much stronger than that for (A,C). This is because A and B are located in a small, densely inter-connected community, and they share a significant portion of common neighbors; while node A and C does not have any common neighbor. In Figure 1(b), node A and node B are not direct neighbors, and connecting them takes three edges. As a result, their features will not directly affect each other in the message passing of GAT'. However, both A and B are strongly connected to a dense community, and node B further connects to the community hub. Therefore, it is reasonable for A and B to directly affect each other.  </p>
<p><img alt="" src="images/982573255e2b8c30b372b24bc494a0bcaa271e8e5f89d18fb18b539a410180ee.jpg" /><br />
Figure 1: Detailed graph structures provide important guidance in determining node attention.  </p>
<p>In these examples, feature based similarity alone is insufficient in computing a faithful attention. One may need to take into account structural details of higher-order neighbors and how they interact with each other. In the literature, structural clues have long been exploited in solving problems of clustering, semi-supervised learning, and community detection . For example, normalized cut minimizes edge connections between clusters for node partitioning (Shi &amp; Malik, 20o0); mean-shift uses density peaks to identify clusters (Comaniciu &amp; Meer, 2002); low-density separation (Chapelle &amp; Zien, 2005) assumes that class boundaries pass through low-density regions, leading to successful semi-supervised classification; in community detection, densely connected subgraphs are the main indicator of community (Girvan &amp; Newman, 2002).  </p>
<p>In the following, we show how to build “adaptive structural fingerprint’ to extract informative structural clues to improve graph attention and henceforth node embedding and classification.  </p>
<h1>2.2ADAPTIVE STRUCTURAL FINGERPRINTS</h1>
<p>Key to exploiting the structural information is the construction of the so-called "adaptive structural fingerprints", by placing each node in the context of the its local "receptive field'". Figure 2 is an illustration. For any node $i$ , consider a spanning process that locates a local subgraph around $i$ ,for example, all the nodes within the $k$ -hop neighbors of $i$ . Denote the resultant subgraph as $(V_{i},E_{i})$ ， where $V_{i}$ and $E_{i}$ are the set of nodes (dark red) and edges (black) in this neighborhood. In the meantime, each node in $_i$ will be assigned a non-negative weight, denoted by $\breve{\mathbf{w}_{i}}\in\mathbb{R}^{n_{i}\times1}$ , which specifies the importance of each node in shaping the receptive filed also determines the effective "shape” and size of the field. The structural fingerprint of node $i$ will be defined as $F_{i}=(V_{i},\mathbf{w}_{i})$ ：  </p>
<h1>2.3  CONSTRUCTION OF THE STRUCTURAL FINGERPRINTS</h1>
<p>Intuitively, the weight of the nodes should decay with their distance from the center of the fingerprint. A simple idea is to compute the weight of the node $j$ using a Gaussian function of its distance from $i$ $\begin{array}{r}{,\mathbf{w}_{i}(j)\,=\,\exp(-\frac{d i s(i,j)^{2}}{2h^{2}})}\end{array}$ can map the node distance levels $[1,2,...,k]$ to weight levels $\mathbf{u}=[u_{1},u_{2},...,u_{k}]$ which are nonnegative and monotonic; we call nonparametric decay, which is more fexible and will be used in our experiments. In both cases, the decay parameter (Gaussian bandwidth $h$ or nonparametric decay profile $\textbf{u}$ ) can be optimized, making the structural fingerprint more adaptive to the learning process.  </p>
<p>In practice, it is more desirable if node weights can be automatically adjusted by local graph structures (shapes or density of connections). To achieve this, we propose to use Random Walk with Restart (RWR). Random walks were first developed to explore global topology of a network by simulating a particle iteratively moving among neighboring nodes (Lovasz, 1993). If the particle is  </p>
<p><img alt="" src="images/42398e866ba7643200e7b5d6a390b500d3787d373595703c8f9e1c75d0f2fb9b.jpg" /><br />
Figure 2: Structural fingerprint for a given node $i$ ,denotedby $F_{i}=(V_{i},\mathbf{w}_{i})$ ,where $V_{i}$ is the set of nodes in this local receptive field and $\mathbf{w}_{i}$ is the contributing weights of the nodes.  </p>
<p><img alt="" src="images/7e49be8e54a3442b492b703216e31f11e62b07a2d0f3e871a37cb6b0f82cfbb3.jpg" />  </p>
<p>Figure3:Weights $\mathbf{w}_{i}$ of a toy structural fingerprint. Left: the local subgraph/fingerprint; middle: visualization of the weights; right: contours of the weights. Gaussian decay and RWR(random walk with restart) decay leads to different weight contours, the latter more adaptively adjusting the weights to structural details of the local graph.  </p>
<p>forced to always restart in the same node (or set of “seed"' nodes), random walk with restart (RWR) can then quantify the structural proximity between the seed(s) and all the other nodes in the graph, which has been widely used in information retrieval (Tong et al., 2006; Pan et al., 2004).  </p>
<p>Consider a random walk with restart on a structural fingerprint centered around node $i$ with altogether $n_{i}$ nodes and adjacency matrix $\mathbf{E}_{i}$ . The particle starts from the center node $i$ and randomly walks to its neighbors in $V_{i}$ with a probability proportional to edge weights. In each step, it also has a certain probability to return to the center node. The iteration can be written as  </p>
<p>$$
\mathbf{w}_{i}^{(t+1)}=c\cdot\tilde{\mathbf{E}}_{i}\mathbf{w}_{i}^{(t)}+(1-c)\cdot\mathbf{e}_{i}
$$  </p>
<p>where $\tilde{\bf E}$ is the transition probability matrix by normalizing columns of $\mathbf{E}_{i}$ $,c\in[0,1]$ is a tradeoff parameter between random walk and restart, and ${\bf{e}}_{i}$ is a vector of all zeros except the entry corresponding to the center node $i$ . The converged solution can be written in closed form as  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We start by analyzing the random walk with restart (RWR) process on a structural fingerprint centered around node $i$. By iteratively applying the transition probability matrix $\tilde{\mathbf{E}}_i$ and the restart vector $\mathbf{e}_i$, we can derive the converged solution for the node weights $\mathbf{w}_i$. This involves inverting the matrix $\mathbf{I} - c\cdot\tilde{\mathbf{E}}_i$ and multiplying it by the restart vector $\mathbf{e}_i$., 
 before perplexity: 0.9581990229277315, after perplexity: 0.906083332347537</p>
<p>Completions End </p>
<p></span>$$
\begin{array}{r}{\mathbf{w}_{i}=(\mathbf{I}-c\cdot\tilde{\mathbf{E}}_{i})^{-1}\mathbf{e}_{i}.}\end{array}
$$  </p>
<p>The $\mathbf{w}_{i}$ quantifies the proximity between the center node $i$ and all other nodes of the fingerprint, and in the meantime naturally refects local structural details of the graph. The $c$ controls the decaying rate (effective size) of the fingerprint: if $c=0$ ， $\mathbf{w}_{i}$ will all be zeros except the ith node; if $c=1$ ， $\mathbf{w}_{i}$ will be the stationary distribution of a standard random walk on graph $(V_{i},\mathbf{E}_{i})$ . In practice, $c$ will be optimized so that the fingerprint adapts naturally to both the graph structure and the learning task.  </p>
<p>In Figure 3, we illustrate Gaussian-based and RWR-based fingerprint weights. We created a toy example of local receptive field, which for convenience is a uniform grid but with a small denser "patch" on the right bottom. As can be expected, the Gaussian based weight decay has contours that are center-symmetric. In comparison, the RWR automatically takes into account salient local structures and so the contours will be biased towards the dense subgraph. This is particularly desirable in case the center node is close to (or residing in) a community; it will then be represented more closely by the nodes from community, achieving the effect of a “structural attractor".  </p>
<p>As can be seen, the “structural attractor’ effect in building the fingerprint with RWR coincides well with commonly used clustering assumptions, namely, the structural fingerprint of a node will emphasize more on densely inter-connected neighbors within predefined range. Since highly weighted nodes are more likely to come from the same cluster as the center node, the fingerprint of each node is supposed to provide highly informative guidance on its structural identity, thus improving evaluation of node similarites and finally the graph attention.  </p>
<h1>2.4ALGORITHM DESCRIPTION</h1>
<p>In this section we will exploit both the content and structural details of the graph in the GAT framework (Velickovic et al., 2017). Our algorithm is illustrated in Figure 4.  </p>
<p><img alt="" src="images/fe8477a8f5651a296bb6c618bddb4080119570e349130cb3b708d2113b69045b.jpg" /><br />
Figure 4: The work fow of adaptive structural fingerprint model.  </p>
<p>Suppose we want to compute attention coefficients between a pair of nodes $i$ and $j$ ,eachwith their features and structural fingerprints. Content-wise, features of the two nodes will be used to compute their content similarity; structure-wise, structural fingerprints of the two nodes will be used to evaluate their interaction. Both scores will be incorporated in the attention layer, which will then be used in the message passing step to update node features. Following Velickovic et al. (2017), we also apply a transform on the features, and apply multiple steps of message passing.  </p>
<p>More specifically, given a graph of $n$ nodes $G=(V,\mathbf{E})$ where $V$ is the set of the nodes and $\mathbf{E}$ is the set of the edges; let ${{\bf h}_{i}}_{i=1}^{n}$ be the $d_{\cdot}$ -dimensional input features for each node. We will follow the basic structure of GAT algorithm (Velickovic et al., 2017) and describe our algorithms as follows.  </p>
<p>· Step 1. Evaluate the content similarity between node $i$ and $j$ as  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We first consider the content similarity between two nodes $i$ and $j$, which is typically computed using their input features $\mathbf{h}_i$ and $\mathbf{h}_j$. We apply a linear transformation $\mathbf{W}$ to these features, and then compute the similarity between the transformed features using a function $\mathcal{A}_{feats}$., 
 before perplexity: 2.183591955529326, after perplexity: 1.961674478070511</p>
<p>Completions End </p>
<p></span>$$
e_{i j}=\mathcal{A}_{f e a}(\mathbf{Wh}_{i},\mathbf{Wh}_{j})
$$  </p>
<p>where $\mathbf{W}\in\mathbb{R}^{d\times d}$ is the transformation that maps the node features to a latent space, and function $\mathcal{A}_{f e a}(\cdot,\cdot)$ computes similarity (or interaction) between two feature $\mathbf{h}_{i}$ and $\mathbf{h}_{j}$  </p>
<p>$$
\mathscr{A}_{f e a}(\mathbf{Wh}_{i},\mathbf{Wh}_{j})=\mathbf{a}^{\top}(\mathbf{Wh}_{i}||\mathbf{Wh}_{j})
$$  </p>
<p>· Step 2. Evaluate structural interaction between the structural fingerprints of node $i$ and $j$  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We consider the structural interaction between the structural fingerprints of node $i$ and $j$, denoted as $F_i$ and $F_j$ respectively. To evaluate this interaction, we compute the similarity between the weighted adjacency matrices of $F_i$ and $F_j$, which can be represented as $\mathcal{A}_{s t r}(F_{i},F_{j})$. This similarity score captures the structural relationship between the two nodes, taking into account the proximity and connectivity of their local neighborhoods., 
 before perplexity: 1.7215487484095826, after perplexity: 1.6016445039961211</p>
<p>Completions End </p>
<p></span>$$
s_{i j}=\mathcal{A}_{s t r}(F_{i},F_{j})
$$  </p>
<p>where $A_{s t r}(F_{i},F_{j})$ quantifies the interaction between two fingerprints. Let $\mathbf{w}_{i}$ and $\mathbf{w}_{j}$ be the node weights of the fingerprints for node $i$ and $j$ , as discussed in Section 2.3. Then we can adopt the weighted Jacard similarity to evaluate the structural interactions, as  </p>
<p>$$
A_{s t r}(F_{i},F_{j})=\frac{\sum_{p\in(V_{i}\cup V_{j})}\operatorname_{min}(w_{i p},w_{j p})}{\sum_{p\in(V_{i}\cup V_{j})}\operatorname_{max}(w_{i p},w_{j p})}
$$  </p>
<p>Here, with an abuse of notations, we have expanded $\mathbf{w}_{i}$ and ${\bf w}_{j}$ to all the nodes in $V_{i}\cup V_{i}$ by filling zeros. We can consider the following smooth version, or other smooth alternative2.  </p>
<p>$$
\operatorname_{max}(x,y)=\operatorname_{lim}_{t}\frac{\log(e^{t\cdot x}+e^{t\cdot y})}{t},\:\:\operatorname_{min}(x,y)=\operatorname_{lim}_{t}-\frac{\log(e^{t\cdot(-x)}+e^{t\cdot(-y)})}{t}
$$  </p>
<p>● Step 3. Normalize (sparsify) feature similarities (2) and the structural interactions as (4)  </p>
<p>$$
\bar{e}_{i j}\leftarrow\frac{\mathrm{exp}(\mathrm{LeakyRelu}(e_{i j}))}{\sum\mathrm{exp}(\mathrm{LeakyRelu}(e_{i k}))},\;\;\bar{s}_{i j}\leftarrow\frac{\mathrm{exp}(s_{i j})}{\sum\mathrm{exp}(s_{i k})}
$$  </p>
<p>and then combine them to compute the final attention  </p>
<p>$$
a_{i j}=\frac{\alpha(\bar{e}_{i j})\bar{e}_{i j}+\beta(\bar{s}_{i j})\bar{s}_{i j}}{\alpha_{(}\bar{e}_{i j})+\beta(\bar{s}_{i j})}.
$$  </p>
<p>Here $\alpha(\cdot)$ and $\beta(\cdot)$ are transfer functions (such as Sigmoid) that adjust feature similarity and structure interaction scores before combining them. For simplicity (and in our experiments), we use scalar $\alpha$ and $\beta$ , which leads to a standard weighted average.  </p>
<p>· Step 4. Perform message passing to update the features of each node as  </p>
<p>$$
\mathbf{h}_{i}^{(t+1)}=\sigma\left(\sum_{j\in\mathcal{N}_{i}}\alpha_{i j}\mathbf{W}\mathbf{h}_{j}^{(t)}\right)
$$  </p>
<p>Our algorithm has a particular advantage when multi-head attention is pursued. Note that our model simultaneously calculates two attention scores: the content-based $e_{i j}$ and structure-based $s_{i j}$ , and combine them together. Therefore each attention head will accommodate two sets of parameters: (1) those of the content-based attention, $\mathbf{W}$ and a (3), which explores the subspace of the node features, and (2) those of the structure-based attention, $c$ (1), which explores the decay rate of the structural fingerprint. As a result, by learning an optimal mixture of the two attention (6), our model provides a flexible platform for different subspaces of node features and various scales of local graph structures to “cross-talk" with each other, which can be quite useful in exploring complex real-world data.  </p>
<p>Computationally, the local receptive field will be confined within a $k$ -hop neighborhood, which we call “fingerprint size"'; on the other hand, both structural attention and content-based attention will be considered only when the center node of the two receptive fields have a distance below the threshold $k^{\prime}$ , which we call “attention range'". In experiments we simply set $k\,=\,k^{\prime}\,=\,2$ , and use breadthfirst-search (BFS) to localize the neighborhood. As a result, the complexity involved in structure exploration will be $O\big(n|\overline{{\Lambda\mathord{\left/{\vphantom{\Lambda\sqrt{\Lambda_{k^{\prime}}}}\right.\kern-\nulldelimiterspace}}}}\big)$ , where $|\overline{{\mathcal{N}_{k^{\prime}}}}|$ is the averaged $k^{\prime}$ -hop-neighbor size.  </p>
<h1>3  COMPARISONS WITH RELATED WORK</h1>
<p>Note that in graph convolutional network (Kipf &amp; Welling, 2017), the node representation is updated $\begin{array}{r}{\mathbf{h}_{i}^{(t+1)}=\sigma\left(\sum_{j\in\mathcal{N}_{i}^{\mathbf{A}}}[\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}]_{i j}\mathbf{h}_{j}^{(t)}\mathbf{W}\right)}\end{array}$ $\mathbf{A}$ $\mathbf{D}$ $\mathbf{h}_{i}$ $\mathbf{W}$  </p>
<p>Table 1: Summary statistics of the benchmark graph-structured data sets used in the experiment.   </p>
<p><img alt="" src="images/af6a1e686bac04643a1d0625614312d8c3b382da803db37f9a3a83a82e8b173e.jpg" />  </p>
<p>passing is mainly determined by the (normalized) adjacency matrix. The GAT method (Velickovic et al., 2017) replaces the fixed adjacency matrix with an inductive, trainable attention function that relies instead on the node features within one-hop neighbors. Our approach has a notable difference. First, our message passing is determined by a mixed attention from both structure and content (6). Second, the structural component of our attention is not simply based on the graph adjacency (Kipf &amp; Welling, 2017), or one-hop neighbor (Velickovic et al., 2017), but instead relies on a local receptive field whose “shapes” are optimized adaptively through learning (1). Furthermore, our method fully exploits structural details (e.g. density and topology of local connections). There are also a number of works that explore structures in graph classification (Lee et al., 2018; Rossi et al., 2019). There, attention is used to identify small but discriminative parts of the graph, also called “graphlets" or “motifs" (Morris et al., 2019), in order to perform classification on the graph level.  </p>
<h1>4 EXPERIMENTS</h1>
<p>In this section, we report experimental results of the proposed method and state-of-the-art algorithms using graph-based benchmark data sets and transductive classification problem. Our codes can be downloaded from the anonymous Github link http://github.com/AvigdorZ.  </p>
<h1>4.1 EXPERIMENTAL SETTING AND RESULTS</h1>
<p>We have reported results of the following baseline algorithms: Gaussian fields and harmonic function (Gaussian Fields) (Zhu et al., 2003), manifold regularization (Manifold Reg.) (Belkin et al., 2006); Deep Semi-supervised learning (Deep-Semi) (Weston et al., 2012); link-based classification (Link-based) Lu &amp; Getoor. (2003); skip-gram based graph embedding (Deep-Walk) (Perozzi et al., 2014); semi-supervised learning with graph embedding (Planetoid) (Yang et al., 2016); graph convolutional networks (GCN) (Kipf &amp; Welling, 2017); high-order chebyshev filters with GCN (Chebyshev) (Defferrard et al., 2016), and the mixture model CNN (Mixture-CNN) (Monti et al., 2016). We have selected three benchmark graph-structured data set from (Sen et al., 2008), namely Cora, Citeseer, and Pubmed. The three data sets are all citation networks. Here, each node denotes one document, and an edge will connect two nodes if there is citation link between the two document; the raw features of each document are bags-of-words representations. Each node (document) will be associated with one label, and following the transductive setting in (Velickovic et al., 2017; Yang et al., 2016) we only use 20 labeled samples for each class but with all the remaining, unlabelled data for training. We split the data set into three parts: training, validation, and testing, as shown in table 1. Algorithm performance will be evaluated on the classification precision on the test split. For algorithm using random initialization, averaged performance over 10 runs will be reported.  </p>
<p>The network structures of our methods follow the GAT method (Velickovic et al., 2017), with the following details. Altogether two layers of message passing are adopted. In the first layer, one transformation matrix $\bar{\mathbf{W}}\in\mathbb{R}^{d\times8}$ is learned for each of altogether 8 attention heads; in the second layer, a transformation matrix $\mathbf{W}\in\mathbb{R}^{64\times C}$ is used on the concatenated features (from the 8 attention head from the first layer), and one attention head is adopted followed by a softmax operator, where $C$ is the number of classes. The number of parameters is $64(d+C)$ . For the Pubmed data set, 8 attention heads are used in the second layer due to the larger graph size. Adam SGD is used for optimization, with learning rate $\lambda=5\times1e-4$ . See more details in (Velickovic et al., 2017). Both the fingerprint size and the attention range is chosen as 2-hop neighbors in our approach.  </p>
<p>Results are reported in table 2. Our method has 2 variations, including ADSF-Nonparametric, where we learn a non-parametric decay profile w.r.t. node distance; ADSF-RWR, where we use randomwalk with re-start to build fingerprints. For both cases, we have used 2-hop neighbors, and the restart probability is simply chosen as $c=0.5$ . As can be seen, our approach consistently improves the performance on all benchmark data sets. Since Velickovic et al. (2017) has performed an extensive set of evaluations in their work, we will use their reported results forbaseline methods. It is also worthwhile to note that our method only has a few more parameters compared with GAT (e.g., nonparametric decay profile u, $c$ in RWR, and the mixing ratios in (6)). For example, for Cora data set, GAT model has about 91k parameters, while our method adds around 30 extra parameters on top of it (about $0.03\%$ of the original model size).  </p>
<p>Table 2: Classification accuracy for different transductive methods on the benchmark data sets.   </p>
<p><img alt="" src="images/2026ff94a73ae4e7e3e946969ac42f5bf7b581f8627f8404bd20299b1969ccbd.jpg" />  </p>
<p><img alt="" src="images/928b7aef215da8923ec53dfbbf35534cc104e6a534e094cb840c682f2ce7e595.jpg" /><br />
Figure 5: Convergence of the accuracy and loss of the proposed method and GAT method.  </p>
<p>In Figure 5, we plot evolution of the testing accuracy and loss function (on the test split) for GAT and our method (ADSF-RWR). Since the loss functions of the two methods are the same, they are directly comparable. The accuracy of GAT fuctuates through iterations, while our approach is more stable and converges to a higher accuracy. The objective value of our method also converges faster thanks to the utilization of higher-order neighborhood information through structural fingerprints.  </p>
<h1>4.2ABLATION STUDIES</h1>
<p>In this section, We further examined some interesting details of the proposed method, all using the Cora data set and the RWR fingerprint construction scheme.  </p>
<p>Table 3: Performance of GAT under different choices of the neighborhood range.   </p>
<p><img alt="" src="images/4911c3af988b859650ed50beb1c0e3bea184f2167da0ef05b99332372e3715aa.jpg" />  </p>
<p>Impact of the fingerprint size. Intuitively, the structural fingerprint should neither be too small or too large. We have two steps to control the effective size of the fingerprint. In constructing the fingerprint, we will first choose neighboring nodes within a certain range, namely the $k$ -hop neighbors; then we will fine-tune the weight of each node in the fingerprint, either through nonparametric decay profile or the $c$ parameter in random walk. Here, for comparison, we have fixed $c=0.5$ and varied $k$ as $k=1,2,3$ . As shown in Figure 6(a), the optimal order of neighborhood is two in this setting.  </p>
<p>Impact of the attention range. In GAT, only direct neighbors have attention to each other, namely the attention range is $k^{\prime}=1$ . In our approach, the optimal choice is $k^{\prime}=2$ . The attention range determines the number of neighboring nodes involved in message passing. In the Cora dataset, the average 1st-order neighborhood size is $|\mathcal{N}_{1}|={\bf3.9}$ , while for 2nd-order neighbors $|\mathcal{N}_{2}|\,=\,42.5$ Namely, our approach has involved 10 times more neighbors in message passing than GAT, yet the performance is improved instead. This is an evidence that our approach helps alleviate oversmoothing in GAT (or GNN's in general) by exploiting high-order structural details more effectively.  </p>
<p><img alt="" src="images/f4b58384f6cc4c745046077479d020c8734e974294e226420e475de0eacb9fd6.jpg" /><br />
Figure 6: Detailed studies of the adaptive structural fingerprint method.  </p>
<p>Impract of the re-start probability in RWR. In Figure 6(b), we plot the learning performance w.r.t. the choice of the $c$ parameter in RWR when fixing the neighborhood range $k=2$ . As can be seen, the best performance is around $c=0.5$ , meaning that the “random walking” and “restarting” should be given similar chances within 2-hop neighbors. Empirically, we can always use back-propagation to optimize the choice of $c$ , which can be more adaptive to the choice of the $k$ -hop neighbors.  </p>
<p>Non-parametric decay profile. In Figure 6(c), we plot the non-parametric decay profile $\mathbf{u}$ learned by our method, when setting the neighborhood orders to $k\,=\,3$ .As can be seen, the first-order and second-order neighbors have higher weights, while the third-order neighbors almost have zero weights, meaning that they almost make no contributions to computing the structural attention. This is consistent to our evaluations in Figure 5(a), and demonstrates the power of our method in identifying useful high-order neighbors.  </p>
<h1>4.3 IMPACT OF NEIGHBORHOOD RANGE IN GAT</h1>
<p>As suggested by the anonymous reviewer, we perform an empirical study on the GAT performance when larger attention range is considered. Note that in the original GAT by Velickovic et al. (2017), attention score is computed only for nodes that are direct neighbors with each other. Here, we increase the domain of interaction to up to 2-hop and 3-hop neighbors, with all other components remaining the same. As can be seen from Table 3, the performance of GAT is the best when choosing only 1-hop neighbors to compute attention scores, and deteriorates rapidly when larger neighborhood is considered.  </p>
<p>We speculate that although larger attention range involves more neighbors and possibly more useful nodes, it bring a significant number of noisy nodes as well. As a result, the GAT performance is negatively affected. This seems to indicate that feature-based attention alone might not be sufficient in identifying noisy neighbors. In our approach, attention scores are computed from both node features and informative structural details, therefore it enables exploring a wider collection of higherorder-neighbors while simultaneously removing the impact of irrelevant nodes, validating the benefit of structure-based attention.  </p>
<h1>5  CONCLUSION AND FUTURE WORK</h1>
<p>In this work, we proposed an adaptive structural fingerprint model to encode complex topological and structural information of the graph to improve graph representation learning. In the future, we will consider varying fingerprint parameters (e.g. decay profile) instead of sharing them across all the nodes; we will consider applying our approach to graph partitioning and community detection, where node features might be unavailable and graph structures will be the main source of information that can be explored; we will also extend our approach to the challenging problem of graph classification. On the theoretical side, we will borrow existing tools in semi-supervised learning and study the generalization performance of our approach on semi-supervised node embedding and classification.  </p>
<h1>ACKNOWLEDGEMENT</h1>
<p>We highly appreciate the valuable comments from anonymous reviewers, which allow us to significantly improve both the presentation and experimental design of our work. This work is supported in part by the National Science Foundation of China No.61672236, National Science Foundation of China No.61573107, Shanghai National Science Foundation 17ZR1444200, Shanghai Municipal Science and Technology Major Project No.2018SHZDZX01.  </p>
<h1>REFERENCES</h1>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, 2015.  </p>
<p>Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of machine learning research, 7:2399-2434,2006. <br />
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In International Conference on Learning Representations, 2014. <br />
Olivier Chapelle and Alexander Zien. Semi-supervised classification by low density separation. In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, 2005. <br />
Dorin Comaniciu and Peet Meer. Mean shift: a robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24:603 - 619, 08 2002. <br />
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems,2016. <br />
David K . Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems, 2015. <br />
Michelle Girvan and Mark Newman. Girvan, m. newman, m. e. j. community structure in social and biological networks. proc. natl acad. sci. usa 99, 7821-7826. Proceedings of the National Academy of Sciences of the United States of America, 99:7821-6, 07 2002. <br />
Marco Gori, Gabriele Monfardini, Gori ScarselliMarco, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. <br />
William L . Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on lare graphs. In Neural Information Processing Systems, 2017. <br />
Mikael Henaff Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data. In arXiv: 1506.05163, 2015. <br />
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. <br />
Yann LeCun and Clément Kavukcuoglu, Koray Farabet. Convolutional networks and applications in vision. In Proceedings of IEEE International Symposium on Circuits and Systems, 2010. <br />
John Boaz Lee, Ryan Ross, and Xiangnan Kong. Graph classification using structural attention. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery Data Mining, 2018. <br />
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In 32nd AAAI Conference on Artificial Intelligence, 2018. <br />
L. Lovasz. Random walks on graphs: a survey. Combinatorics, 2:1-46, 1993. <br />
Qing Lu and Lise Getoor. Link-based classification. In Proceedings of the 2Oth International Conference on Machine Learning, 2003. <br />
Minh-Thang Luong, Hieu Pham, and Christoper Manning. Effective approaches to attention-based neural machine translation. 08 2015. doi: 10.18653/v1/D15-1166. <br />
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M. Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In arXiv:1611.08402, 2016. <br />
Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In AAAl Conference on Artificial Intelligence, 2019. <br />
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In In Proceedings of The 33rd International Conference on Machine Learning, 2016. <br />
Jia-Yu Pan, HyungJeong Yang, Pinar Duygulu, and Christos Faloutsos. Automatic multimedia cross-modal correlation discovery. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2004. <br />
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Procedings of the 2Oth ACM SIGKDD international conference on Knowledge discovery and data mining, 2014. <br />
Ryan A.Rossi, Nesreen K. Ahmed, and Eunyee Koh. Higher-order network representation learning. In WWw Web Conference, 2019. <br />
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008. <br />
Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22:888-905, 2000. <br />
A. Sperduti, A. andStarita. Trans. neur. netw. Supervised neural networks for the classification of structures., 8:714-735, 1997. <br />
Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan. Fast random walk with restart and its applia tions. In Proceedings of the Sixth International Conference on Data Mining, 2006. <br />
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2017. <br />
Jason Weston, Frederic Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semisupervised embedding. Neural Networks: Tricks of the Trade, pp. 639-655, 2012. <br />
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the 32 nd International Conference on Machine Learning, 2015. <br />
Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. <br />
Jiawei Zhang and Lin Meng. Gresnet: Graph residual network for reviving deep GNNs from suspended animation. In arXiv.org. 2019. <br />
Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the 2Oth International conference on Machine learning,2003.  </p>
    </body>
    </html>