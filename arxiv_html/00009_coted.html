<!doctype html>
    <html>
    <head>
        <meta charset="UTF-8">
        <link rel="stylesheet" media="all" href="normalize.css">
        <link rel="stylesheet" media="all" href="core.css">
        <link rel="stylesheet" media="all" href="style.css">
        <script type="text/javascript"
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
    
        <script type="text/javascript">
          MathJax.Hub.Config({
            tex2jax: {
              inlineMath: [['$', '$'], ['\(', '\)']],
              displayMath: [['$$', '$$'], ['\[', '\]']],
              processEscapes: true
            }
          });
        </script>
        
</head>
    <body>
    <h1>HYPER-SAGNN: A SELF-ATTENTION BASED GRAPH NEURAL NETWORK FOR HYPERGRAPHS</h1>
<p>Ruochi Zhang School of Computer Science Carnegie Mellon University  </p>
<p>Yuesong Zou School of Computer Science Carnegie Mellon University IIIS, Tsinghua University  </p>
<p>Jian Ma School of Computer Science Carnegie Mellon University jianma@cs.cmu.edu  </p>
<p>ABSTRACT  </p>
<p>Graph representation learning for hypergraphs can be used to extract patterns among higher-order interactions that are critically important in many real world problems. Current approaches designed for hypergraphs, however, are unable to handle different types of hypergraphs and are typically not generic for various learning tasks. Indeed, models that can predict variable-sized heterogeneous hyperedges have not been available. Here we develop a new self-attention based graph neural network called Hyper-SAGNN applicable to homogeneous and heterogeneous hypergraphs with variable hyperedge sizes. We perform extensive evaluations on multiple datasets, including four benchmark network datasets and two single-cell Hi-C datasets in genomics. We demonstrate that Hyper-SAGNN significantly outperforms the state-of-the-art methods on traditional tasks while also achieving great performance on a new task called outsider identification. Hyper-SAGNN will be useful for graph representation learning to uncover complex higher-order interactions in different applications.  </p>
<h1>1 INTRODUCTION</h1>
<p>Graph structure is a widely used representation for data with complex interactions. Learning on graphs has also been an active research area in machine learning on how to predict or discover patterns based on the graph structure (Hamilton et al., 2017b). Although existing methods can achieve strong performance in tasks such as link prediction and node classification, they are mostly designed for analyzing pair-wise interactions and thus are unable to effectively capture higher-order interactions in graphs.  In many real-world applications, however, relationships among multiple instances are key to capturing critical properties, e.g., co-authorship involving more than two authors or relationships among multiple heterogeneous objects such as “(human, location, activity)". Hypergraphs can be used to represent higher-order interactions (Zhou et al., 2007). To analyze higher-order interaction data, it is straightforward to expand each hyperedge into pair-wise edges with the assumption that the hyperedge is decomposable.  </p>
<p>Several previous methods were developed based on this notion (Sun et al., 2008; Feng et al., 2018). However, earlier work DHNE (Deep Hyper-Network Embedding) (Tu et al., 2018) suggested the existence of heterogeneous indecomposable hyperedges where relationships within an incomplete subset of a hyperedge do not exist. Although DHNE provides a potential solution by modeling the hyperedge directly without decomposing it, due to the neural network structure used in DHNE, the method is limited to the fixed type and fixed-size heterogeneous hyperedges and is unable to consider relationships among multiple types of instances with variable size. For example, Fig. 1 shows a heterogeneous co-authorship hypergraph with two types of nodes (corresponding author and coauthor). Due to the variable number of both authors and corresponding authors in a publication, the hyperedges (co-authorship) have different sizes or types. Unfortunately, methods for representation learning of heterogeneous hypergraph with variable-sized hyperedges, especially those that can predict variable-sized hyperedges, have not been developed.  </p>
<p><img alt="" src="images/ace7c2a21a903a7a83a72e8d2ea3c063adb8f03e8eec7189ffe4c875efb486f6.jpg" /><br />
Figure1: Anexample of the coauthorship hypergraph. Here authors are represented as nodes (in dark blue and light blue) and coauthorships are represented as hyperedges.  </p>
<p>In this work, we developed a self-attention based graph neural network, called Hyper-SAGNN that can work with both homogeneous and heterogeneous hypergraphs with variable hyperedge size. Using the same datasets in the DHNE paper (Tu et al., 2018), we demonstrated the advantage of Hyper-SAGNN over DHNE in multiple tasks. We further tested the effectiveness of the method in predicting edges and hyperedges and showed that the model can achieve better performance from the multi-tasking setting. We also formulated a novel task called outsider identification and showed that Hyper-SAGNN performs strongly. Importantly, as an application of Hyper-SAGNN to singlecell genomics, we were able to learn the embeddings for the most recently produced single-cell Hi-C (scHi-C) datasets to uncover the clustering of cells based on their 3D genome structure (Ramani et al., 2017; Nagano et al., 2017). We showed that Hyper-SAGNN achieved improved results in identifying distinct cell populations as compared to existing scHi-C clustering methods. Taken together, Hyper-SAGNN can significantly outperform state-of-the-art methods and can be applied to a wide range of hypergraphs for different applications.  </p>
<h1>2 RELATED WORK</h1>
<p>Deep learning based models have been developed recently to generalize from graphs to hypergraphs (Gui et al., 2016; Tu et al., 2018). The HyperEdge Based Embedding (HEBE) method (Gui et al., 2016) aims to learn the embeddings for each object in a specific heterogeneous event by representing it as a hyperedge. However, as demonstrated in Tu et al. (2018), HEBE does not perform well on sparse hypergraphs. Notably, previous methods typically decompose the hyperedge into pair-wise relationships where the decomposition methods can be divided into two categories: explicit and implicit. For instance, given a hyperedge $(v_{1},v_{2},v_{3})$ , the explicit approach would decompose it directly into three edges, $(v_{1},v_{2}),(v_{2},v_{3}),(v_{1},v_{3})$ , while the implicit approach would add a hidden node $e$ representing the hyperedge before decomposition, i.e., $(\bar{v}_{1},e),(\bar{v}_{2},e),(v_{3},e)$ .The deep hypergraph embedding (DHNE) model, however, directly models the tuple-wise relationship using MLP (Multilayer Perceptron). The method is able to achieve better performance on multiple tasks as compared to other methods designed for graphs or hypergraphs such as Deepwalk (Perozzi et al., 2014), node2vec (Grover &amp; Leskovec, 2016), and HEBE. Unfortunately, the structure of MLP takes fixed-size input, making DHNE only capable of handling $k$ -uniform hypergraphs, i.e., hyperedges containing $k$ nodes. To use DHNE for non- $k$ -uniform hypergraphs or hypergraphs with different types of hyperedges, a function for each type of hyperedges needs to be trained individually, which leads to significant computational cost and loss of the capability to generalize to unseen types of hyperedges. Similarly, heterogeneous hyper-network embedding (Baytas et al., 2018) also used MLP as part of the model which requires fixed size input to train the model. Another recent method, hyper2vec (Huang et al., 2019), can also generate embeddings for nodes within the hypergraph and outperforms other hypergraph embedding methods such as HGE (Yu et al., 2018) in the node classification task. However, hyper2vec cannot solve the link prediction problem directly as it only generates the embeddings of nodes in an unsupervised manner without a learned function to map from embeddings of nodes to hyperedges. Also, for $k$ -uniform hypergraphs, hyper2vec is equivalent to node2vec, which cannot capture the high-order network structures for indecomposable hyperedges (as shown in Tu et al. (2018)). Moreover, graph neural network based methods (Yadati et al., 2018; Feng et al., 2019; Bai et al., 2019) have been proposed to generalize the convolution operation or attention mechanism from graphs to hypergraphs. However, these methods mainly focus on the hypergraphs where node attributes are known and are typically used for semi-supervised node classification tasks. Similar to hyper2vec, these methods cannot be directly used for predicting hyperedges. Our Hyper-SAGNN in this work addresses all these challenges with a self-attention based graph neural network that can learn embeddings of the nodes and predict hyperedges for non- $k$ -uniform heterogeneous hypergraphs.  </p>
<h1>3 METHOD</h1>
<h1>3.1 DEFINITIONS AND NOTATIONS</h1>
<p>Definition 1. (Hypergraph) A hypergraph is defined as $G\;=\;(V,E)$ ,where $V\;=\;{v_{1},...,v_{n}}$ repesentsthf ndetgrapad(repreents tsf hyperedges within a hypergraph have the same size of $k$ , it is called a $k$ -uniform hypergraph. Note  </p>
<p>that even if a hypergraph is $k$ -uniform, it can still have different types of hyperedges because the node type can vary for nodes within the hyperedges.  </p>
<p>Definition 2. (The hyperedge prediction problem) We formally define the hyperedge prediction problem. For a given tuple $\left(v_{1},v_{2},...,v_{k}\right)$ , our goal is to learn a function $f$ that satisfies:  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We aim to develop a function $f$ that predicts the likelihood of a hyperedge $(v_1, v_2,..., v_k)$ existing in the hypergraph $G$. To achieve this, we need to define the function's output such that it returns a value greater than or equal to a threshold $s$ when the hyperedge is present in the graph, and a value less than $s$ when it is not., 
 before perplexity: 1.0588975630974995, after perplexity: 0.9988402607146695</p>
<p>Completions End </p>
<p></span>$$
f{\big(}v_{1},v_{2},...,v_{k}{\big)}={\left{\begin{array}{l l}{\geq s,}&amp;{{\mathrm{if~}}(v_{1},v_{2},...,v_{k})\in E}\ {&lt;s,}&amp;{{\mathrm{if~}}(v_{1},v_{2},...,v_{k})\notin E}\end{array}\right.}
$$  </p>
<p>where $s$ is the threshold to binarize the continuous value of $f$ into a label, which indicates whether the tuple is an hyperedge or not. Specifically, when we are given the pre-trained embedding vectors or the features of nodes $X={x_{1},...,x_{i}}$ , we can rewrite this function as:  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We aim to learn a function f that predicts whether a given tuple of nodes is a hyperedge or not. To do this, we need to define f in terms of the input node features X, and we achieve this by applying a transformation function g to each node embedding x_i, resulting in a transformed feature representation g(x_i) that we can use as input to f., 
 before perplexity: 1.5245950267080364, after perplexity: 1.2691751009429266</p>
<p>Completions End </p>
<p></span>$$
f(v_{1},v_{2},...,v_{k})\triangleq f(g(x_{1}),g(x_{2}),...,g(x_{k}))
$$  </p>
<p>where the vectors $g(x_{i})$ can be considered as the fine-tuned embedding or embedding vectors for the nodes. For convenience, we refer to $x_{i}$ as the features and $g(x_{i})$ as the learned embeddings.  </p>
<h1>3.2 STRUCTURE OF HYPER-SAGNN</h1>
<p>Our goal is to learn the functions $f$ and $g$ that take tuples of node features $(x_{1},...,x_{k})$ as input and produce the probability of these nodes forming a hyperedge. Without the assumption that the hypergraph is $k$ -uniform and the type of each hyperedge is identical, we require that $f$ can take variable-sized, non-ordered input. Although simple functions such as average pooling $\begin{array}{r}{f(g(x_{1}),...,g(x_{k}))=\frac{1}{K}\sum_{i=1}^{k}g(x_{i})}\end{array}$ satisfy this tuple-wise condition, previous work showed that the linear function is not sufficient to model this relationship (Tu et al., 2018). DHNE used an MLP to model the non-linear function, but it requires that an individual function needs to be trained for different types of hyperedges. Here we propose a new method to tackle the general hyperedge prediction problem.  </p>
<p>Graph neural network based methods such as GraphSAGE (Hamilton et al., 2017a) typically define a unique computational graph for each node, allowing it to perform efficient information aggregation for nodes with different degrees. Graph Attention Network (GAT) (Velickovic et al., 2017) utilizes a self-attention mechanism in the information aggregation process. Motivated by these properties, we propose our method Hyper-SAGNN based on the self-attention mechanism within each tuple to learn the function $f$  </p>
<p>Wefirst briefly introduce the selfattention mechanism. We use the same terms as the self-attention mechanism described in Vaswani et al. (2017); Velickovic et al. (2017). Given a group of nodes $(\vec{x}_{1},\vec{x}_{2},...,\vec{x}_{k})$ and  weight matrices $W_{Q},W_{K},W_{V}$ that represent linear transformations of features before applying the dot-product attention to be trained, we first compute the attention coefficients that reflect the pairwise importance of nodes:  </p>
<p><img alt="" src="images/980e87708ef56f1505bf88d92de440b908aea6288d288db3e1a522d7889e187a.jpg" /><br />
Figure 2: Structure of the neural network used in HyperSAGNN. The input $\left(\vec{x}_{1},\vec{x}_{2},...,\vec{x}_{k}\right)$ , representing the features for nodes 1 to $k$ ,passes through two branches of the network resulting in static embeddings $\left(\vec{s}_{1},\vec{s}_{2},...,\vec{s}_{k}\right)$ and dynamic embeddings $(\vec{d_{1}},\vec{d_{2}},...,\vec{d_{k}})$ , respectively. The layer for generating dynamic embeddings is the multi-head attention layer. An example for its mechanism on node 1 here is shown in the figure as well. Then the pseudo-euclidean distance of each pair of static and dynamic embeddings is calculated by onelayered position-wise feed-forward network to produce probability scores $\left(p_{1},p_{2},...,p_{k}\right)$ . These scores are further averaged to represent whether this group of nodes form a hyperedge.  </p>
<p>$$
e_{i j}=\left(W_{Q}^{T}x_{i}\right)^{T}\left(W_{K}^{T}x_{j}\right),\forall1\leq i,j\leq k
$$  </p>
<p>We then normalize $e_{i j}$ by all possible $j$ within the tuple through the softmax function, i.e.,  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We first compute the attention coefficients that reflect the pairwise importance of nodes by taking the dot product of the query and key vectors for each pair of nodes. To obtain the attention coefficients, we normalize the dot product by all possible key vectors within the tuple using the softmax function, resulting in the equation: </p>
<p>$$
\alpha_{i j}=\frac{\exp(e_{i j})}{\sum_{1\le l\le k}\exp(e_{i l})}
$$, 
 before perplexity: 1.5886901950029455, after perplexity: 1.1004882583526794</p>
<p>Completions End </p>
<p></span>$$
\alpha_{i j}=\frac{\exp(e_{i j})}{\sum_{1\le l\le k}\exp(e_{i l})}
$$  </p>
<p>Finally, a weighted sum of the transformed features with an activation function is calculated:  </p>
<p><span style="color:blue"></p>
<p>Completions Start:</p>
<p>COT: We first compute the attention coefficients that reflect the pairwise importance of nodes by normalizing the dot product of the query and key vectors using the softmax function. This is achieved by calculating the exponential of the dot product and dividing it by the sum of the exponential of the dot product with all possible nodes in the tuple., 
 before perplexity: 1.3508689126177078, after perplexity: 1.308154874228322</p>
<p>Completions End </p>
<p></span>$$
\vec{d}_{i}=\operatorname{tanh}\left(\sum_{1\leq j\leq k,i\neq j}\alpha_{i j}W_{V}^{T}x_{j}\right)
$$  </p>
<p>In GAT, each node is applied to the self-attention mechanism usually with all its first-order neighbors. In Hyper-SAGNN, we aggregate the information for a node $v_{i}$ only with its neighbors for a given tuple. The structure of Hyper-SAGNN is illustrated in Fig. 2.  </p>
<p>The input to our model can be represented as tuples, i.e., $(\vec{x}_{1},\vec{x}_{2},...,\vec{x}_{k})$ . Each tuple first passes through a position-wise feed-forward network to produce $\left(\vec{s}_{1},\vec{s}_{2},...,\vec{s}_{k}\right)$ ,where $\vec{s}_{i}=\mathrm{tanh}(\dot{W}_{s}^{T}\vec{x}_{i})$ We refer to each $\vec{s_{i}}$ as the static embedding for node $i$ since it remains the same for node $i$ no matter what the given tuple is. The tuple also passes through a multi-head graph attention layer to produce a new set of node embedding vectors $({\dot{\vec{d_{1}}}},{\vec{d_{2}}},...,{\vec{d_{k}}})$ , which we refer to as the dynamic embeddings because they are dependent on all the node features within this tuple.  </p>
<p>Note that unlike the standard attention mechanism described above, when calculating $\vec{d}_{i}$ ,werequire that $j\neq i$ in Eqn. (5). In other words, we exclude the term $\alpha_{i i}W_{V}^{T}x_{i}$ in the calculation of dynamic embeddings. Based on our results we found that including $\vec{\alpha}_{i i}$ would lead to either similar or worse performance in terms of hyperedge prediction and node classification (see Appendix A.6 for details). We will elaborate on the motivation of this choice later in this section.  </p>
<p>With the static and dynamic embedding vectors for each node, we calculate the Hadamard power (element-wise power) of the difference of the corresponding static/dynamic pair. It is then further passed through a one-layered neural network with sigmoid as the activation function to produce a probability score $p_{i}$ . Finally, all the output $p_{i}\,\in\,[0,1]$ is averaged to get the final $p$ i.e.,  </p>
<p>$$
\begin{array}{l l}{\displaystyle o_{i}=W_{o}^{T}((\vec{d_{i}}-\vec{s_{i}})^{\circ2})+b}\ {\displaystyle p=\frac{1}{K}\sum_{i=1}^{k}p_{i}=\frac{1}{K}\sum_{i=1}^{k}\sigma(o_{i})}\end{array}
$$  </p>
<p>By design, $o_{i}$ can be regarded as the squared weighted pseudo-euclidean distance between the static embedding $\vec{s}_{i}$ and the dynamic one $\vec{d}_{i}$ . It is called pseudo-euclidean distance because we do not require the weight to be non-zero or to sum up to 1. One rationale for allowing negative weights when calculating the distance could be the Minkowski space where the distance is defined as $d^{2}=x^{2}+\dot{y}^{2}+z^{2}-t^{2}$ . Therefore, for these high-dimensional embedding vectors, we do not specifically treat them as euclidean vectors.  </p>
<p>Our network essentially aims to build the correlation of the average “distance” of the static/dynamic embedding pairs with the probability of the node group forming a hyperedge. Since the dynamic embedding is the weighted sum of features (with potential non-linear transformation) from neighbors within the tuple, this "distance” reflects how well the static embedding of each node can be approximated by the features of their neighbors within that tuple. This design strategy shares some similarities with the CBOW model in natural language processing (Mikolov  </p>
<p><img alt="" src="images/888f0663aa15599ce17f4721c9991c9fe4df1864d39e4f91121f1bca540cb5cb.jpg" /><br />
Figure 3: Mllustration of the method for generating node features for node $i$ in the hypergraph. In the walk based approach, a biased random walk on hypergraphs is used to produce walking paths (the yellow circles in the walking paths represent node i). These walks are further used to train a skip-gram model to generate features. In the encoder based approach, the $i\cdot$ -th row of the adjacency matrix (as shown in the figure where the orange/white blocks represent whether or not node $i$ is adjacent to other nodes in the graph) is used as the input to an autoencoder. The output of the encoder part is used as the features for node $i$  </p>
<p>et al., 2013), where the model aims to predict the target word given its context (see Appendix A.5 for the analysis of the static/dynamic embedding pairs). In principle, we could still include the $\vec{\alpha}_{i i}$ term to obtain the embedding $\vec{d}_{i}^{_}$ . Alternatively, we can directly pass $\vec{d}_{i}^{_}$ through a fully connected layer to produce $p_{i}^{*}$ while the rest remains the same. However, we argue that our proposed model would be able to produce $s_{i}$ that can be directly used for tasks such as node classification while the alternative approach is unable to achieve that (see Appendix A.6 for detailed analysis).  </p>
<h1>3.3 APPROACHES FOR GENERATING FEATURES</h1>
<p>In an inductive learning setting with known attributes for the nodes, ${\vec{x}}_{i}$ can just be the attributes of the node. However, in a transductive learning setting without knowing the attributes of the nodes, we havetogenerate ${\vec{x}}_{i}$ based on the graph structure solely. Here we use two existing strategies to generatefeatures ${\vec{x}}_{i}$ . As shown in Fig. 3, the first approach is the random walk based method. We designed a biased random walk scheme for nodes in hypergraphs and used that to sample walks. Then, similar to node2vec, a skip-gram model is trained to generate features. The second approach is the encoder based approach where the corresponding row of the adjacency matrix is used as the features. The features are further passed through an autoencoder-like structure to reduce the dimensionality with the output of the hidden layer used as the features. The detailed description of these two approaches can be found in Appendix A.1.  </p>
<h1>4 RESULTS</h1>
<p>We sought to compare Hyper-SAGNN with the state-of-the-art method DHNE as it has already been demonstrated with superior performance over previous algorithms such as Deep Walk, LINE, and HEBE. We first used the same four datasets in the DHNE paper to have a direct comparison. The details on these datasets can be found in the Appendix A.2. The details on the parameters used in this section for both Hyper-SAGNN and other methods can also be found in the Appendix A.3. The details of the tasks and the evaluation metrics used in this section are explained in the Appendix A.4.  </p>
<h1>4.1  PERFORMANCE COMPARISON WITH EXISTING METHODS</h1>
<p>We evaluated the effectiveness of our embedding vectors and the learned function with the network reconstruction task. We compared our Hyper-SAGNN using the encoder based approach and also the model using the random walk based pre-trained embeddings against DHNE and the baseline node2vec. We first trained the model and then used the learned embeddings to predict the hyperedges of the original network. We sampled the negative samples to be 5 times the amount of the positive samples following the same setup of DHNE. We evaluated the performance based on both the AUROC (Area Under the Receiver Operating Characteristic curve) and the AUPR (Area under the Precision-Recall curve). As shown in Table 1, Hyper-SAGNN can capture the network structure better than DHNE over all datasets either using the encoder based approach or the random walk based approach.  </p>
<p>Table 1: AUC and AUPR values for network reconstruction. The models trained with the random walk based approach and the encoder based approach are marked as Hyper-SAGNN-W and Hyper-SAGNN-E, respectively.   </p>
<p><img alt="" src="images/ba2ba1bb1827521d4b75fd30fb4db37100acaecacc687ad0b4e559eb8c107d3e.jpg" />  </p>
<p>We further assessed the performance of Hyper-SAGNN on the hyperedge prediction task. We randomly split the hyperedge set into training and testing set by a ratio of 4:1. The way to generate negative samples is the same as the network reconstruction task. As shown in Table 2, our model again achieves significant improvement over DHNE for predicting the unseen hyperedges. The most significant improvement is from the wordnet dataset, which is about a $24.6\%$ increase on theAUPR score. For network reconstruction and hyperedge prediction tasks, the difference between the random walk based Hyper-SAGNN and the encoder based Hyper-SAGNN is minor.  </p>
<p>In addition to the tasks related to the prediction of hyperedges, we also evaluated whether the learned embeddings are effective for node classification tasks. A multi-label classification experiment and a multi-class classification experiment were carried out for the MovieLens dataset and the wordnet dataset, respectively. We used Logistic Regression as the classifier. The proportion of the training data was chosen to be from $10\%$ to $90\%$ for the MovieLens dataset, and $1\%$ to $10\%$ for thewordnet dataset. We used averaged Mirco-F1 and Macro-F1 to evaluate the performance. The results are in Fig. 4. We observed that Hyper-SAGNN consistently achieves both higher Micro-F1 and Macro-F1 scores over DHNE for different fractions of the training data. Also, Hyper-SAGNN based on the random walk generally achieves the best performance (Hyper-SAGNN-W in Fig. 4).  </p>
<p><img alt="" src="images/c5f4208583cd989a9de4fe90fab42ffa04e154baf0ee6f2880e073319d8692ee.jpg" /><br />
Figure 4: Performance of classification on MovieLens and wordnet datasets. Hyper-SAGNNs trained with the random walk based approach and the encoder based approach are marked as Hyper-SAGNN-W, HyperSAGNN-E, respectively. The models trained with a mix of edges and hyperedges are denoted with “(mix)".  </p>
<h1>4.2  PERFORMANCE ON NON- $k$ -UNIFORM HYPERGRAPH</h1>
<p>Next, we evaluated Hyper-SAGNN using the non- $k$ -uniform heterogeneous hypergraph. For the above four datasets, we decomposed each hyperedge into 3 pairwise edges and added them to the existing graph. We trained our model to predict both the hyperedges and the edges (i.e., nonhyperedges). We then evaluated the performance for link prediction tasks for both the hyperedges and the edges. We also performed the node classification task following the same setting as above. The results for link prediction are in Table 2. Fig. 4 shows the results for the node classification task.  </p>
<p>Table 2: Performance evaluation based on AUROC and AUPR for hyperedge/edge prediction. Methods with annotation (mix) represent Hyper-SAGNN trained with a mixture of edges and hyper-edges. Datasets marked with *(2)" represent the performance on pair-wise edge prediction (i.e., non-hyperedges).  </p>
<p><img alt="" src="images/eeb20d153881972c90eb5a9f5a925e927ad1013c5539bdcc180e468a6abe921f.jpg" />  </p>
<p>We observed that Hyper-SAGNN can preserve the graph structure on different levels. Compared to training the model with hyperedges only, including the edges into the training would not cause obvious changes in performance for hyperedge predictions (about a $1\%$ fluctuation for AUC/AUPR).  </p>
<p>We then further assessed the model in a new evaluation setting where there are adequate edges but only a few hyperedges presented. We asked whether the model can still achieve good performance on the hyperedge prediction based on this dataset. This scenario is possible in real-world applications especially when the dataset is combined from different sources. For example, in the drug dataset, it is possible that, in addition to the (user, drug, reaction) hyperedges, there are also extra edges that come from other sources, e.g., (drug, reaction) edges from the drug database, (user, drug) and (user, reaction) edges from the medical record. Here for each dataset that we tested, we used $50\%$ of the edges and only $5\%$ of the hyperedges to train the model. The results are in Fig. 5.  </p>
<p>When using only the edges to train the model, our method still achieves higher AUROC and AUPR score for hyperedge prediction as compared to node2vec (Table 2). We found that when the model is trained with both the downsampled hyperedge dataset and the edge dataset, it would be able to reach higher performance or suffer less from overfitting than being trained with each of the datasets individually. This demonstrates that our model can capture the consensus information on the graph structure across different sizes of hyperedges.  </p>
<p><img alt="" src="images/51a4b20b95cdb7df4db3f676a6eed3aa45a013f0706cc05cb12b6aa85bc93c03.jpg" /><br />
Figure 5: AUROC and AUPR scores of Hyper-SAGNN for hyperedge prediction on the downsampled dataset over training epochs.  </p>
<h1>4.3 OUTSIDER IDENTIFICATION</h1>
<p>In addition to the standard link prediction and node classification, we further formulated a new task called “outsider identification". Previous methods such as DHNE can answer the question of whether a specific tuple of nodes $(v_{1},v_{2},...,v_{k})$ form a hyperedge. However, in many settings, we might also want to know the reason why this group of nodes will not form a hyperedge. We first define the outsider of a group of nodes as follows. Node $v_{i}$ is the outsider of the node group $\left(v_{1},v_{2},...,v_{k}\right)$ if itsatisfies:  </p>
<p>$$
\begin{array}{r l}&amp;{\exists e\in E,(v_{1},v_{2},...,v_{i-1},v_{i+1},...,v_{k})\in e}\ &amp;{\nexists e\in E,s.t.\;\exists j\in{1,2,..,k},j\neq i,(v_{i},v_{j})\in e}\end{array}
$$  </p>
<p>We speculated that Hyper-SAGNN can answer this question by analyzing the probability score $p_{1}$ to $p_{k}$ (defined in Eqn. 7). We assume that the node $v_{i}$ with the smallest $p_{i}$ would be the outsider. We then set the evaluation as follows. We first trained the model as usual, but at the final stage, we replaced the average pooling layer with the min pooling layer and fine-tuned the model for several epochs. We then fed the generated triplets with known outsider node into the trained model and calculated the top- ${\cdot k}$ accuracy of the outsider node matching the node with the smallest probability. Because this task is based on the prediction results of the hyperedges, we only tested on the dataset that achieves the best hyperedge prediction, i.e., the drug dataset. We found that we have $81.9\%$ accuracy for the smallest probability and $95.3\%$ accuracy for the top-2 smallest probability. These results show that by switching the pooling layer we would have better outsider identification accuracy (from $78.5\%$ to $81.9\%$ ) with the cost of slightly decreased hyperedge prediction performance (AUC from 0.955 to 0.935). This demonstrates that our model is able to accurately predict the outsider within the group even without further labeled information. Moreover, the performance of outsider identification can be further improved if we include the cross-entropy between $p_{i}$ and the label of whether $v_{i}$ is an outsider for all applicable triplets in the loss term. Together, these results show the advantage of Hyper-SAGNN in terms of the interpretability of hyperedge prediction.  </p>
<h1>4.4 APPLICATION TO SINGLE-CELL HI-C DATASETS</h1>
<p>We next applied Hyper-SAGNN to the recently produced single-cell Hi-C (scHi-C) datasets (Ramani et al., 2017; Nagano et al., 2017). Genome-wide mapping of chromatin interactions by HiC (Lieberman-Aiden et al., 2009; Rao et al., 2014) has enabled comprehensive characterization of the 3D genome organization that reveals patterns of chromatin interactions between genomic loci. However, unlike bulk Hi-C data where signals are aggregated from cell populations, scHi-C provides unique information about chromatin interactions at single-cell resolution, thus allowing us to ascertain cell-to-cell variation of the 3D genome organization. Specifically, we propose that scHi-C makes it possible to model the cell-to-cell variation of chromatin interaction as a hyperedge, i.e., (cell, genomic locus, genomic locus). Note that the hyperedege here is “partially non-ordered", namely (cell $i$ , locus $j$ , locus $k$ ) should be equivalent to (cell $i$ , locus $k$ , locus $j]$ 0. Our method is able to guarantee that while DHNE cannot achieve that directly. For the analysis of scHi-C, the most common strategy would be revealing the cell-to-cell variation by embedding the cells based on the contact matrix and then applying the clustering algorithms such as $K_{\l}$ -means clustering on the embedded vectors. We performed the following evaluation to assess the effectiveness of HyperSAGNN on learning the embeddings of cells by representing the scHi-C data as hypergraphs.  </p>
<p>We tested Hyper-SAGNN on two datasets. The first one consists of scHi-C from four human cell lines: HAP1, GM12878, K562, and HeLa (Ramani et al., 2017). The second one includes the scHiC that represents the cell cycle of the mouse embryonic stem cells (Nagano et al., 2017). We refer to the first dataset as “Ramani et al. data", and the second as “Nagano et al. data" for abbreviation. We trained Hyper-SAGNN with the corresponding datasets. Due to the large average degrees of cell nodes, the random walk approach would take an extensive amount of time to sample the walks. Thus, we only applied the encoder version of our method. We visualize the learned embeddings by reducing them to 2 dimensions with PCA and UMAP (McInnes et al., 2018) (Fig. 6A-D).  </p>
<p>We quantified the effectiveness of the embeddings by applying $K$ -means clustering on the Ramani et al. data and evaluating with Adjusted Rand Index (ARI). In addition, we also assessed the effectiveness of the embeddings with a supervised scenario. We used Logistic Regression as the classifier with $10\%$ of the cell as training samples and evaluated the multi-class classification task with Micro-F1 and Macro-F1. We did not run $K$ -means clustering on the Nagano et al. data as it represents a state of continuous cell cycle which is not suitable for a clustering task. We instead used the metric ACROC (Average Circular ROC) developed in the HiCRep/MDS paper (Liu et al., 2018) to evaluate the performance of the three methods on the Nagano et al. data. We compared the performance with two recently developed computational methods based on dimensionality reduction of the contact matrix, HiC-Rep/MDS (Liu et al., 2018) and scHiCluster (Zhou et al., 2019). Because Hyper-SAGNN is not a deterministic method for generating embeddings for scHi-C, we repeated the training process 5 times and averaged the score. All these results are in Fig. 6E.  </p>
<p><img alt="" src="images/2f8fc2a78222537c26c9838f05fc4a765484be433ca714efcd750ff5d77fcb91.jpg" /><br />
Figure 6: (A) and (B): Visualization of the learned embedding based on Hyper-SAGNN for the Ramani et al. data. (C) and (D): Visualization of the learned embedding based on Hyper-SAGNN for the Nagano et al. data. Embedding vectors are projected to two dimensional space using either UMAP or PCA. (E): Quantitative evaluation of the Hyper-SAGNN on two scHi-C datasets  </p>
<p>For the Ramani et al. data (Fig. 6A-B), the visualization of the embedding vectors learned by HyperSAGNN exhibits clear patterns that cells with the same cell type are clustered together. Moreover, cell line HAP1, GM12878, and K562 are all blood-related cell lines, which are likely to be more similar to each other in terms of 3D genome organization as compared to HeLa. Indeed, we observed that they are also closer to each other in the embedding space. Quantitative results in Fig. 6E are consistent with the visualization as our method achieves the highest ARI, Micro-F1, MacroF1 score among all three methods. For the Nagano et al. data, as shown in Fig. 6C-D, we found that the embeddings exhibit a circular pattern that corresponds to the cell cycle. Also, both HiCRep/MDS and Hyper-SAGNN achieve high ACROC scores. All these results show the effectiveness of representing the scHi-C datasets as hypergraphs using Hyper-SAGNN, which has great potential to provide insights into the cell-to-cell variation of higher-order genome organization.  </p>
<h1>5 CONCLUSION</h1>
<p>In this work, we developed a new graph neural network called Hyper-SAGNN for the representation learning of general hypergraphs. The model has the fexibility to deal with homogeneous and heterogeneous, and uniform and non-uniform hypergraphs. We demonstrated that Hyper-SAGNN can improve or match state-of-the-art performance for hypergraph representation learning while addressing the shortcomings of prior methods such as the inability to predict hyperedges for non- $k$ -uniform heterogeneous hypergraphs. Hyper-SAGNN is computationally efficient as the input size to the attention layer is bounded by the maximum hyperedge size as opposed to the number of neighbors.  </p>
<p>One potential improvement of Hyper-SAGNN as future work would be to allow information aggregation over all the first-order neighbors before calculating the static/dynamic embeddings for a node with the additional computational cost. With this design, the static embedding for a node would still satisfy our constraint that it is fixed for a known hypergraph with varying input tuples. This would allow us to incorporate previously developed methods on graphs, such as GraphSAGE (Hamilton et al., 2017a) and GCN (Kipf &amp; Welling, 2016), as well as methods designed for hypergraphs like HyperGCN (Yadati et al., 2018) into this framework for better link prediction performance. Such improvement may also extend the application of Hyper-SAGNN to semi-supervised learning.  </p>
<h1>ACKNOWLEDGMENT</h1>
<p>J.M. acknowledges support from the National Institutes of Health Common Fund 4D Nucleome Program grant U54DK107965, National Institutes of Health grant R01HG007352, and National Science Foundation grant 1717205. Y.Z. (Yao Class, IIIS, Tsinghua University) contributed to this work as a visiting undergraduate student at Carnegie Mellon University during summer 2019.  </p>
<h1>REFERENCES</h1>
<p>Song Bal, Feihu Zhang, and Phup HS Torr. Hypergraph convolution and hypergraph attenton. arXiv preprint arXiv: 1901.08150, 2019. <br />
Inci M Baytas, Cao Xiao, Fei Wang, Anil K Jain, and Jiayu Zhou. Heterogeneous hyper-network embedding. In 2018 IEEE International Conference on Data Mining (ICDM), pp. 875-880. IEEE, 2018. <br />
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems 26, pp. 2787-2795. Curran Associates, Inc., 2013. <br />
Fuli Feng, Xiangnan He, Yiqun Liu, Liqiang Nie, and Tat-Seng Chua. Learning on partial-order hypergraphs. In Proceedings of the 2018 World Wide Web Conference, pp. 1523-1532. International World Wide Web Conferences Steering Committee, 2018. <br />
Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3558-3565, 2019. <br />
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge Discovery and Data Mining, Pp. 855-864. ACM, 2016. <br />
Huan Gui, Jialu Liu, Fangbo Tao, Meng Jiang, Brandon Norick, and Jiawei Han. Large-scale embedding learning in heterogeneous event data. In 2016 IEEE 16th International Conference on Data Mining (ICDM), pp. 907-912. IEEE, 2016. <br />
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017a. <br />
William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and applications. arXiv preprint arXiv: 1709.05584, 2017b. <br />
F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. ACM Trans. Interact. Intell. Syst., 5(4):19:1-19:19, December 2015. ISSN 2160-6455. <br />
Jie Huang, Chuan Chen, Fanghua Ye, Jiajing Wu, Zibin Zheng, and Guohui Ling. Hyper2vec: Biased random walk for hyper-network embedding. In International Conference on Database Systems for Advanced Applications, pp. 273-277. Springer, 2019. <br />
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. <br />
Erez Lieberman-Aiden, Nynke L Van Berkum, Louise Williams, Maxim Imakaev, Tobias Ragoczy, Agnes Telling, Ido Amit, Bryan R Lajoie, Peter J Sabo, Michael O Dorschner, et al. Comprehensive mapping of long-range interactions reveals folding principles of the human genome. Science, 326(5950):289-293, 2009. <br />
Jie Liu, Dejun Lin, Galip Gurkan Yardmci, and Wiliam Stafford Noble. Unsupervised embedding of single-cell hi-c data. Bioinformatics, 34(13):i96-i104, 2018. <br />
Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv: 1802.03426, 2018. <br />
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. arXiv e-prints, art. arXiv:1301.3781, Jan 2013. <br />
Tomas Mikolov, Iya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling. Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp. 3111-3119. Curran Associates, Inc., 2013. <br />
Takashi Nagano, Yaniv Lubling, Csilla Varnai, Carmel Dudley, Wing Leung, Yael Baran, Netta Mendelson Cohen, Steven Wingett, Peter Fraser, and Amos Tanay. Cell-cycle dynamics of chromosomal organization at single-cell resolution. Nature, 547(7661):61, 2017. <br />
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 2Oth ACM SIGKDD international conference on Knowledge Discovery and Data Mining, Pp. 701-710. ACM, 2014. <br />
Vijay Ramani, Xinxian Deng, Ruolan Qiu, Kevin L Gunderson, Frank J Steemers, Christine M Disteche, William S Noble, Zhijun Duan, and Jay Shendure. Massively multiplex single-cell hi-c. Nature Methods, 14(3):263, 2017. <br />
Suhas SP Rao, Miriam H Huntley, Neva C Durand, Elena K Stamenova, Ivan D Bochkov, James T Robinson, Adrian L Sanborn, Ido Machol, Arina D Omer, Eric S Lander, et al. A 3d map of the human genome at kilobase resolution reveals principles of chromatin looping. Cell, 159(7): 1665-1680, 2014. <br />
Liang Sun, Shuiwang Ji, and Jeping Ye. Hypergraph spectral learning for multi-label classifcation. In Proceedings of the 14th ACM SIGKDD international Conference on Knowledge Discovery and Data Mining, Pp. 668-676. ACM, 2008. <br />
Ke Tu, Peng Cui, Xiao Wang, Fei Wang, and Wenwu Zhu. Structural deep embedding for hypernetworks. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018. <br />
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Ilia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998-6008, 2017. <br />
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. <br />
Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Anand Louis, and Partha Talukdar. Hypergcn: Hypergraph convolutional networks for semi-supervised classification. arXiv preprint arXiv: 1809.02589, 2018. <br />
Chia-An Yu, Ching-Lun Tai, Tak-Shing Chan, and Yi-Hsuan Yang. Modeling multi-way relations with hypergraph embedding. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pp. 1707-1710, 2018. <br />
Vincent W. Zheng, Bin Cao, Yu Zheng, Xing Xie, and Qiang Yang. Collaborative filtering meets mobile recommendation: A user-centered approach. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI'10, pp. 236-241. AAAI Press, 2010. <br />
Dengyong Zhou, Jiayuan Huang, and Bernhard Scholkopf. Learning with hypergraphs: Clustering, classification, and embedding. In Advances in Neural Information Processing systems, pp. 1601- 1608, 2007. <br />
Jingtian Zhou, Jianzhu Ma, Yusi Chen, Chuankai Cheng, Bokan Bao, Jian Peng, Terrence J Sejnowski, Jesse R Dixon, and Joseph R Ecker. Robust single-cell hi-c clustering by convolutionand random-walk-based imputation. Proceedings of the National Academy of Sciences, pp. 201901423, 2019.  </p>
<h1>A APPENDIX</h1>
<h1>A.1 DETAILS OF THE STRATEGIES FOR GENERATING FEATURE VECTORS</h1>
<p>We first define the functions used in the subsequent sections as follows: a hyperedge $e$ with weight $w(e)$ is incident with a vertex $v$ if and only if $v\in e$ . We denote the indicator function that represents the incident relationship between $v$ and $e$ by $h(v,e)$ , which equals 1 when $e$ is incident with $v$ .The degree of vertex and the size of hyperedge are defined as:  </p>
<p>$$
\begin{array}{l}{{d(v)\triangleq\displaystyle\sum_{e\in E}h(v,e)w(e)}}\ {{\delta(e)\triangleq\displaystyle\sum_{v\in V}h(v,e)=|e|}}\end{array}
$$  </p>
<h1>A.1.1 ENCODER BASED APPROACH</h1>
<p>As shown on the right side of Fig. 3, the first method to generate features is referred to as the encoder based approach, which is similar to the structure used in DHNE (Tu et al., 2018). We first obtain the incident matrix of the hypergraph $H\in\mathbb{R}^{|V|\times|E|}$ with entries $h(v,e)=1$ $v\in e$ and O otherwise. We also calculate the diagonal degree matrix $D_{v}$ containing the vertex degree $\begin{array}{r}{\boldsymbol{d}(\boldsymbol{v})=\sum_{\boldsymbol{e}\in E}h(\boldsymbol{v},\boldsymbol{e})}\end{array}$ We thus have the adjacency matrix $A\,=\,H H^{T}\,-\,D_{v}$ : of which the entries $a(v_{i},v_{j})$ denote the concurrent times between each pair of nodes $(v_{i},v_{j})$ . The $i$ -th row of $A$ , denoted by $\vec{a}_{i}$ , shows the neighborhood structures of the node $v_{i}$ , which then passes through a one-layer neural network to produce ${\vec{x}}_{i}$  </p>
<p>$$
{\vec{x}}_{i}=\operatorname{tanh}\left(W_{\mathrm{enc}}\cdot{\vec{a}}_{i}+{\vec{b}}_{\mathrm{enc}}\right)
$$  </p>
<p>In DHNE, a symmetric structure was introduced where there are corresponding decoders to transform the ${\vec{x}}_{i}$ backto $\vec{a}_{i}$ . Tu et al. (2018) remarked that including this reconstruction error term would help DHNE to learn the graph structure better. We also include the reconstruction error term into the loss function, but with tied-weights between encoder and decoder to reduce the number of parameters that need to be trained.  </p>
<h1>A.1.2 RANDOM WALK BASED APPROACH</h1>
<p>Besides the encoder based approach, we also utilize a random walk based framework to generate the featurevectors ${\vec{x}}_{i}$ (shown on the left side of Fig. 3). We extend the biased 2nd-order random walks proposed in node2vec (Grover &amp; Leskovec, 2016) to generalize to hypergraphs. For a walk from $v$ to $x$ thento $t$ , the strategies are described as follows.  </p>
<p>The 1st-order random walk strategy given the current vertex $x$ is to randomly select a hyperedge $e$ incident with $x$ based on the weight of $e$ and then to choose the next vertex $y$ from $e$ uniformly (Zhou et al., 2007). Therefore, the 1st-order transition probability is defined as:  </p>
<p>$$
\pi_{1}(t|x)\triangleq\sum_{e\in E}w(e)\frac{h(t,e)h(x,e)}{\delta(e)}
$$  </p>
<p>We then generalize the 2nd-order bias $\alpha_{p q}$ from ordinary graph to hypergraph for a walk from $v$ to $x$ to $t$ as:  </p>
<p>$$
\alpha_{p,q}(t,v)={\left{\begin{array}{l l}{1/p,}&amp;{{\mathrm{if~}}\exists e\in E{\mathrm{,~s.t.~}}t,v,x\in e}\ {1,}&amp;{{\mathrm{else~if~}}\exists e\in E{\mathrm{,~s.t.~}}t,x\in e}\ {1/q,}&amp;{{\mathrm{otherwise}}}\end{array}\right.}
$$  </p>
<p>where the parameters $p$ and $q$ are to control the tendencies that encourage outward exploration and obtain a local view.  </p>
<p>Next we add the above terms to set the biased 2nd-order transition probability as:  </p>
<p>$$
\pi(t|v,x)={\left{\begin{array}{l l}{{\frac{\pi_{1}(t|x)\cdot\alpha_{p q}(t,v)}{Z}},}&amp;{{\mathrm{if~}}\exists e\in E,{\mathrm{~s.t.~}}v,x\in e}\ {0,}&amp;{{\mathrm{otherwise}}}\end{array}\right.}
$$  </p>
<p>where $Z$ is a normalizing factor.  </p>
<p>With the well-defined 2nd-order transition probability $\pi(t|v,x)$ , we simulate a random walk of fixed length $l$ through a 2nd-order Markov process marked by $P(c_{i}=t|c_{i-1}=x,c_{i-2}=v)=\pi(t|v,x).$ where $c_{i}$ is the $i$ -th node in the walk. A Skip-gram model (Mikolov et al., 2013; Mikolov et al., 2013) is then used to extract the node features from sampled walks such that the nodes that appear in similar contexts would have similar embeddings.  </p>
<h1>A.2 DETAILS OF THE DATASETS USED IN THIS WORK</h1>
<p>The four datasets used in the first part of our evaluation are:  </p>
<p>· GPS (Zheng et al., 2010): GPS network. The hyperedges are based on (user, location, activity)relations. <br />
● MovieLens (Harper &amp; Konstan, 2015): Social network. The hyperedges are based on (user, movie, tag) relations, describing peoples? tagging activities. <br />
● drug: Medicine network from FAERS1. The hyperedges are based on (user, drug, reaction) relations. <br />
● wordnet (Bordes et al., 2013): Semantic network from WordNet 3.0. The hyperedges are based on (head entity, relation, tail entity), expressing the relationships between words.  </p>
<p>Details of the datasets are shown in Table A1.  </p>
<p>Table A1: Datasets used in this work. Note that the columns under “#(V)" correspond to the columns under "'node type' for each dataset.   </p>
<p><img alt="" src="images/27a4e873ffc6c8183b1332fe174582e05de2bead2de34155c14c11480a7c38e0.jpg" />  </p>
<h1>A.3PARAMETER SETTING</h1>
<p>We downloaded the source code of DHNE from its GitHub repository. The structure of the neural network of DHNE was set to be the same as what the authors described in Tu et al. (2018). We tuned parameters such as the $\alpha$ term and the learning rate following the same procedure. We also tried adding dropout between representation vectors and the fully connected layer for better performance of DHNE. All these parameters were tuned until it was able to replicate or even improve the performance reported in the original paper. To make a fair comparison, for all the results below, we made sure that the training and validation data setups were the same across different methods.  </p>
<p>For node2vec, we decomposed the hypergraph into pairwise edges and ran node2vec on the decomposed graph. For the hyperedge prediction task, we first used the learned embedding to predict pairwise edges. We then used the mean or min of the pairwise similarity as the probability for the tuple to form a hyperedge. We set the window size to 10, walk length to 40, the number of walks per vertex to 10, which are the same parameters used in DHNE for node2vec. However, we found that for the baseline method node2vec, when we tuned the hyper-parameter $p,q$ and alsoused larger walk length, window size and walks per vertex (120, 20, 80 instead of 40, 10, 10), it would achieve comparable performance for node classification task as DHNE. This observation is consistent with our designed biased hypergraph random walk. But this would result in a longer time for sampling the walks and training the skip-gram model. We therefore kept the parameters consistent with what was used in DHNE paper.  </p>
<p>For our Hyper-SAGNN, we set the representation size to 64, which is the same as DHNE. The number of heads in the multi-head attention layer is set to 8. When using the encoder based approach tocalculate $x_{i}$ , we set the encoder structure to be the same as the encoder part in DHNE. When using the random walk based approach, we decomposed the hypergraph into a graph as described above. We set the window size to 10, walk length to 40, the number of walks per vertex to 10, to allow time-efficient generation of feature vector ${\vec{x}}_{i}$ . The results in Section 4.1 showed that even when the pre-trained embeddings are not so ideal, Hyper-SAGNN can still well capture the structure of the graph.  </p>
<p>To train the model, we used the Adam optimizer with learning rate 1e-3. Each batch contains 96 positive hyperedges with 480 negative samples. The training is terminated when it reaches the maximum training epoch number (1o0) or the performance on the validation set no longer improves.  </p>
<h1>A.4 EXPERIMENT SETTING</h1>
<p>To compare the performance of Hyper-SAGNN with other baseline methods, we used the same three tasks in the DHNE paper, i.e., network reconstruction, link prediction, and node classification. In this section, we describe the setting of these three tasks and the evaluation metrics.  </p>
<p>Network reconstruction aims to reconstruct the input hypergraph from the learned embedding. Specifically, a hypergraph $G=(V,E)$ is used as input to the algorithm. After training, the model makes predictions for the original hyperedge set $E$ . Link prediction, on the other hand, aims to predict the unseen hyperedge set $E^{'}$ based on the model trained with hypergraph $G=(V,E)$ .These two tasks can be regarded as binary classification tasks and thus can be evaluated by metrics such as AUROC and AUPR. For both tasks, the negative samples are set to be 5 times the amount of the positive samples following the same setup of DHNE.  </p>
<p>For the node classification, after training the model, the embeddings for the nodes are used to train a Logistic Regression classifier with targets as the pre-defined labels for the nodes. Here we have a multi-label classification and a multi-class classification task for dataset MovieLens and wordnet, respectively, making the metrics defined for binary classification not applicable. Therefore, the performances are evaluated by Micro and Macro F1 scores that are used for quantifying the performance of multi-label/multi-class classification.  </p>
<h1>A.5 ANALYSIS OF THE DYNAMIC EMBEDDINGS</h1>
<p>In this section, we discuss the relationships between dynamic embeddings and static embeddings. As mentioned in the Method section, we design the model to establish the connection between the probability score for a tuple with how well the static embedding of each node can be approximated by the features of their neighbor within that tuple. If the model works as we designed, the dynamic embedding of node $i$ conditioned on an actual hyperedge $(i,j,...,k)$ should approximate the static embedding of node $i$ better. In contrast, the dynamic embedding of node $i$ when it is a member of a non-hyperedge tuple would not have a good “approximation". To evaluate this, after training the model we collected all the dynamic embeddings for nodes when they are within the tuples of positive samples, which would be referred to as positive dynamic embeddings. We also collected the dynamic embeddings for nodes when conditioned on the “hard negative samples” (the negative samples that are generated by only changing one node in the positive samples), which would be referred to as negative dynamic embeddings. We tested whether the positive dynamic embeddings indeed resemble the static embeddings better by comparing the performance of node classification using dynamic embeddings and static embeddings. For each node, since the number of positive and negative dynamic embeddings is not finite, we sampled and averaged a number of them as the features. The remaining setting of node classification is the same as described in the main text. As shown in Fig. A1, in general, the positive dynamic embeddings can achieve much better performance as compared to the negative dynamic embeddings. Both the micro and macro F1 scores increase when more positive dynamic embeddings are sampled and averaged. When averaging all positive dynamic embeddings and using those as features, the node classification performance is close to what we achieved using static embeddings. This analysis demonstrates that the positive dynamic embeddings “approximate”’ the static embeddings as they contain sufficient information for accurate node classification. In addition, for the negative dynamic embeddings where only one node is changed when generating negative samples, it performs dramatically worse. Note that we exclude the $\alpha_{i i}$ term in the calculation of dynamic embeddings, which makes the dynamic embedding for node $i$ the combination of features of its neighbor within a given tuple. These results demonstrate that the probability scores for each node indicate the “distances” of static/dynamic embedding pairs that reflect how well the static embedding of each node can be approximated by the features of their neighbor within that tuple.  </p>
<p><img alt="" src="images/463da8bea5e0d55286efafd89fc5ae45176dc09c78d423c75c09d25fbb4d4498.jpg" /><br />
Figure A1: Node classification performance comparison for static embeddings and dynamic embeddings. The results based on static embeddings are marked as “static". Results based on positive/negative dynamic embeddings are marked as “dynamic P/dynamic $\mathbf{N}^{\bullet}$ . The numbers of dynamic embeddings sampled during this process are also included in the legend.  </p>
<h1>A.6 COMPARISON OF HYPER-SAGNN VS. THE VARIANTS</h1>
<p>As mentioned above, unlike the standard GAT model, we exclude the $\alpha_{i i}$ term in the self-attention mechanism. To test whether this constraint would improve or reduce the model's ability to learn, we implemented a variant of our model (referred to as variant type I) by including this term. Also, as mentioned in the Method section, another potential variant of our model would be directly using the $\vec{d}_{i}^{_}$ to calculate the probability score $p_{i}^{_}$ . We refer to this variant as variant type II. For variant type II, on node classification task, since it does not have a static embedding, we used $W_{v}^{T}x_{i}$ .The rest of the parameters and structure of the neural network remain the same.  </p>
<p>We then compared the performance of Hyper-SAGNN and two variants in terms of AUC and AUPR values for network reconstruction task and hyperedge link prediction task on the following four datasets: MovieLens, wordnet, drug, and GPS. We also compared the performance in terms of the Micro F1 score and Macro F1 score on the node classification task on the MovieLens and the wordnet dataset. For the MovieLens dataset, we used $90\%$ nodes as training data while for wordnet, weused $1\%$ of the nodes as training data. All the evaluation setup is the same as described in the main text. To avoid the effect of randomness from the neural network training, we repeated the training process for each experiment five times and made the line plot of the score versus the epoch number. To illustrate the differences more clearly, we started the plot at epoch 3 for the random walk based approach and epoch 12 for the encoder based approach. The performance of the model using the random walk based approach is shown in Fig. A2 to Fig. A5. The performance of the model using the encoder based approach is shown in Fig. A6 to Fig. A9.  </p>
<p>For models with the random walk based approach, Hyper-SAGNN is the best in terms of all metrics for the GPS, MovieLens, and wordnet dataset. On the drug dataset, Hyper-SAGNN achieves higher AUROC and AUPR score on the network reconstruction task than two variants, but slightly lower AUROC score for the link prediction task (less than $0.5\%$  </p>
<p>For models with the encoder based approach, the advantage is not that obvious. All 3 methods achieve similar performance in terms of all metrics for the GPS and the drug dataset. For the MovieLens and wordnet dataset, Hyper-SAGNN performs similar to variant type I, higher than variant type II on the network reconstruction and link prediction task. However, our model achieves slightly higher accuracy on the node classification task than variant type I.  </p>
<p>Therefore, these evaluations show that the choice of the structure of Hyper-SAGNN can achieve higher or at least comparable performance than the two potential variants over multiple tasks on multiple datasets.  </p>
<p><img alt="" src="images/35743f1f9d7e5f9284a2cace9db08e463d316ba0317d2da857237bbfb9f913c9.jpg" /><br />
Figure A2: Performance comparison of Hyper-SAGNN - Walk and Variant Type I, II (GPS)  </p>
<p><img alt="" src="images/fd6e29631cac53927090984e56c1f7da8f6f7f1c555b83035993c036de1b69e0.jpg" /><br />
Figure A3: Performance comparison of Hyper-SAGNN - Walk and Variant Type I, I (MovieLens)  </p>
<p><img alt="" src="images/cfff6e7568bfddcb47a5bc1ab9cd910362332148c73536ef916c460f334c9b5f.jpg" /><br />
Figure A4: Performance comparison of Hyper-SAGNN - Walk and Variant Type I, II (drug)  </p>
<p><img alt="" src="images/721fee4e2e39faf871c6f4786b8e71e7d2f273928c1b0dd9e2fc68abb4228384.jpg" /><br />
Figure A5: Performance comparison of Hyper-SAGNN - Walk and Variant Type I, II (wordnet)  </p>
<p><img alt="" src="images/38eba66c542594d90cceebcb571e2992ba3f96456dde5893e915d64f3848a180.jpg" /><br />
Figure A6: Performance comparison of Hyper-SAGNN - Encoder and Variant Type I, I (GPS)  </p>
<p><img alt="" src="images/93a0e6596f217733d5c7ec5cae862b295365d66b4acde9cb66317e3933be71e7.jpg" /><br />
Figure A7: Performance comparison of Hyper-SAGNN - Encoder and Variant Type I, II (MovieLens)  </p>
<p><img alt="" src="images/d2f7139ce1f11d15b7d5ee5fb5dee76f5226cd9125d7f7b992b377786763be12.jpg" /><br />
Figure A8: Performance comparison of Hyper-SAGNN - Encoder and Variant Type I, II (drug)  </p>
<p><img alt="" src="images/6507a77b5b50c7b28e3edfc6e92cba847c5d601b0274cf74e02305316d653e0a.jpg" /><br />
Figure A9: Performance comparison of Hyper-SAGNN - Encoder and Variant Type I, II (wordnet)  </p>
    </body>
    </html>